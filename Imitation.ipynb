{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.data import rollout\n",
    "from imitation.algorithms import bc\n",
    "from imitation.util import logger\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, S, A):\n",
    "        self.S = torch.tensor(S, dtype=torch.float32)\n",
    "        self.A = torch.tensor(A, dtype=torch.float32)\n",
    "        self.num_samples = self.S.shape[0]\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Action space\n",
    "        action_dim = self.A.shape[1]\n",
    "        self.action_space = spaces.Box(low=-1e6, high=1e6, shape=(action_dim,), dtype=np.float32)\n",
    "\n",
    "        # Observation space\n",
    "        obs_dim_S = self.S.shape[1]\n",
    "        observation_space = {}\n",
    "        for i in range(obs_dim_S):\n",
    "            observation_space[f'S_{i+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        self.observation_space = spaces.Dict(observation_space)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self._get_observation()\n",
    "        reward = self._calculate_reward(action)\n",
    "        done = (self.current_step == self.num_samples - 1)\n",
    "        self.current_step = (self.current_step + 1) % self.num_samples\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        obs = {}\n",
    "        for i in range(self.S.shape[1]):\n",
    "            obs[f'S_{i+1}'] = np.array([self.S[self.current_step, i]], dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        reward = torch.sum(self.A[self.current_step] * action)\n",
    "        return reward.item()\n",
    "\n",
    "# # 假設 S 和 A 是你的狀態和動作矩陣\n",
    "# S = np.random.rand(1000, 9)\n",
    "# A = np.random.rand(1000, 45)\n",
    "\n",
    "# # 創建自訂環境\n",
    "# env = CustomEnv(S, A)\n",
    "# vec_env = DummyVecEnv([lambda: env])\n",
    "# # 使用 PPO 訓練一個專家模型\n",
    "# expert_model = PPO(\"MultiInputPolicy\", vec_env, verbose=1)\n",
    "# expert_model.learn(total_timesteps=10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2553.184532403946,\n",
       "  2553.184532403946,\n",
       "  2553.184532403946,\n",
       "  2553.184532403946,\n",
       "  2553.184532403946,\n",
       "  2553.184532403946,\n",
       "  2553.184532403946,\n",
       "  2553.184532403946,\n",
       "  2553.184532403946,\n",
       "  2553.184532403946],\n",
       " [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "evaluate_policy(expert_model, vec_env, 10,return_episode_rewards=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "num_observation = S.shape[1]\n",
    "num_action = A.shape[1]\n",
    "\n",
    "# class ObservationMatchingEnv(gym.Env):\n",
    "#     def __init__(self, num_observation=num_observation,num_action=num_action):\n",
    "#         self.state = None\n",
    "#         self.num_observation = num_observation\n",
    "#         self.num_action = num_action\n",
    "#         self.observation_space = spaces.Box(low=0, high=1, shape=(num_observation,), dtype=np.float32)\n",
    "#         self.action_space = spaces.Box(low=0, high=1, shape=(num_action,), dtype=np.float32)\n",
    "\n",
    "#     def reset(self, seed: int = None, options: Optional[Dict[str, Any]] = None):\n",
    "#         super().reset(seed=seed, options=options)\n",
    "#         self.state = self.observation_space.sample()\n",
    "#         return self.state, {}\n",
    "\n",
    "#     def step(self, action):\n",
    "#         reward = -np.abs(self.state - action).mean()\n",
    "#         self.state = self.observation_space.sample()\n",
    "#         return self.state, reward, False, False, {}\n",
    "    \n",
    "\n",
    "\n",
    "class ObservationMatchingEnv(gym.Env):\n",
    "    def __init__(self, num_options: int = 2):\n",
    "        self.state = None\n",
    "        self.num_options = num_options\n",
    "        self.observation_space = Box(0, 1, shape=(num_options,))\n",
    "        self.action_space = Box(0, 1, shape=(num_options,))\n",
    "\n",
    "    def reset(self, seed: int = None, options: Optional[Dict[str, Any]] = None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.state = self.observation_space.sample()\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = -np.abs(self.state - action).mean()\n",
    "        self.state = self.observation_space.sample()\n",
    "        return self.state, reward, False, False, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "\n",
    "class ObservationMatchingEnv(gym.Env):\n",
    "    def __init__(self, num_observation: int = 23, num_action: int = 42 ):\n",
    "        self.state = None\n",
    "        self.num_observation = num_observation\n",
    "        self.num_action = num_action\n",
    "        self.observation_space = Box(-np.inf, np.inf, shape=(num_observation,))\n",
    "        self.action_space = Box(-np.inf, np.inf, shape=(num_action,))\n",
    "\n",
    "    def reset(self, seed: int = None, options: Optional[Dict[str, Any]] = None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.state = self.observation_space.sample()\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = -np.abs(self.state - action).mean()\n",
    "        self.state = self.observation_space.sample()\n",
    "        return self.state, reward, False, False, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gym.register(\n",
    "    id=\"custom/ObservationMatchingEnv-v0\",\n",
    "    entry_point=ObservationMatchingEnv,  \n",
    "    max_episode_steps=500,\n",
    ")\n",
    "\n",
    "# Create a single environment for training an expert with SB3\n",
    "env = gym.make(\"custom/ObservationMatchingEnv-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Create a vectorized environment for training with `imitation`\n",
    "\n",
    "# Option A: use the `make_vec_env` helper function - make sure to pass `post_wrappers=[lambda env, _: RolloutInfoWrapper(env)]`\n",
    "venv = make_vec_env(\n",
    "    \"custom/ObservationMatchingEnv-v0\",\n",
    "    rng=np.random.default_rng(),\n",
    "    n_envs=4,\n",
    "    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ObservationMatchingEnv(gym.Env):\n",
    "    def __init__(self, observation_dim, action_dim):\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Action space\n",
    "        self.action_dim = action_dim\n",
    "        self.observation_dim = observation_dim\n",
    "        self.action_space = Box(low=-1e6, high=1e6, shape=(self.action_dim,), dtype=np.float32)\n",
    "        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(self.observation_dim,), dtype=np.float32)\n",
    "\n",
    "\n",
    "    def reset(self, seed: int = None, options: Optional[Dict[str, Any]] = None):\n",
    "        self.state = self.observation_space.sample()\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        print(self.state)\n",
    "        print(action)\n",
    "        reward = -np.abs(self.state - action).mean()\n",
    "        self.state = self.observation_space.sample()\n",
    "        return self.state, reward, False, False, {}\n",
    "# Register the environment with Gym\n",
    "# gym.register(\n",
    "#     id=\"ObservationMatchingEnv-v0\",\n",
    "#     entry_point=ObservationMatchingEnv(S,A),  # Replace 'your_module_path' with the actual module path\n",
    "# )\n",
    "\n",
    "# # Create an instance of the environment\n",
    "# env = gym.make(\"ObservationMatchingEnv-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Any\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "\n",
    "from gymnasium.spaces.box import Box\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, S, A, R):\n",
    "        self.S = torch.tensor(S, dtype=torch.float32)  # Convert S to torch tensor\n",
    "        self.A = torch.tensor(A, dtype=torch.float32)  # Convert A to torch tensor\n",
    "        self.R = torch.tensor(R, dtype=torch.float32) if R is not None else None  # Convert R to torch tensor if provided\n",
    "        self.num_samples = self.S.shape[0]\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Action space: assume discrete actions for simplicity\n",
    "        action_dim = self.A.shape[1]\n",
    "        self.observation_dim = self.S.shape[1]\n",
    "        self.action_space = Box(low=-1e6, high=1e6, shape=(action_dim,), dtype=np.float32)\n",
    "        \n",
    "        # Observation space: separate S and Y with specific labels\n",
    "        # obs_dim_S = self.S.shape[1]\n",
    "        # observation_space = {}\n",
    "        # for i in range(obs_dim_S):\n",
    "        #     observation_space[f'S_{i+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        \n",
    "        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(self.observation_dim,), dtype=np.float32)\n",
    "        \n",
    "    # def reset(self, seed: int = None):\n",
    "    #     self.current_step = 0\n",
    "    #     return self._get_observation()\n",
    "\n",
    "\n",
    "    def reset(self, seed: int = None, options: Optional[Dict[str, Any]] = None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.state = self.observation_space.sample()\n",
    "        return self.state, {}\n",
    "    \n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        print(self.state)\n",
    "        print(action)\n",
    "        reward = -np.abs(self.state - action).mean()\n",
    "        self.state = self.observation_space.sample()\n",
    "        return self.state, reward, False, False, {}\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply action and get next state and reward\n",
    "        #next_state = self._get_observation()\n",
    "        self.state = self.observation_space.sample()\n",
    "        reward = self._calculate_reward(action)\n",
    "        #done = (self.current_step == self.num_samples - 1)\n",
    "        #self.current_step = (self.current_step + 1) % self.num_samples\n",
    "        return self.state, reward, False, False, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        obs = {}\n",
    "        for i in range(self.S.shape[1]):\n",
    "            obs[f'S_{i+1}'] = np.array([self.S[self.current_step, i]])\n",
    "        return obs\n",
    "    \n",
    "    def _calculate_reward(self, action):\n",
    "        # Example: using a reward matrix R based on expected Y values\n",
    "        if self.R is not None:\n",
    "            ideal_y = self.R[action]  # Ideal Y value for the chosen action\n",
    "            actual_y = self.S[self.current_step]  # Actual Y value\n",
    "            reward = -torch.norm(actual_y - ideal_y)  # Negative L2 norm difference as reward\n",
    "        else:\n",
    "            # Default reward function (replace with your own logic)\n",
    "            reward = torch.sum(self.A[self.current_step] * action)\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import TimeLimit\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import numpy as np\n",
    "\n",
    "# 假設 S 和 A 是你的狀態和動作矩陣\n",
    "S = np.random.rand(10000, 9)\n",
    "A = np.random.rand(1000, 45)\n",
    "env = CustomEnv(S, A,R=None)\n",
    "\n",
    "env = TimeLimit(env, max_episode_steps=500)\n",
    "\n",
    "# Create a vectorized environment for training with `imitation`\n",
    "\n",
    "\n",
    "# Option A: use a helper function to create multiple environments\n",
    "def _make_env(s=S,a=A,R=None):\n",
    "    \"\"\"Helper function to create a single environment. Put any logic here, but make sure to return a RolloutInfoWrapper.\"\"\"\n",
    "    _env = CustomEnv(s,a,R)\n",
    "    _env = TimeLimit(_env, max_episode_steps=500)\n",
    "    _env = RolloutInfoWrapper(_env)\n",
    "    return _env\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "venv = DummyVecEnv([_make_env for _ in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward before training: 0.16735349398804827\n",
      "Reward before training: 0.16735349398804827\n",
      "Expert reward: 1910.5735375\n",
      "Reward before training: 0.04479575853911229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.0639  |\n",
      "|    entropy        | 63.9     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 82.5     |\n",
      "|    loss           | 64       |\n",
      "|    neglogp        | 64       |\n",
      "|    prob_true_act  | 1.65e-26 |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480batch [00:01, 291.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 500      |\n",
      "|    ent_loss       | -0.0638  |\n",
      "|    entropy        | 63.8     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 84.3     |\n",
      "|    loss           | 63.6     |\n",
      "|    neglogp        | 63.6     |\n",
      "|    prob_true_act  | 1.01e-25 |\n",
      "|    samples_so_far | 16032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "812batch [00:02, 272.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward after training: 1898.6023386627435\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "expert = PPO(\n",
    "    policy=MlpPolicy,\n",
    "    env=env,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    "    n_steps=64,\n",
    ")\n",
    "\n",
    "reward, _ = evaluate_policy(expert, env, 10)\n",
    "print(f\"Reward before training: {reward}\")\n",
    "\n",
    "\n",
    "# Note: if you followed step 2a, i.e. registered the environment, you can use the environment name directly\n",
    "\n",
    "# expert = PPO(\n",
    "#     policy=MlpPolicy,\n",
    "#     env=\"custom/ObservationMatching-v0\",\n",
    "#     seed=0,\n",
    "#     batch_size=64,\n",
    "#     ent_coef=0.0,\n",
    "#     learning_rate=0.0003,\n",
    "#     n_epochs=10,\n",
    "#     n_steps=64,\n",
    "# )\n",
    "expert.learn(10_000)  # Note: set to 100000 to train a proficient expert\n",
    "reward, _ = evaluate_policy(expert, expert.get_env(), 10)\n",
    "print(f\"Expert reward: {reward}\")\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "rollouts = rollout.rollout(\n",
    "    expert,\n",
    "    venv,\n",
    "    rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n",
    "    rng=rng,\n",
    ")\n",
    "transitions = rollout.flatten_trajectories(rollouts)\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "reward_before_training, _ = evaluate_policy(bc_trainer.policy, env, 10)\n",
    "print(f\"Reward before training: {reward_before_training}\")\n",
    "\n",
    "bc_trainer.train(n_epochs=1)\n",
    "reward_after_training, _ = evaluate_policy( .policy, env, 10)\n",
    "print(f\"Reward after training: {reward_after_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate_policy\u001b[49m(bc_trainer\u001b[38;5;241m.\u001b[39mpolicy, env, \u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_policy' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_policy(bc_trainer.policy, env, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ActorCriticPolicy:\n\tsize mismatch for mlp_extractor.policy_net.0.weight: copying a param with shape torch.Size([32, 9]) from checkpoint, the shape in current model is torch.Size([64, 9]).\n\tsize mismatch for mlp_extractor.policy_net.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp_extractor.policy_net.2.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for mlp_extractor.policy_net.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp_extractor.value_net.0.weight: copying a param with shape torch.Size([32, 9]) from checkpoint, the shape in current model is torch.Size([64, 9]).\n\tsize mismatch for mlp_extractor.value_net.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp_extractor.value_net.2.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for mlp_extractor.value_net.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for action_net.weight: copying a param with shape torch.Size([45, 32]) from checkpoint, the shape in current model is torch.Size([45, 64]).\n\tsize mismatch for value_net.weight: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([1, 64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mexpert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbc_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\torch\\nn\\modules\\module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ActorCriticPolicy:\n\tsize mismatch for mlp_extractor.policy_net.0.weight: copying a param with shape torch.Size([32, 9]) from checkpoint, the shape in current model is torch.Size([64, 9]).\n\tsize mismatch for mlp_extractor.policy_net.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp_extractor.policy_net.2.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for mlp_extractor.policy_net.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp_extractor.value_net.0.weight: copying a param with shape torch.Size([32, 9]) from checkpoint, the shape in current model is torch.Size([64, 9]).\n\tsize mismatch for mlp_extractor.value_net.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp_extractor.value_net.2.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for mlp_extractor.value_net.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for action_net.weight: copying a param with shape torch.Size([45, 32]) from checkpoint, the shape in current model is torch.Size([45, 64]).\n\tsize mismatch for value_net.weight: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([1, 64])."
     ]
    }
   ],
   "source": [
    "expert.policy.load_state_dict(bc_trainer.policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward before training: 0.07935524935601279\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.0639  |\n",
      "|    entropy        | 63.9     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 82.5     |\n",
      "|    loss           | 64.8     |\n",
      "|    neglogp        | 64.9     |\n",
      "|    prob_true_act  | 2.64e-26 |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "475batch [00:01, 273.62batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 500      |\n",
      "|    ent_loss       | -0.0638  |\n",
      "|    entropy        | 63.8     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 84.5     |\n",
      "|    loss           | 62.6     |\n",
      "|    neglogp        | 62.7     |\n",
      "|    prob_true_act  | 1.72e-25 |\n",
      "|    samples_so_far | 16032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "812batch [00:02, 271.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward after training: 1785.1691998738795\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForward32Policy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=9, out_features=32, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=9, out_features=32, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=32, out_features=45, bias=True)\n",
       "  (value_net): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 14\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DummyVecEnv, SubprocVecEnv\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create a single environment for training an expert with SB3\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#env = gym.make(\"custom/ObservationMatching-v0\")\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Option A: use the `make_vec_env` helper function - make sure to pass `post_wrappers=[lambda env, _: RolloutInfoWrapper(env)]`\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m venv \u001b[38;5;241m=\u001b[39m \u001b[43mmake_vec_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_wrappers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mRolloutInfoWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\imitation\\util\\util.py:117\u001b[0m, in \u001b[0;36mmake_vec_env\u001b[1;34m(env_name, rng, n_envs, parallel, log_dir, max_episode_steps, post_wrappers, env_make_kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes a vectorized environment.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    A VecEnv initialized with `n_envs` environments.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Resolve the spec outside of the subprocess first, so that it is available to\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# subprocesses running `make_env` via automatic pickling.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Just to ensure packages are imported and spec is properly resolved\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m tmp_env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m tmp_env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    119\u001b[0m spec \u001b[38;5;241m=\u001b[39m tmp_env\u001b[38;5;241m.\u001b[39mspec\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\gymnasium\\envs\\registration.py:738\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    735\u001b[0m         env_spec\u001b[38;5;241m.\u001b[39madditional_wrappers \u001b[38;5;241m=\u001b[39m ()\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;66;03m# For string id's, load the environment spec from the registry then make the environment spec\u001b[39;00m\n\u001b[1;32m--> 738\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[0;32m    741\u001b[0m     env_spec \u001b[38;5;241m=\u001b[39m _find_spec(\u001b[38;5;28mid\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import TimeLimit\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "# Create a single environment for training an expert with SB3\n",
    "#env = gym.make(\"custom/ObservationMatching-v0\")\n",
    "\n",
    "\n",
    "# Create a vectorized environment for training with `imitation`\n",
    "\n",
    "# Option A: use the `make_vec_env` helper function - make sure to pass `post_wrappers=[lambda env, _: RolloutInfoWrapper(env)]`\n",
    "venv = make_vec_env(\n",
    "    env,\n",
    "    rng=np.random.default_rng(),\n",
    "    n_envs=4,\n",
    "    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
