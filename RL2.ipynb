{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install sb3-contrib\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#discrete_action_max = 25\n",
    "# class CustomEnv(gym.Env):\n",
    "#     def __init__(self, S, A, s, R):\n",
    "#         self.S = torch.tensor(S, dtype=torch.float32)  # Convert S to torch tensor\n",
    "#         self.A = torch.tensor(A, dtype=torch.float32)  # Convert A to torch tensor\n",
    "#         self.s = s\n",
    "#         if self.s.all != None:\n",
    "#             self.s = torch.tensor(s, dtype=torch.float32)  # Convert Y to torch tensor\n",
    "#             obs_dim_s = self.s.shape[1]\n",
    "#         self.R = torch.tensor(R, dtype=torch.float32) if R is not None else None  # Convert R to torch tensor if provided\n",
    "#         self.num_samples = self.S.shape[0]\n",
    "#         self.current_step = 0\n",
    "        \n",
    "#         # Action space: assume discrete actions for simplicity\n",
    "#         action_dim = self.A.shape[1]\n",
    "#         #self.action_space = spaces.MultiDiscrete([self.discrete_action_max]*action_dim)\n",
    "#         self.action_space = spaces.Box(low=-1e6, high=1e6, shape=(action_dim,), dtype=np.float32)\n",
    "#         # Observation space: separate S and Y with specific labels\n",
    "#         obs_dim_S = self.S.shape[1]\n",
    "#         observation_space = {}\n",
    "#         for i in range(obs_dim_S):\n",
    "#             observation_space[f'S_{i+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "\n",
    "#         if self.s.all != None:\n",
    "#             for j in range(obs_dim_s):\n",
    "#                 observation_space[f's_{j+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "#         self.observation_space = spaces.Dict(observation_space)\n",
    "        \n",
    "#     def reset(self):\n",
    "#         self.current_step = 0\n",
    "#         return self._get_observation()\n",
    "    \n",
    "#     def step(self, action):\n",
    "#         # Apply action and get next state and reward\n",
    "#         next_state = self._get_observation()\n",
    "#         reward = self._calculate_reward(action)\n",
    "#         done = (self.current_step == self.num_samples - 1)\n",
    "#         self.current_step += 1\n",
    "#         return next_state, reward, done, {}\n",
    "    \n",
    "#     def _get_observation(self):\n",
    "#         obs = {}\n",
    "#         for i in range(self.S.shape[1]):\n",
    "#             obs[f'S_{i+1}'] = np.array([self.S[self.current_step,i]])\n",
    "#         if self.s.all != None:\n",
    "#             for j in range(self.s.shape[1]):\n",
    "#                 obs[f's_{j+1}'] = np.array([self.s[self.current_step,j]])\n",
    "#         return obs\n",
    "    \n",
    "#     def _calculate_reward(self, action):\n",
    "#         # Example: using a reward matrix R based on expected Y values\n",
    "#         if self.R is not None:\n",
    "#             ideal_y = self.R[action]  # Ideal Y value for the chosen action\n",
    "#             actual_y = self.S[self.current_step]  # Actual Y value\n",
    "#             reward = -torch.norm(actual_y - ideal_y)  # Negative L2 norm difference as reward\n",
    "#         else:\n",
    "#             # Default reward function (replace with your own logic)\n",
    "#             reward = torch.sum(self.A[self.current_step] * action)\n",
    "        \n",
    "#         return reward\n",
    "\n",
    "# class CustomEnv(gym.Env):\n",
    "#     def __init__(self, S, A, Y, R):\n",
    "#         self.S = torch.tensor(S, dtype=torch.float32)  # Convert S to torch tensor\n",
    "#         self.A = torch.tensor(A, dtype=torch.float32)  # Convert A to torch tensor\n",
    "#         self.Y = torch.tensor(Y, dtype=torch.float32)  # Convert Y to torch tensor\n",
    "#         self.R = torch.tensor(R, dtype=torch.float32) if R is not None else None  # Convert R to torch tensor if provided\n",
    "#         self.num_samples = S.shape[0]\n",
    "#         self.current_step = 0\n",
    "#         #self.discrete_action_max = discrete_action_max\n",
    "        \n",
    "#         # Action space: assume discrete actions for simplicity\n",
    "#         action_dim = A.shape[1]\n",
    "#         #self.action_space = spaces.MultiDiscrete([self.discrete_action_max]*action_dim)\n",
    "#         self.action_space = spaces.Box(low=-1e6, high=1e6, shape=(action_dim,), dtype=np.float32)\n",
    "        \n",
    "#         #self.action_space = spaces.Box([self.discrete_action_max]*action_dim)\n",
    "#         # Observation space: separate S and Y with specific labels\n",
    "#         obs_dim_s = S.shape[1]\n",
    "#         obs_dim_y = Y.shape[1]\n",
    "#         observation_space = {}\n",
    "#         for i in range(obs_dim_s):\n",
    "#             observation_space[f'S_{i+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "#         for j in range(obs_dim_y):\n",
    "#             observation_space[f'Y_{j+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "#         self.observation_space = spaces.Dict(observation_space)\n",
    "        \n",
    "#     def reset(self):\n",
    "#         self.current_step = 0\n",
    "#         return self._get_observation()\n",
    "    \n",
    "#     def step(self, action):\n",
    "#         # Apply action and get next state and reward\n",
    "#         next_state = self._get_observation()\n",
    "#         reward = self._calculate_reward(action)\n",
    "#         done = (self.current_step == self.num_samples - 1)\n",
    "#         self.current_step += 1\n",
    "#         return next_state, reward, done, {}\n",
    "    \n",
    "#     def _get_observation(self):\n",
    "#         obs = {}\n",
    "#         for i in range(self.S.shape[1]):\n",
    "#             obs[f'S_{i+1}'] = np.array([self.S[self.current_step, i]])\n",
    "#         for j in range(self.Y.shape[1]):\n",
    "#             obs[f'Y_{j+1}'] = np.array([self.Y[self.current_step, j]])\n",
    "#         return obs\n",
    "    \n",
    "#     def _calculate_reward(self, action):\n",
    "#         # Example: using a reward matrix R based on expected Y values\n",
    "#         if self.R is not None:\n",
    "#             ideal_y = self.R[action]  # Ideal Y value for the chosen action\n",
    "#             actual_y = self.Y[self.current_step]  # Actual Y value\n",
    "#             reward = -torch.norm(actual_y - ideal_y)  # Negative L2 norm difference as reward\n",
    "#         else:\n",
    "#             # Default reward function (replace with your own logic)\n",
    "#             reward = torch.sum(self.A[self.current_step] * action)\n",
    "        \n",
    "#         return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, S, A, R):\n",
    "        self.S = S#torch.tensor(S, dtype=torch.float32)  # Convert S to torch tensor\n",
    "        self.A = A#torch.tensor(A, dtype=torch.float32)  # Convert A to torch tensor\n",
    "        self.R = R#torch.tensor(R, dtype=torch.float32) if R is not None else None  # Convert R to torch tensor if provided\n",
    "        self.num_samples = self.S.shape[0]\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Action space: assume discrete actions for simplicity\n",
    "        self.action_dim = self.A.shape[1]\n",
    "        self.action_space = spaces.Box(low=-1e6, high=1e6, shape=(self.action_dim,))\n",
    "        \n",
    "        # Observation space: separate S and Y with specific labels\n",
    "        self.obs_dim_S = self.S.shape[1]\n",
    "        # observation_space = {}\n",
    "        # for i in range(obs_dim_S):\n",
    "        #     observation_space[f'S_{i+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        \n",
    "        # self.observation_space = spaces.Dict(observation_space)\n",
    "        self.observation_space = spaces.Box(low=-torch.inf, high=torch.inf, shape=(self.obs_dim_S,))\n",
    "        #print(self.observation_space)\n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply action and get next state and reward\n",
    "        next_state = self._get_observation()\n",
    "        action = torch.from_numpy(action.astype(np.float64) )\n",
    "        reward = self._calculate_reward(action)\n",
    "        #print(\"next_state\",next_state.shape)\n",
    "        #print(\"action\",action.shape)\n",
    "        #print(\"reward\",reward.shape)\n",
    "        #print(\"self.current_step\",self.current_step)\n",
    "        done = (self.current_step == self.obs_dim_S - 1)\n",
    "        #print(\"done\",done)\n",
    "        self.current_step = self.current_step + 1\n",
    "        #print(\"self.current_step\",self.current_step)\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # obs = {}\n",
    "        # for i in range(self.S.shape[1]):\n",
    "        #     obs[f'S_{i+1}'] = np.array([self.S[self.current_step, i]])\n",
    "        obs = self.S[self.current_step, :]\n",
    "        #print(\"obs.shape\",obs.shape)\n",
    "        return obs\n",
    "    \n",
    "    def _calculate_reward(self, action):\n",
    "        # Example: using a reward matrix R based on expected Y values\n",
    "        if self.R is not None:\n",
    "            # 将 A 转换成 124 x 1 的矩阵以便与 SA 进行距离计算\n",
    "            #print(action.shape)\n",
    "            #print(self.R.shape)\n",
    "            # 現在這只是單純計算 action 離 RWARD 的值哪個比較接近\n",
    "            # 理論上? RWARD 的 action 維度要離散化，多少區間內給多少分數\n",
    "            # 最後把該動作的rewar轉成action的矩陣\n",
    "            # 但是別人怎麼做呢?\n",
    "            #要看PAPER\n",
    "            distances = torch.cdist(action.reshape(-1,1), self.R[:,self.current_step].reshape(-1,1), p=2)\n",
    "\n",
    "            # Step 3: 找到最小距离对应的索引\n",
    "            # np.argmin(distances, axis=1) 会返回每行中最小值的索引\n",
    "            nearest_indices = torch.argmin(distances, axis=1)\n",
    "            # Step 4: 根据索引找到 SA 中对应的值\n",
    "            # 使用这些索引从 SA 中提取对应的值\n",
    "            reward = self.R[nearest_indices,self.current_step].mean() # 整體動作的REWARD，但要怎麼取? MAX ? MEIDAN ? AVERAGE ?\n",
    "            #print(reward.shape)\n",
    "\n",
    "        else:\n",
    "            #print(self.A.shape,action.shape)\n",
    "            # Default reward function (replace with your own logic)\n",
    "            # 這邊有問題\n",
    "            reward = torch.sum(self.A[self.current_step] * action)\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "state_dim = 29\n",
    "action_dim = 124\n",
    "S = np.random.randn(15000, state_dim)\n",
    "S = torch.from_numpy(S)\n",
    "A = np.random.randn(5000, action_dim)\n",
    "#A = np.random.randint(low=1, high=discrete_action_max, size=(1500, 45))\n",
    "A = torch.from_numpy(A)\n",
    "# Example reward matrix R based on Y (replace with your own logic)\n",
    "R = np.random.uniform(low=-100,high=100,size=(action_dim, state_dim))  # Example: random R matrix for 45 actions and 23 Y features\n",
    "R = torch.from_numpy(R)\n",
    "# Create the custom environment\n",
    "env = CustomEnv(S=S, A=A, R=R)\n",
    "\n",
    "# Wrap the environment in a vectorized form\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.evaluation import evaluate_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import PPO,A2C\n",
    "# model = A2C(\"MultiInputPolicy\", vec_env, verbose=1)\n",
    "# pip install sb3-contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs = dict()\n",
    "for i in range(S.shape[1]):\n",
    "    new_obs[f'S_{i+1}'] = np.array([np.random.randn(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  2,  7, 15,  6, 11, 17,  4, 19])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19.,  5., 19., 11.,  5., 19.,  4., 19., 19.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from scipy.spatial.distance import cdist\n",
    "\n",
    "# # Step 1: 生成随机数据\n",
    "# np.random.seed(42)  # 设置随机种子以保证结果可重复\n",
    "# step = 2\n",
    "# A = np.random.randint(1,20, (9,3)).astype(np.float64)  # SA 是 124 x 1 的矩阵\n",
    "# action = np.random.randint(1,30,9).astype(np.float64)    # A 是 124 的一维数组\n",
    "\n",
    "\n",
    "\n",
    "# A = torch.from_numpy(A)\n",
    "# action = torch.from_numpy(action)\n",
    "\n",
    "# # Step 2: 计算 A 和 SA 之间的距离\n",
    "# # 将 A 转换成 124 x 1 的矩阵以便与 SA 进行距离计算\n",
    "# distances = torch.cdist(action.reshape(-1,1), A[:,step].reshape(-1,1), p=1)\n",
    "\n",
    "# # Step 3: 找到最小距离对应的索引\n",
    "# # np.argmin(distances, axis=1) 会返回每行中最小值的索引\n",
    "# nearest_indices = np.argmin(distances, axis=1)\n",
    "\n",
    "# # Step 4: 根据索引找到 SA 中对应的值\n",
    "# # 使用这些索引从 SA 中提取对应的值\n",
    "# nearest_values = A[nearest_indices,step]\n",
    "\n",
    "# # 打印结果\n",
    "# print(nearest_values.flatten())  # 展平数组以便更清晰地查看结果\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43maction\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'action' is not defined"
     ]
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0288, dtype=torch.float64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R[:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19.,  5., 19., 11.,  5., 19.,  4., 19., 19.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.080909518537936"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv at 0x1edffdec190>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '，' (U+FF0C) (3618413405.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[31], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    我想透過index，依照A的值找出，SA的最接近的值，並且回傳該值。假設 A的index 是2，他最接近SA 的index是9，其值是6，那我就依照A\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '，' (U+FF0C)\n"
     ]
    }
   ],
   "source": [
    "SA = np.random.randn(124, 1)\n",
    "A = np.random.randn(124,)\n",
    "我想透過index，依照A的值找出，SA的最接近的值，並且回傳該值。假設 \n",
    "\n",
    "A的index 是1，他最接近SA 的index是9，其值是6，\n",
    "A的index 是2，他最接近SA 的index是9，其值是6，\n",
    "A的index 是3，他最接近SA 的index是2，其值是8，\n",
    "那我就依照A的格式，回傳[6,6,8,.....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sb3_contrib.trpo.trpo.TRPO'>\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29       |\n",
      "|    ep_rew_mean     | -5.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 1050     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 29       |\n",
      "|    ep_rew_mean            | -4.99    |\n",
      "| time/                     |          |\n",
      "|    fps                    | 920      |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 4        |\n",
      "|    total_timesteps        | 4096     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | -0.449   |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.000999 |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 1        |\n",
      "|    policy_objective       | 0.629    |\n",
      "|    std                    | 1        |\n",
      "|    value_loss             | 0.591    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 29       |\n",
      "|    ep_rew_mean            | -4.9     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 915      |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 6        |\n",
      "|    total_timesteps        | 6144     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.734    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00103  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 2        |\n",
      "|    policy_objective       | 0.58     |\n",
      "|    std                    | 1        |\n",
      "|    value_loss             | 0.184    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 29       |\n",
      "|    ep_rew_mean            | -4.5     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 901      |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 9        |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.896    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00108  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 3        |\n",
      "|    policy_objective       | 0.647    |\n",
      "|    std                    | 1        |\n",
      "|    value_loss             | 0.144    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 29       |\n",
      "|    ep_rew_mean            | -3.98    |\n",
      "| time/                     |          |\n",
      "|    fps                    | 888      |\n",
      "|    iterations             | 5        |\n",
      "|    time_elapsed           | 11       |\n",
      "|    total_timesteps        | 10240    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.897    |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00106  |\n",
      "|    learning_rate          | 0.001    |\n",
      "|    n_updates              | 4        |\n",
      "|    policy_objective       | 0.79     |\n",
      "|    std                    | 1.01     |\n",
      "|    value_loss             | 0.167    |\n",
      "----------------------------------------\n",
      "29\n",
      "(1, 124)\n"
     ]
    }
   ],
   "source": [
    "from sb3_contrib import ARS # 機率輸出不會改變\n",
    "from stable_baselines3 import A2C # 機率輸出會改變\n",
    "from stable_baselines3 import DDPG # 輸出機率和動作長依樣?\n",
    "from stable_baselines3 import HER # 這要搭配其他使用\n",
    "from stable_baselines3 import PPO # 機率輸出會改變\n",
    "from stable_baselines3 import SAC # 機率輸出會改變\n",
    "from stable_baselines3 import TD3 # 輸出機率和動作長依樣?\n",
    "from sb3_contrib import TRPO\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib import RecurrentPPO # MlpLstmPolicy\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "when you add deterministic=True, all the predicted actions will be always determined by the maximum probability, instead of the probability by itself.\n",
    "\n",
    "Just to give you an example, let's suppose you have the following probabilities:\n",
    "\n",
    "25% of action A\n",
    "75% of action B\n",
    "If you don't use the deterministic=True, the model will use those probabilities to return a prediction.\n",
    "If you use deterministic=True, the model is going to return always action B.\n",
    "\n",
    "\"\"\"\n",
    "for RL_model in [TRPO]:#ARS,A2C,DDPG,PPO,SAC,TD3,TRPO,TQC\n",
    "    # Example data (replace with your actual data)\n",
    "    state_dim = 29\n",
    "    action_dim = 124\n",
    "    S = np.random.randn(15000, state_dim)\n",
    "\n",
    "    A = np.random.randn(5000, action_dim)\n",
    "    #A = np.random.randint(low=1, high=discrete_action_max, size=(1500, 45))\n",
    "\n",
    "    # Example reward matrix R based on Y (replace with your own logic)\n",
    "    R = np.random.uniform(low=-100,high=100,size=(action_dim, state_dim))  # Example: random R matrix for 45 actions and 23 Y features\n",
    "\n",
    "    S = torch.from_numpy(S)\n",
    "    A = torch.from_numpy(A)\n",
    "    R = torch.from_numpy(R)\n",
    "    # Create the custom environment\n",
    "    env = CustomEnv(S=S, A=A, R=R)\n",
    "\n",
    "    # Wrap the environment in a vectorized form\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    print(RL_model)\n",
    "    # Initialize and train the RL agent (using PPO as an example)\n",
    "    model = RL_model(\"MlpPolicy\", env, verbose=1) # MultiInputPolicy\n",
    "    model.learn(total_timesteps=10000)\n",
    "    # new_obs = dict()\n",
    "    # for i in range(S.shape[1]):\n",
    "    #     new_obs[f'S_{i+1}'] = np.array([np.random.randn(1)])\n",
    "    new_obs = np.random.randn(1,S.shape[1])\n",
    "    new_obs = torch.from_numpy(new_obs)\n",
    "    print(new_obs.shape[1])\n",
    "    action, _ = model.predict(new_obs,deterministic=True) # deterministic=False輸出action的機率，true則輸出action的值\n",
    "    print(action.shape)\n",
    "\n",
    "    # r = None\n",
    "    # best_action = None\n",
    "    # for i in range(100):  # Test for 10 steps\n",
    "    #     action, _ = model.predict(new_obs)\n",
    "    #     obs, reward, done, info = env.step(action)\n",
    "    #     if r == None:\n",
    "    #         r = reward\n",
    "    #     elif r < reward:\n",
    "    #         r = reward\n",
    "    #         best_action = action\n",
    "\n",
    "    #         print(f\"Action: {best_action}, Reward: {r}, {i}\")\n",
    "# Save and reload the model (optional)\n",
    "#model.save(\"rl_model\")\n",
    "# model = PPO.load(\"rl_model\")\n",
    "\n",
    "# Test the trained model\n",
    "\n",
    "\n",
    "# # 官網標準寫法\n",
    "# env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# model.learn(total_timesteps=10_000)\n",
    "\n",
    "# vec_env = model.get_env()\n",
    "# obs = vec_env.reset()\n",
    "# for i in range(1000):\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, info = vec_env.step(action)\n",
    "#     vec_env.render()\n",
    "#     # VecEnv resets automatically\n",
    "#     # if done:\n",
    "#     #   obs = env.reset()\n",
    "\n",
    "# env.close()\n",
    "# #\n",
    "\n",
    "\n",
    "# # 這邊是觀察內部\n",
    "# for i_episode in range(5): #how many episodes you want to run\n",
    "#     observation = env.reset() #reset() returns initial observation\n",
    "#     for t in range(200):\n",
    "#         env.render()\n",
    "#         print(observation)\n",
    "#         action = env.action_space.sample()\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         if done:\n",
    "#             print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "#             break\n",
    "\n",
    "# # 這邊則是進行預測\n",
    "# for _ in range(10):  # Test for 10 steps\n",
    "#     action, _ = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     print(f\"Action: {action}, Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7871,  0.9039,  1.5504, -0.4691, -0.0908, -0.7343, -1.6763, -0.6661,\n",
       "          1.0916,  0.9902,  1.9747,  0.9964,  1.8461, -1.0884, -1.3171,  1.7140,\n",
       "          0.9146, -0.2111, -0.5123, -0.2285,  0.0553,  0.4445,  0.0920,  0.0404,\n",
       "         -0.8069,  0.6070, -0.1988, -0.2639, -0.6756]], dtype=torch.float64)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.10429512,  0.05155674, -0.0768303 ,  0.01281125,  0.08048081,\n",
       "          0.06557958,  0.09027342,  0.08771456,  0.05187207,  0.19846253,\n",
       "          0.0664503 ,  0.03900621,  0.09570971,  0.03734688,  0.06144251,\n",
       "          0.07476458,  0.07706401,  0.08076768,  0.1373069 ,  0.06146199,\n",
       "          0.12250422,  0.01001924, -0.02432631,  0.11008753,  0.0784854 ,\n",
       "          0.0826609 ,  0.05172022,  0.07447641,  0.08007077,  0.06980799,\n",
       "          0.02578136, -0.05455348, -0.02192363, -0.01564816,  0.0992328 ,\n",
       "         -0.03860246,  0.00382761,  0.12530567, -0.02701663, -0.07405656,\n",
       "          0.12358637,  0.01014275, -0.02857167, -0.08359521,  0.02672946,\n",
       "          0.02346602,  0.11189777,  0.028679  , -0.00456877,  0.0316258 ,\n",
       "          0.03485448,  0.09060949,  0.10247485,  0.05347365,  0.04779212,\n",
       "          0.10448733,  0.14473774,  0.05419446,  0.10553341,  0.1627338 ,\n",
       "          0.07155152,  0.02600072,  0.14005902,  0.07336207,  0.07702881,\n",
       "          0.06494298,  0.11994858,  0.06660507,  0.22068508,  0.09841689,\n",
       "          0.02169234,  0.19368662,  0.10729146,  0.03242978,  0.03014574,\n",
       "          0.11558938, -0.08022583,  0.04498315,  0.16768214, -0.0457701 ,\n",
       "         -0.05388057,  0.01517851,  0.18463928,  0.03333288,  0.14272764,\n",
       "          0.06733267, -0.00090824,  0.12634999,  0.09708978,  0.09194849,\n",
       "          0.13035801, -0.0272175 ,  0.18346822,  0.06638939,  0.08899145,\n",
       "          0.28345668,  0.06569174,  0.10563944,  0.1166729 ,  0.05341444,\n",
       "          0.07530696,  0.19575822, -0.02107483,  0.07300003,  0.10533256,\n",
       "          0.03180702, -0.07956112,  0.12303638,  0.07267784,  0.17171384,\n",
       "          0.15123145,  0.05336631,  0.01451873,  0.12525716,  0.11813435,\n",
       "          0.03759789,  0.03352744,  0.05173347,  0.21474162,  0.07382204,\n",
       "          0.00571257,  0.02524425, -0.01190102,  0.133028  ]],\n",
       "       dtype=float32),\n",
       " None)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action, _ = model.predict(new_obs,deterministic=True)\n",
    "model.predict(new_obs,deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44271225, -0.25945538, -0.6767864 , -1.3456639 ,  0.4583108 ,\n",
       "        -0.8694251 ,  0.5728355 ,  0.250616  , -1.7483984 ,  0.71500456,\n",
       "         1.289273  ,  0.1096137 , -0.17016464, -0.14660917,  0.2514745 ,\n",
       "         1.5701727 , -0.06559735,  0.405105  , -0.40092352,  0.1497918 ,\n",
       "        -0.1611617 , -0.03163506,  0.8675919 , -0.31376326, -1.0028843 ,\n",
       "         0.8298509 ,  0.6134912 ,  0.97067845,  0.25776014,  0.31684613,\n",
       "        -0.6789923 , -3.0055926 , -0.02818955, -0.10384521, -1.6048154 ,\n",
       "         0.55964726, -1.0570384 ,  0.92560476, -0.30416143,  0.18905807,\n",
       "        -0.8934595 , -0.40526015,  1.111753  , -0.09417639,  1.4580442 ,\n",
       "         0.886101  , -1.0348625 ,  0.14082423,  1.2454921 ,  0.68530387,\n",
       "         1.2053729 ,  0.7213781 ,  0.71994   , -1.1614175 ,  1.0004908 ,\n",
       "         0.5783862 ,  0.06592529,  0.11703116,  0.54999053, -0.77706444,\n",
       "        -0.35461938,  0.8117135 ,  1.1627471 ,  0.5165836 ,  1.770658  ,\n",
       "        -0.82393616,  0.80179393,  0.9695358 ,  0.66447645,  0.6472105 ,\n",
       "         1.6140834 , -1.3509773 , -0.22791645,  0.52587515,  1.8424789 ,\n",
       "         2.0603578 ,  0.73877984, -0.49977863,  0.801227  ,  0.25063354,\n",
       "         0.27761957,  1.4111291 ,  0.91217875, -0.1892049 ,  0.7323338 ,\n",
       "         0.63657933, -0.9139616 , -1.5036926 , -2.6177313 , -0.8316638 ,\n",
       "        -0.38668266,  1.1074052 ,  0.9387451 ,  0.6299967 ,  0.32612398,\n",
       "        -0.9003711 , -0.06595832, -0.6044726 , -0.8192651 ,  0.5295975 ,\n",
       "        -1.7469251 , -1.0468599 ,  0.82971066, -1.4158378 ,  0.099641  ,\n",
       "        -2.2608097 , -1.3416742 ,  0.68439627, -1.3425015 , -1.2293526 ,\n",
       "         1.0210984 , -0.691639  ,  0.5370077 ,  0.2919166 ,  0.03308535,\n",
       "         1.9675612 , -0.0909664 ,  0.6750918 ,  0.42792547, -0.81251454,\n",
       "        -0.18440177, -1.5222864 ,  0.53050125, -0.41027305]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action, _ = model.predict(new_obs,deterministic=False)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rewards = float('-inf')\n",
    "best_action = None\n",
    "\n",
    "\n",
    "while True:\n",
    "    action, _states = model.predict(new_obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    if not dones:  # 使用適當的布林條件檢查\n",
    "        if rewards > best_rewards:\n",
    "            best_rewards = rewards\n",
    "            best_action = action\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10429512,  0.05155674, -0.0768303 ,  0.01281125,  0.08048081,\n",
       "         0.06557958,  0.09027342,  0.08771456,  0.05187207,  0.19846253,\n",
       "         0.0664503 ,  0.03900621,  0.09570971,  0.03734688,  0.06144251,\n",
       "         0.07476458,  0.07706401,  0.08076768,  0.1373069 ,  0.06146199,\n",
       "         0.12250422,  0.01001924, -0.02432631,  0.11008753,  0.0784854 ,\n",
       "         0.0826609 ,  0.05172022,  0.07447641,  0.08007077,  0.06980799,\n",
       "         0.02578136, -0.05455348, -0.02192363, -0.01564816,  0.0992328 ,\n",
       "        -0.03860246,  0.00382761,  0.12530567, -0.02701663, -0.07405656,\n",
       "         0.12358637,  0.01014275, -0.02857167, -0.08359521,  0.02672946,\n",
       "         0.02346602,  0.11189777,  0.028679  , -0.00456877,  0.0316258 ,\n",
       "         0.03485448,  0.09060949,  0.10247485,  0.05347365,  0.04779212,\n",
       "         0.10448733,  0.14473774,  0.05419446,  0.10553341,  0.1627338 ,\n",
       "         0.07155152,  0.02600072,  0.14005902,  0.07336207,  0.07702881,\n",
       "         0.06494298,  0.11994858,  0.06660507,  0.22068508,  0.09841689,\n",
       "         0.02169234,  0.19368662,  0.10729146,  0.03242978,  0.03014574,\n",
       "         0.11558938, -0.08022583,  0.04498315,  0.16768214, -0.0457701 ,\n",
       "        -0.05388057,  0.01517851,  0.18463928,  0.03333288,  0.14272764,\n",
       "         0.06733267, -0.00090824,  0.12634999,  0.09708978,  0.09194849,\n",
       "         0.13035801, -0.0272175 ,  0.18346822,  0.06638939,  0.08899145,\n",
       "         0.28345668,  0.06569174,  0.10563944,  0.1166729 ,  0.05341444,\n",
       "         0.07530696,  0.19575822, -0.02107483,  0.07300003,  0.10533256,\n",
       "         0.03180702, -0.07956112,  0.12303638,  0.07267784,  0.17171384,\n",
       "         0.15123145,  0.05336631,  0.01451873,  0.12525716,  0.11813435,\n",
       "         0.03759789,  0.03352744,  0.05173347,  0.21474162,  0.07382204,\n",
       "         0.00571257,  0.02524425, -0.01190102,  0.133028  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sb3_contrib.ppo_recurrent.ppo_recurrent.RecurrentPPO'>\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29       |\n",
      "|    ep_rew_mean     | 0.692    |\n",
      "| time/              |          |\n",
      "|    fps             | 410      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 128      |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 0.611       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 174         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 256         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040017627 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | -0.111      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0285      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0986     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.602       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 384         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031642165 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0638     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 0.595      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 122        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 512        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04251341 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.706      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0714    |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0753    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.186      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 0.546       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 640         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040757507 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0663     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0733     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 0.535       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 768         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044848237 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.045      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.194       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 0.591      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 896        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04024813 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.905      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.107     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0711    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.0548     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 0.595      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 1024       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06165795 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.894      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.109     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0817    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.0713     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 0.603      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 111        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 1152       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05884534 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.856      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0939    |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0765    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.0898     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 1280        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077518605 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0667     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0875     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 0.776      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 1408       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08008591 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.856      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.104     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0852    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.0848     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 0.88       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 1536       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07348034 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.877      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.113     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0813    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.0867     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 0.941      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 1664       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06755667 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.788      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.079     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0765    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.138      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 0.999      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 1792       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08256068 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.705      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0769    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0831    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.141      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 1.05       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 17         |\n",
      "|    total_timesteps      | 1920       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06982945 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.83       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.103     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0773    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.0794     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075234115 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0592     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0793     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 1.2        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 2176       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08353467 |\n",
      "|    clip_fraction        | 0.327      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.771      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.073     |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0861    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.167      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 1.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 2304        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088494435 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0997     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0882     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 1.29        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 2432        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092307426 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0997     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0855     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.0794      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 1.37       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 106        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 23         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07251215 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.771      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0866    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0871    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2688        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086033724 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0901     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0888     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 1.46       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2816       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10772409 |\n",
      "|    clip_fraction        | 0.416      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.809      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.084     |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0873    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.143      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 1.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2944        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090904824 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0567     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0826     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 1.59       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 28         |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08436097 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.667      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0755    |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0862    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.161      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 1.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 3200       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08742833 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.754      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0745    |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0866    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 1.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 3328       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09272221 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.77       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0968    |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0939    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 2          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 31         |\n",
      "|    total_timesteps      | 3456       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11615244 |\n",
      "|    clip_fraction        | 0.436      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.805      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0958    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0925    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 2.13       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 33         |\n",
      "|    total_timesteps      | 3584       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09452446 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.836      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.095     |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0879    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 2.32       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 34         |\n",
      "|    total_timesteps      | 3712       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09966999 |\n",
      "|    clip_fraction        | 0.395      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.842      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.109     |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0936    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.094      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 2.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 3840        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106652655 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0909     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0919     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 2.55       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 36         |\n",
      "|    total_timesteps      | 3968       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10721584 |\n",
      "|    clip_fraction        | 0.411      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.855      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.108     |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.09      |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.0968     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 2.71        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108177565 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0518     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0948     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.22        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 2.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 4224       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11425466 |\n",
      "|    clip_fraction        | 0.426      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.838      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0854    |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0935    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 3          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 39         |\n",
      "|    total_timesteps      | 4352       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10792144 |\n",
      "|    clip_fraction        | 0.416      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.89       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.113     |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.0931    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29        |\n",
      "|    ep_rew_mean          | 3.12      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 109       |\n",
      "|    iterations           | 35        |\n",
      "|    time_elapsed         | 40        |\n",
      "|    total_timesteps      | 4480      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1167519 |\n",
      "|    clip_fraction        | 0.42      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -176      |\n",
      "|    explained_variance   | 0.887     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 340       |\n",
      "|    policy_gradient_loss | -0.0893   |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 0.0933    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 3.26       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 41         |\n",
      "|    total_timesteps      | 4608       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10233411 |\n",
      "|    clip_fraction        | 0.394      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.805      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0897    |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.0883    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.14       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 3.44       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 43         |\n",
      "|    total_timesteps      | 4736       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10707023 |\n",
      "|    clip_fraction        | 0.434      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.841      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.076     |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.095     |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 3.61       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 44         |\n",
      "|    total_timesteps      | 4864       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10331602 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.886      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.077     |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0804    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.13       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 3.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 4992        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103433274 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.118      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0935     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 3.95       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 46         |\n",
      "|    total_timesteps      | 5120       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09245139 |\n",
      "|    clip_fraction        | 0.406      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.91       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0978    |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.0899    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.124      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29        |\n",
      "|    ep_rew_mean          | 4.1       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 110       |\n",
      "|    iterations           | 41        |\n",
      "|    time_elapsed         | 47        |\n",
      "|    total_timesteps      | 5248      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1184808 |\n",
      "|    clip_fraction        | 0.407     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -176      |\n",
      "|    explained_variance   | 0.922     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.106    |\n",
      "|    n_updates            | 400       |\n",
      "|    policy_gradient_loss | -0.0933   |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 0.101     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 4.31       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 48         |\n",
      "|    total_timesteps      | 5376       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13767688 |\n",
      "|    clip_fraction        | 0.426      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.869      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0791    |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | -0.0948    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.134      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 4.46       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 49         |\n",
      "|    total_timesteps      | 5504       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11948596 |\n",
      "|    clip_fraction        | 0.478      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.885      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0655    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0882    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 4.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 5632        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108780675 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0872     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 0.312       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 4.9        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 111        |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 51         |\n",
      "|    total_timesteps      | 5760       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09800591 |\n",
      "|    clip_fraction        | 0.395      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.926      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0889    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.088     |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.139      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 5.14       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 111        |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 52         |\n",
      "|    total_timesteps      | 5888       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11500695 |\n",
      "|    clip_fraction        | 0.42       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.837      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0172    |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.0873    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.254      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 5.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 6016        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112247914 |\n",
      "|    clip_fraction        | 0.441       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.081      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0847     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 5.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124467224 |\n",
      "|    clip_fraction        | 0.466       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0846     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.088      |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 5.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 111        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 56         |\n",
      "|    total_timesteps      | 6272       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11003886 |\n",
      "|    clip_fraction        | 0.4        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.93       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.1       |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0942    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 5.97       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 111        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 57         |\n",
      "|    total_timesteps      | 6400       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13031621 |\n",
      "|    clip_fraction        | 0.435      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.908      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0429    |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | -0.0798    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.208      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 6.24        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 6528        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116490506 |\n",
      "|    clip_fraction        | 0.441       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.124      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0961     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.101       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 6.46       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 59         |\n",
      "|    total_timesteps      | 6656       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12005431 |\n",
      "|    clip_fraction        | 0.455      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.938      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0544    |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.0964    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.223      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 6.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 6784        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.120922625 |\n",
      "|    clip_fraction        | 0.448       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0828     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0878     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29        |\n",
      "|    ep_rew_mean          | 7         |\n",
      "| time/                   |           |\n",
      "|    fps                  | 112       |\n",
      "|    iterations           | 54        |\n",
      "|    time_elapsed         | 61        |\n",
      "|    total_timesteps      | 6912      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1305687 |\n",
      "|    clip_fraction        | 0.413     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -176      |\n",
      "|    explained_variance   | 0.973     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.107    |\n",
      "|    n_updates            | 530       |\n",
      "|    policy_gradient_loss | -0.0893   |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 0.0882    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 7.25       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 62         |\n",
      "|    total_timesteps      | 7040       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15925384 |\n",
      "|    clip_fraction        | 0.478      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.946      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0552    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0949    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.206      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 7.53       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 63         |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12727049 |\n",
      "|    clip_fraction        | 0.441      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.12      |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.094     |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.0828     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 7.75       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 57         |\n",
      "|    time_elapsed         | 64         |\n",
      "|    total_timesteps      | 7296       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11152974 |\n",
      "|    clip_fraction        | 0.423      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0937    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0863    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29        |\n",
      "|    ep_rew_mean          | 8.07      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 112       |\n",
      "|    iterations           | 58        |\n",
      "|    time_elapsed         | 65        |\n",
      "|    total_timesteps      | 7424      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1840967 |\n",
      "|    clip_fraction        | 0.5       |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -176      |\n",
      "|    explained_variance   | 0.969     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.091    |\n",
      "|    n_updates            | 570       |\n",
      "|    policy_gradient_loss | -0.093    |\n",
      "|    std                  | 0.999     |\n",
      "|    value_loss           | 0.125     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 8.32       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 59         |\n",
      "|    time_elapsed         | 67         |\n",
      "|    total_timesteps      | 7552       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21831433 |\n",
      "|    clip_fraction        | 0.498      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0876    |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0933    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.168      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 8.58       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 68         |\n",
      "|    total_timesteps      | 7680       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13958709 |\n",
      "|    clip_fraction        | 0.445      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0874    |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0868    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 8.88       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 69         |\n",
      "|    total_timesteps      | 7808       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12545042 |\n",
      "|    clip_fraction        | 0.441      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0504    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0799    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 9.15       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 70         |\n",
      "|    total_timesteps      | 7936       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18402287 |\n",
      "|    clip_fraction        | 0.534      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0729    |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.0853    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.147      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 9.5        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 63         |\n",
      "|    time_elapsed         | 71         |\n",
      "|    total_timesteps      | 8064       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13384135 |\n",
      "|    clip_fraction        | 0.48       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0916    |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.087     |\n",
      "|    std                  | 0.998      |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 9.79        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.118439406 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -176        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0744     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0826     |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 10.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 73         |\n",
      "|    total_timesteps      | 8320       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14206153 |\n",
      "|    clip_fraction        | 0.52       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0808    |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0786    |\n",
      "|    std                  | 0.998      |\n",
      "|    value_loss           | 0.0981     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 10.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 66         |\n",
      "|    time_elapsed         | 75         |\n",
      "|    total_timesteps      | 8448       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18013674 |\n",
      "|    clip_fraction        | 0.559      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0983    |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | -0.099     |\n",
      "|    std                  | 0.998      |\n",
      "|    value_loss           | 0.116      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 10.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 67         |\n",
      "|    time_elapsed         | 76         |\n",
      "|    total_timesteps      | 8576       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14344013 |\n",
      "|    clip_fraction        | 0.462      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.107     |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0874    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.0953     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 11         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 68         |\n",
      "|    time_elapsed         | 77         |\n",
      "|    total_timesteps      | 8704       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16201104 |\n",
      "|    clip_fraction        | 0.501      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0534    |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | -0.0898    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.238      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 11.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 69         |\n",
      "|    time_elapsed         | 78         |\n",
      "|    total_timesteps      | 8832       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14927742 |\n",
      "|    clip_fraction        | 0.469      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0734    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0906    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.191      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 11.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 79         |\n",
      "|    total_timesteps      | 8960       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14596793 |\n",
      "|    clip_fraction        | 0.463      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0899    |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.0865    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 11.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 80         |\n",
      "|    total_timesteps      | 9088       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14807293 |\n",
      "|    clip_fraction        | 0.507      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0846    |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0864    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 12.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 72         |\n",
      "|    time_elapsed         | 81         |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16152765 |\n",
      "|    clip_fraction        | 0.463      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0624    |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | -0.0807    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.168      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 12.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 82         |\n",
      "|    total_timesteps      | 9344       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15224034 |\n",
      "|    clip_fraction        | 0.462      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0648    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0983    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.213      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 12.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 83         |\n",
      "|    total_timesteps      | 9472       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15011062 |\n",
      "|    clip_fraction        | 0.512      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0669    |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0911    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.167      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29        |\n",
      "|    ep_rew_mean          | 13.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 113       |\n",
      "|    iterations           | 75        |\n",
      "|    time_elapsed         | 84        |\n",
      "|    total_timesteps      | 9600      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1476998 |\n",
      "|    clip_fraction        | 0.463     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -176      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.107    |\n",
      "|    n_updates            | 740       |\n",
      "|    policy_gradient_loss | -0.0949   |\n",
      "|    std                  | 0.999     |\n",
      "|    value_loss           | 0.121     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 29       |\n",
      "|    ep_rew_mean          | 13.4     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 113      |\n",
      "|    iterations           | 76       |\n",
      "|    time_elapsed         | 86       |\n",
      "|    total_timesteps      | 9728     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.17478  |\n",
      "|    clip_fraction        | 0.471    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -176     |\n",
      "|    explained_variance   | 0.987    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.0619  |\n",
      "|    n_updates            | 750      |\n",
      "|    policy_gradient_loss | -0.0897  |\n",
      "|    std                  | 0.999    |\n",
      "|    value_loss           | 0.25     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 13.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 77         |\n",
      "|    time_elapsed         | 87         |\n",
      "|    total_timesteps      | 9856       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17168193 |\n",
      "|    clip_fraction        | 0.448      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0337    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0855    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.245      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 14         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 88         |\n",
      "|    total_timesteps      | 9984       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14310656 |\n",
      "|    clip_fraction        | 0.5        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0747    |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.0828    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.147      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29         |\n",
      "|    ep_rew_mean          | 14.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 79         |\n",
      "|    time_elapsed         | 89         |\n",
      "|    total_timesteps      | 10112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15494879 |\n",
      "|    clip_fraction        | 0.48       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -176       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0696    |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0896    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "29\n",
      "(1, 124)\n"
     ]
    }
   ],
   "source": [
    "print(RecurrentPPO) # \n",
    "# Initialize and train the RL agent (using PPO as an example)\n",
    "model = RecurrentPPO(\"MlpLstmPolicy\", env, verbose=1) # MultiInputPolicy\n",
    "model.learn(total_timesteps=10000)\n",
    "# new_obs = dict()\n",
    "# for i in range(S.shape[1]):\n",
    "#     new_obs[f'S_{i+1}'] = np.array([np.random.randn(1)])\n",
    "new_obs = np.random.randn(1,S.shape[1])\n",
    "new_obs = torch.from_numpy(new_obs)\n",
    "print(new_obs.shape[1])\n",
    "action, _ = model.predict(new_obs,deterministic=True)\n",
    "print(action.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.64193556e-01, -1.30733192e-01,  1.69350788e-01,\n",
       "         1.03534460e-01,  3.47802222e-01,  3.11723407e-02,\n",
       "         3.89672905e-01,  2.71234572e-01,  2.34658375e-01,\n",
       "         2.57128745e-01,  2.93758839e-01,  1.74956918e-01,\n",
       "         2.02014759e-01,  1.64778739e-01,  1.39585614e-01,\n",
       "         1.68799058e-01,  1.76782727e-01,  2.54211605e-01,\n",
       "         9.34491456e-02,  4.07167137e-01,  7.59055242e-02,\n",
       "         1.25501037e-01,  1.10017411e-01,  1.11715980e-01,\n",
       "         2.76867092e-01,  2.01592118e-01,  1.05613992e-01,\n",
       "         2.06894234e-01,  4.40128595e-02,  3.04735243e-01,\n",
       "         3.06183755e-01,  1.49329588e-01,  1.02381542e-01,\n",
       "         1.18145354e-01,  1.44413441e-01, -1.48292914e-01,\n",
       "         3.23862582e-01,  2.95034111e-01,  2.57538795e-01,\n",
       "         4.37002955e-03, -9.48045179e-02, -5.39447106e-02,\n",
       "         1.57504901e-01,  1.21384166e-01,  3.82774442e-01,\n",
       "         9.79488343e-02,  2.69628525e-01,  1.42206281e-01,\n",
       "         1.49647385e-01,  3.87357295e-01,  1.27654716e-01,\n",
       "         9.82445702e-02,  4.42977846e-02,  4.58466172e-01,\n",
       "         3.12110409e-02,  3.06115240e-01,  1.90505803e-01,\n",
       "         3.60386878e-01,  1.32288128e-01,  2.06803396e-01,\n",
       "         1.47585168e-01, -3.37041765e-02,  4.10804749e-01,\n",
       "         1.25094235e-01,  1.13658875e-01,  1.91498995e-01,\n",
       "         8.53940472e-02,  1.33057386e-01,  2.28858963e-01,\n",
       "         2.89391279e-01,  2.64640898e-01,  1.88194990e-01,\n",
       "         3.28635797e-04,  1.77090034e-01,  7.05443770e-02,\n",
       "         6.86549619e-02,  2.74937242e-01,  2.04552501e-01,\n",
       "         9.17624608e-02,  1.59957692e-01,  1.20399654e-01,\n",
       "         1.37661442e-01,  3.27163674e-02,  2.36731142e-01,\n",
       "         2.92156011e-01,  2.60717481e-01,  1.18844479e-01,\n",
       "         9.02970359e-02,  1.19971290e-01,  6.10205680e-02,\n",
       "         2.88807601e-01,  3.68457675e-01,  2.46279493e-01,\n",
       "         4.12310839e-01,  2.35942751e-01,  2.16932505e-01,\n",
       "         1.37234136e-01,  2.75980711e-01,  2.00701192e-01,\n",
       "         1.48986995e-01,  9.96666923e-02,  1.64580986e-01,\n",
       "         9.43799019e-02,  1.30788255e-02,  2.16834545e-01,\n",
       "         3.07134360e-01,  5.40815555e-02,  2.61932313e-01,\n",
       "         2.62905449e-01,  3.34283769e-01,  1.39833197e-01,\n",
       "         1.06613956e-01,  1.54706895e-01,  3.70798409e-01,\n",
       "         1.74763501e-01,  2.58574754e-01,  2.10150316e-01,\n",
       "         3.26198101e-01,  3.70415784e-02,  1.21050052e-01,\n",
       "         1.92492247e-01,  2.70595044e-01,  2.08803028e-01,\n",
       "         1.83521986e-01]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action, _ = model.predict(new_obs,deterministic=True)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m state_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m29\u001b[39m\n\u001b[0;32m      2\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m124\u001b[39m\n\u001b[1;32m----> 3\u001b[0m S \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m15000\u001b[39m, state_dim)\n\u001b[0;32m      5\u001b[0m A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m5000\u001b[39m, action_dim)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#A = np.random.randint(low=1, high=discrete_action_max, size=(1500, 45))\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Example reward matrix R based on Y (replace with your own logic)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "state_dim = 29\n",
    "action_dim = 124\n",
    "S = np.random.randn(15000, state_dim)\n",
    "\n",
    "A = np.random.randn(5000, action_dim)\n",
    "#A = np.random.randint(low=1, high=discrete_action_max, size=(1500, 45))\n",
    "\n",
    "# Example reward matrix R based on Y (replace with your own logic)\n",
    "R = np.random.uniform(low=-100,high=100,size=(action_dim, state_dim))  # Example: random R matrix for 45 actions and 23 Y features\n",
    "\n",
    "# Create the custom environment\n",
    "env = CustomEnv(S=S, A=A, R=None)\n",
    "\n",
    "# Wrap the environment in a vectorized form\n",
    "vec_env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vec_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m RL_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mvec_env\u001b[49m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vec_env' is not defined"
     ]
    }
   ],
   "source": [
    "model = RL_model(\"MultiInputPolicy\", vec_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARS,DDPG,HER,SAC,TD3,TQC,RecurrentPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_model = DDPG\n",
    "# from stable_baselines3 import HerReplayBuffer\n",
    "# model = SAC(\n",
    "#     \"MultiInputPolicy\",\n",
    "#     env,\n",
    "#     replay_buffer_class=HerReplayBuffer,\n",
    "#     replay_buffer_kwargs=dict(\n",
    "#       n_sampled_goal=n_sampled_goal,\n",
    "#       goal_selection_strategy=\"future\",\n",
    "#     ),\n",
    "#     verbose=1,\n",
    "#     buffer_size=int(1e6),\n",
    "#     learning_rate=1e-3,\n",
    "#     gamma=0.95,\n",
    "#     batch_size=256,\n",
    "#     policy_kwargs=dict(net_arch=[256, 256, 256]),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.ddpg.ddpg.DDPG'>\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -450110 is out of bounds for dimension 0 with size 124",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize and train the RL agent (using PPO as an example)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m RL_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, vec_env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\ddpg\\ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[0;32m    116\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDDPG:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[0;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\shimmy\\openai_gym_compatibility.py:251\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[1;32mIn[12], line 28\u001b[0m, in \u001b[0;36mCustomEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Apply action and get next state and reward\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_observation()\n\u001b[1;32m---> 28\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     done \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\n",
      "Cell \u001b[1;32mIn[12], line 42\u001b[0m, in \u001b[0;36mCustomEnv._calculate_reward\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_reward\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Example: using a reward matrix R based on expected Y values\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m         ideal_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Ideal Y value for the chosen action\u001b[39;00m\n\u001b[0;32m     43\u001b[0m         actual_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step]  \u001b[38;5;66;03m# Actual Y value\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnorm(actual_y \u001b[38;5;241m-\u001b[39m ideal_y)  \u001b[38;5;66;03m# Negative L2 norm difference as reward\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -450110 is out of bounds for dimension 0 with size 124"
     ]
    }
   ],
   "source": [
    "state_dim = 29\n",
    "action_dim = 124\n",
    "S = np.random.randn(15000, state_dim)\n",
    "\n",
    "A = np.random.randn(5000, action_dim)\n",
    "#A = np.random.randint(low=1, high=discrete_action_max, size=(1500, 45))\n",
    "\n",
    "# Example reward matrix R based on Y (replace with your own logic)\n",
    "R = np.random.uniform(low=-100,high=100,size=(action_dim, state_dim))  # Example: random R matrix for 45 actions and 23 Y features\n",
    "\n",
    "# Create the custom environment\n",
    "env = CustomEnv(S=S, A=A, R=R)\n",
    "\n",
    "# Wrap the environment in a vectorized form\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "print(RL_model)\n",
    "# Initialize and train the RL agent (using PPO as an example)\n",
    "model = RL_model(\"MultiInputPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDPG\n",
    "#\n",
    "# ...\n",
    "# ---> 42         ideal_y = self.R[action]  # Ideal Y value for the chosen action\n",
    "#      43         actual_y = self.S[self.current_step]  # Actual Y value\n",
    "#      44         reward = -torch.norm(actual_y - ideal_y)  # Negative L2 norm difference as reward\n",
    "\n",
    "# IndexError: index -370166 is out of bounds for dimension 0 with size 124\n",
    "\n",
    "\n",
    "# ---> 42         ideal_y = self.R[action]  # Ideal Y value for the chosen action\n",
    "#      43         actual_y = self.S[self.current_step]  # Actual Y value\n",
    "#      44         reward = -torch.norm(actual_y - ideal_y)  # Negative L2 norm difference as reward\n",
    "\n",
    "# IndexError: index -386851 is out of bounds for dimension 0 with size 124\n",
    "\n",
    "\n",
    "# TD3\n",
    "---> 42         ideal_y = self.R[action]  # Ideal Y value for the chosen action\n",
    "     43         actual_y = self.S[self.current_step]  # Actual Y value\n",
    "     44         reward = -torch.norm(actual_y - ideal_y)  # Negative L2 norm difference as reward\n",
    "\n",
    "IndexError: index -736909 is out of bounds for dimension 0 with size 124\n",
    "     \n",
    "\n",
    "# TQC\n",
    "---> 42         ideal_y = self.R[action]  # Ideal Y value for the chosen action\n",
    "     43         actual_y = self.S[self.current_step]  # Actual Y value\n",
    "     44         reward = -torch.norm(actual_y - ideal_y)  # Negative L2 norm difference as reward\n",
    "\n",
    "IndexError: index -550305 is out of bounds for dimension 0 with size 124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
