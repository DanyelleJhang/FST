{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "from sb3_contrib import ARS # 機率輸出不會改變\n",
    "from stable_baselines3 import A2C # 機率輸出會改變\n",
    "from stable_baselines3 import DDPG # 輸出機率和動作長依樣?\n",
    "from stable_baselines3 import HER # 這要搭配其他使用\n",
    "from stable_baselines3 import PPO # 機率輸出會改變\n",
    "from stable_baselines3 import SAC # 機率輸出會改變\n",
    "from stable_baselines3 import TD3 # 輸出機率和動作長依樣?\n",
    "from sb3_contrib import TRPO\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib import RecurrentPPO # MlpLstmPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "n_samples = 5000\n",
    "n_features = 53\n",
    "n_informative = 53\n",
    "n_targets = 27\n",
    "noise = 0.1\n",
    "random_seed = np.random.randint(0, 1000)[0]\n",
    "\n",
    "X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative, n_targets=n_targets, noise=noise, random_state=random_seed)\n",
    "\n",
    "reg = xgb.XGBRegressor(n_estimators=100,multi_strategy=\"multi_output_tree\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random_seed) # tree_method=\"hist\", device=\"cuda\"\n",
    "reg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, S, A, model, max_step=100, device='cpu'):\n",
    "        self.device = device\n",
    "        \n",
    "        # Normalize S and A\n",
    "        self.S = torch.tensor(S, dtype=torch.float32).to(self.device)#self._normalize(torch.tensor(S, dtype=torch.float32)).to(self.device)\n",
    "        self.A = torch.tensor(A, dtype=torch.float32).to(self.device)#self._normalize(torch.tensor(A, dtype=torch.float32)).to(self.device)\n",
    "        \n",
    "        self.num_samples = self.S.shape[0]\n",
    "        self.current_step = 0\n",
    "        self.model = model\n",
    "        self.max_step = max_step\n",
    "        self.action_instance = self.A.shape[0]\n",
    "        self.action_dim = self.A.shape[1]\n",
    "        \n",
    "        # 计算边界（假设数据大致遵循正态分布）\n",
    "        self.obs_low = float(self.S.min())*1.5\n",
    "        self.obs_high = float(self.S.max())*1.5\n",
    "        self.action_low = float(self.A.min())*1.5\n",
    "        self.action_high = float(self.A.max())*1.5\n",
    "\n",
    "        # Limit the action space\n",
    "        self.action_space = spaces.Box(low=self.action_low, high=self.action_high, shape=(self.action_instance, self.action_dim), dtype=np.float32)\n",
    "        \n",
    "        self.obs_dim_S = self.S.shape[1]\n",
    "        self.observation_space = spaces.Box(low=self.obs_low, high=self.obs_high, shape=(self.num_samples, self.obs_dim_S), dtype=np.float32)\n",
    "        \n",
    "    # def _normalize(self, tensor):\n",
    "    #     mean = tensor.mean(dim=0, keepdim=True)\n",
    "    #     std = tensor.std(dim=0, keepdim=True)\n",
    "    #     return (tensor - mean) / (std + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "    \n",
    "    # def _denormalize(self, tensor, original):\n",
    "    #     mean = original.mean(dim=0, keepdim=True)\n",
    "    #     std = original.std(dim=0, keepdim=True)\n",
    "    #     return tensor * (std + 1e-8) + mean\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # self.current_step 放哪?\n",
    "        num_instance = self.S.shape[0]\n",
    "        random_index = np.random.randint(0,num_instance, size=int(self.num_samples))\n",
    "        obs = self.S[random_index,:]#.shape \n",
    "        #print(\"DONE 2\")    \n",
    "        return obs, random_index\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        obs , _ = self._get_observation()\n",
    "        #print(\"DONE 3\")  \n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 确保动作在边界内\n",
    "        #action = np.clip(action, -1, 1)\n",
    "        next_state, index = self._get_observation()\n",
    "        action = torch.tensor(action, dtype=torch.float32).to(self.device)\n",
    "        #print(action.shape, self.A.shape)\n",
    "        # 在使用之前反标准化动作\n",
    "        denorm_action = action[index,:]#self._denormalize(action, self.A)\n",
    "        \n",
    "        reward, score = self._calculate_reward(denorm_action)\n",
    "        #print(\"STEP.....\",\"reward: \",reward,\"score: \",score)\n",
    "        best_action = None\n",
    "        best_score = float('-inf')\n",
    "        if reward > 0 and score is not None:\n",
    "            # 如果 score 是张量，取其平均值\n",
    "            current_score = score.mean().item() if isinstance(score, torch.Tensor) else score\n",
    "            if current_score > best_score:\n",
    "                best_score = current_score\n",
    "                best_action = denorm_action.cpu().numpy()  # 将最佳动作转换为 numpy 数组\n",
    "\n",
    "        done = (self.current_step == self.max_step - 1)\n",
    "        self.current_step += 1\n",
    "        \n",
    "        return next_state.cpu().numpy(), reward, done, {\"best_action\": best_action, \"best_score\": best_score}\n",
    "    \n",
    "    def _calculate_reward(self, action):\n",
    "        try:\n",
    "            next_state, index = self._get_observation()\n",
    "\n",
    "            #print(action.shape)\n",
    "            #print(self.A.shape)\n",
    "            #print(len(index))\n",
    "            y_pred_observe = torch.tensor(self.model.predict(action[index,:].cpu().numpy()), dtype=torch.float32).to(self.device)\n",
    "            y_pred = torch.tensor(self.model.predict(self.A[index,:].cpu().numpy()), dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # 反标准化预测和真实值以进行正确的 R2 计算\n",
    "            # denorm_S = self._denormalize(self.S, self.S)\n",
    "            # denorm_y_pred_observe = self._denormalize(y_pred_observe, self.S)\n",
    "            # denorm_y_pred = self._denormalize(y_pred, self.S)\n",
    "\n",
    "            r2_observe = r2_score(next_state.cpu().numpy(), y_pred_observe.cpu().numpy(), multioutput='raw_values')\n",
    "            r2_ground = r2_score(next_state.cpu().numpy(), y_pred.cpu().numpy(), multioutput='raw_values')\n",
    "\n",
    "            # 检查 NaN 值\n",
    "            if np.isnan(r2_observe).any() or np.isnan(r2_ground).any():\n",
    "                return 0.0, None\n",
    "\n",
    "            score = None\n",
    "            #print(\"r2_observe: \",r2_observe,\"r2_ground: \",r2_ground)\n",
    "            if stats.ttest_ind(r2_observe, r2_ground, alternative=\"greater\").pvalue < 0.05:\n",
    "                reward = float(np.mean(r2_observe) - np.mean(r2_ground))  # 连续奖励\n",
    "                score = r2_observe\n",
    "                \n",
    "            elif stats.ttest_ind(r2_observe, r2_ground, alternative=\"less\").pvalue < 0.05:\n",
    "                reward = float(np.mean(r2_observe) - np.mean(r2_ground))  # 负奖励\n",
    "            else:\n",
    "                reward = 0.0\n",
    "            \n",
    "            return reward, torch.tensor(score).to(self.device) if score is not None else None\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in _calculate_reward: {e}\")\n",
    "            return 0.0, None"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
