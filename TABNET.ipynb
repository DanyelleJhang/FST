{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "dir = \"C:/Users/foresight_User/Code/測試資料/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CH_TO = pd.read_csv(dir+\"CH_TO.csv\").dropna()\n",
    "feature = CH_TO.drop(columns=[\"Y\",\"Context Name\"])#.to_numpy()\n",
    "target = CH_TO[\"Y\"]#.to_numpy()\n",
    "#加入CROSS項目，有時候會顯著，有時候不會顯著，明顯受到切資料的影響， 但CROSS BEST的模型確實會比較好\n",
    "\n",
    "\n",
    "# feature = pd.read_csv(dir+\"wb/Indicator_Data(1).csv\").drop(columns=[\"Unnamed: 74\",\"Context Name\"])#.to_numpy()\n",
    "# target = pd.read_csv(dir+\"wb/Metrology_Data(1).csv\")[\"Point1\"]#.to_numpy()\n",
    "\n",
    "# data = pd.read_excel(dir+\"Array7_N808測試資料集/Model_TJN808XK_SAMP75.xlsx\").drop(columns=[\"CONTEXTID\"]).dropna(axis=0)\n",
    "# feature = data.drop(columns=list(filter(lambda x: \"T2_CD\" in x,data.columns)))#.to_numpy()\n",
    "# target = data[\"T2_CD01_Mean\"]#.to_numpy()\n",
    "\n",
    "columns_name = list(feature.columns) # or np.arange(0,feature.shape[1])\n",
    "#\n",
    "# 分割數據集\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature.to_numpy(), target.to_numpy().reshape(-1,1), test_size=0.3)\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "#X_valid, _, y_valid, _ = train_test_split(X_train, y_train, test_size=0.2)\n",
    "#X_valid, _, y_valid, _ = train_test_split(X_train, y_train.reshape(-1,1), test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNetWrapper()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TabNetWrapper()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (input): Linear(in_features=30, out_features=16, bias=True)\n",
      "  (hidden_1): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (output): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchensemble.gradient_boosting import GradientBoostingRegressor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# from torchensemble import VotingRegressor\n",
    "\n",
    "# class TabNetWrapper(nn.Module):\n",
    "#     def __init__(self, tabnet_model):\n",
    "#         super(TabNetWrapper, self).__init__()\n",
    "        \n",
    "#         # 複製 TabNetRegressor 的權重\n",
    "#         self.tabnet = tabnet_model\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # 在 forward 函數中定義模型的前向傳播邏輯\n",
    "#         return self.tabnet(x)\n",
    "\n",
    "# # 假設你已經創建了 TabNetRegressor 的實例 tabnet_model\n",
    "# # 然後將它傳遞給 TabNetWrapper\n",
    "# tabnet_model = TabNetRegressor(input_dim=X_train.shape[1], output_dim=1)\n",
    "# model = TabNetWrapper(tabnet_model)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(in_features=30, out_features=16)\n",
    "        self.hidden_1 = nn.Linear(in_features=16, out_features=16)\n",
    "        self.output = nn.Linear(in_features=16, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.input(x))\n",
    "        x = torch.nn.functional.relu(self.hidden_1(x))\n",
    "        return self.output(x)\n",
    "    \n",
    "    \n",
    "model = Net()\n",
    "\n",
    "# 輸出模型結構\n",
    "print(model)\n",
    "# 假設 feature 和 target 是 pandas DataFrame 或 numpy array\n",
    "\n",
    "# 將 feature 和 target 轉換成 TensorDataset\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                              torch.tensor(y_train, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                             torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "# 定義 DataLoader\n",
    "batch_size = 32  # 可以根據需要調整批次大小\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# 訓練TabNet模型\n",
    "batch_size = int(X_train.shape[0]/3)\n",
    "#virtual_batch_size = int(batch_size/3)\n",
    "#model = TabNetRegressor()\n",
    "# model.fit(\n",
    "#     X_train, y_train,\n",
    "#     eval_set=[(X_train, y_train)],\n",
    "#     eval_name=['train'],\n",
    "#     eval_metric=['rmsle', 'mae', 'rmse', 'mse'],\n",
    "#     max_epochs=2000,\n",
    "#     patience=100,\n",
    "#     batch_size=batch_size,\n",
    "#     #virtual_batch_size=virtual_batch_size\n",
    "# )\n",
    "ensemble = GradientBoostingRegressor(model,n_estimators=100,cuda=False)\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "\n",
    "ensemble.set_optimizer(\n",
    "    \"Adam\",                                 # type of parameter optimizer\n",
    "    lr=learning_rate,                       # learning rate of parameter optimizer\n",
    "    weight_decay=5e-4,              # weight decay of parameter optimizer\n",
    ")\n",
    "\n",
    "# Set the learning rate scheduler\n",
    "ensemble.set_scheduler(\n",
    "    \"CosineAnnealingLR\",                    # type of learning rate scheduler\n",
    "    T_max=epochs,                           # additional arguments on the scheduler\n",
    ")\n",
    "\n",
    "# Train the ensemble\n",
    "ensemble.fit(\n",
    "    train_loader,\n",
    "    epochs=epochs,                          # number of training epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1036180268267376"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, np.array(ensemble.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191, 30)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.0     |  0:00:00s\n",
      "epoch 1  | loss: 0.0     |  0:00:00s\n",
      "epoch 2  | loss: 0.0     |  0:00:00s\n",
      "epoch 3  | loss: 0.0     |  0:00:00s\n",
      "epoch 4  | loss: 0.0     |  0:00:00s\n",
      "epoch 5  | loss: 0.0     |  0:00:00s\n",
      "epoch 6  | loss: 0.0     |  0:00:00s\n",
      "epoch 7  | loss: 0.0     |  0:00:00s\n",
      "epoch 8  | loss: 0.0     |  0:00:00s\n",
      "epoch 9  | loss: 0.0     |  0:00:00s\n",
      "epoch 10 | loss: 0.0     |  0:00:00s\n",
      "epoch 11 | loss: 0.0     |  0:00:00s\n",
      "epoch 12 | loss: 0.0     |  0:00:00s\n",
      "epoch 13 | loss: 0.0     |  0:00:00s\n",
      "epoch 14 | loss: 0.0     |  0:00:00s\n",
      "epoch 15 | loss: 0.0     |  0:00:00s\n",
      "epoch 16 | loss: 0.0     |  0:00:00s\n",
      "epoch 17 | loss: 0.0     |  0:00:00s\n",
      "epoch 18 | loss: 0.0     |  0:00:00s\n",
      "epoch 19 | loss: 0.0     |  0:00:00s\n",
      "epoch 20 | loss: 0.0     |  0:00:00s\n",
      "epoch 21 | loss: 0.0     |  0:00:00s\n",
      "epoch 22 | loss: 0.0     |  0:00:00s\n",
      "epoch 23 | loss: 0.0     |  0:00:00s\n",
      "epoch 24 | loss: 0.0     |  0:00:00s\n",
      "epoch 25 | loss: 0.0     |  0:00:00s\n",
      "epoch 26 | loss: 0.0     |  0:00:00s\n",
      "epoch 27 | loss: 0.0     |  0:00:00s\n",
      "epoch 28 | loss: 0.0     |  0:00:00s\n",
      "epoch 29 | loss: 0.0     |  0:00:00s\n",
      "epoch 30 | loss: 0.0     |  0:00:00s\n",
      "epoch 31 | loss: 0.0     |  0:00:00s\n",
      "epoch 32 | loss: 0.0     |  0:00:00s\n",
      "epoch 33 | loss: 0.0     |  0:00:00s\n",
      "epoch 34 | loss: 0.0     |  0:00:00s\n",
      "epoch 35 | loss: 0.0     |  0:00:00s\n",
      "epoch 36 | loss: 0.0     |  0:00:00s\n",
      "epoch 37 | loss: 0.0     |  0:00:00s\n",
      "epoch 38 | loss: 0.0     |  0:00:00s\n",
      "epoch 39 | loss: 0.0     |  0:00:00s\n",
      "epoch 40 | loss: 0.0     |  0:00:00s\n",
      "epoch 41 | loss: 0.0     |  0:00:00s\n",
      "epoch 42 | loss: 0.0     |  0:00:00s\n",
      "epoch 43 | loss: 0.0     |  0:00:00s\n",
      "epoch 44 | loss: 0.0     |  0:00:00s\n",
      "epoch 45 | loss: 0.0     |  0:00:00s\n",
      "epoch 46 | loss: 0.0     |  0:00:00s\n",
      "epoch 47 | loss: 0.0     |  0:00:00s\n",
      "epoch 48 | loss: 0.0     |  0:00:00s\n",
      "epoch 49 | loss: 0.0     |  0:00:00s\n",
      "epoch 50 | loss: 0.0     |  0:00:00s\n",
      "epoch 51 | loss: 0.0     |  0:00:00s\n",
      "epoch 52 | loss: 0.0     |  0:00:00s\n",
      "epoch 53 | loss: 0.0     |  0:00:00s\n",
      "epoch 54 | loss: 0.0     |  0:00:00s\n",
      "epoch 55 | loss: 0.0     |  0:00:00s\n",
      "epoch 56 | loss: 0.0     |  0:00:00s\n",
      "epoch 57 | loss: 0.0     |  0:00:00s\n",
      "epoch 58 | loss: 0.0     |  0:00:00s\n",
      "epoch 59 | loss: 0.0     |  0:00:00s\n",
      "epoch 60 | loss: 0.0     |  0:00:00s\n",
      "epoch 61 | loss: 0.0     |  0:00:00s\n",
      "epoch 62 | loss: 0.0     |  0:00:00s\n",
      "epoch 63 | loss: 0.0     |  0:00:00s\n",
      "epoch 64 | loss: 0.0     |  0:00:00s\n",
      "epoch 65 | loss: 0.0     |  0:00:00s\n",
      "epoch 66 | loss: 0.0     |  0:00:00s\n",
      "epoch 67 | loss: 0.0     |  0:00:00s\n",
      "epoch 68 | loss: 0.0     |  0:00:00s\n",
      "epoch 69 | loss: 0.0     |  0:00:00s\n",
      "epoch 70 | loss: 0.0     |  0:00:00s\n",
      "epoch 71 | loss: 0.0     |  0:00:00s\n",
      "epoch 72 | loss: 0.0     |  0:00:00s\n",
      "epoch 73 | loss: 0.0     |  0:00:00s\n",
      "epoch 74 | loss: 0.0     |  0:00:00s\n",
      "epoch 75 | loss: 0.0     |  0:00:00s\n",
      "epoch 76 | loss: 0.0     |  0:00:00s\n",
      "epoch 77 | loss: 0.0     |  0:00:00s\n",
      "epoch 78 | loss: 0.0     |  0:00:00s\n",
      "epoch 79 | loss: 0.0     |  0:00:00s\n",
      "epoch 80 | loss: 0.0     |  0:00:00s\n",
      "epoch 81 | loss: 0.0     |  0:00:00s\n",
      "epoch 82 | loss: 0.0     |  0:00:00s\n",
      "epoch 83 | loss: 0.0     |  0:00:00s\n",
      "epoch 84 | loss: 0.0     |  0:00:00s\n",
      "epoch 85 | loss: 0.0     |  0:00:00s\n",
      "epoch 86 | loss: 0.0     |  0:00:00s\n",
      "epoch 87 | loss: 0.0     |  0:00:00s\n",
      "epoch 88 | loss: 0.0     |  0:00:00s\n",
      "epoch 89 | loss: 0.0     |  0:00:00s\n",
      "epoch 90 | loss: 0.0     |  0:00:00s\n",
      "epoch 91 | loss: 0.0     |  0:00:00s\n",
      "epoch 92 | loss: 0.0     |  0:00:00s\n",
      "epoch 93 | loss: 0.0     |  0:00:00s\n",
      "epoch 94 | loss: 0.0     |  0:00:00s\n",
      "epoch 95 | loss: 0.0     |  0:00:00s\n",
      "epoch 96 | loss: 0.0     |  0:00:00s\n",
      "epoch 97 | loss: 0.0     |  0:00:00s\n",
      "epoch 98 | loss: 0.0     |  0:00:00s\n",
      "epoch 99 | loss: 0.0     |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "\n",
    "# # TabNetPretrainer\n",
    "# unsupervised_model = TabNetPretrainer(\n",
    "#     optimizer_fn=torch.optim.Adam,\n",
    "#     optimizer_params=dict(lr=2e-2),\n",
    "#     mask_type='entmax' # \"sparsemax\"\n",
    "# )\n",
    "\n",
    "# unsupervised_model.fit(\n",
    "#     X_train=X_train,\n",
    "#     pretraining_ratio=0.8,\n",
    "# )\n",
    "\n",
    "reg = TabNetRegressor(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params={\"step_size\":10, # how to use learning rate scheduler\n",
    "                      \"gamma\":0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='sparsemax' # This will be overwritten if using pretrain model\n",
    ")\n",
    "\n",
    "reg.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_metric=['rmse'],\n",
    "    #from_unsupervised=unsupervised_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7638.519027486152"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1675.7844  ],\n",
       "       [1095.1241  ],\n",
       "       [1675.6028  ],\n",
       "       [ -21.589806],\n",
       "       [1683.2109  ],\n",
       "       [1683.123   ],\n",
       "       [1093.7427  ],\n",
       "       [1675.7667  ],\n",
       "       [1675.899   ],\n",
       "       [1675.7941  ],\n",
       "       [1095.1273  ],\n",
       "       [1683.0305  ],\n",
       "       [1133.1093  ],\n",
       "       [1683.1528  ],\n",
       "       [1683.1223  ],\n",
       "       [1675.8704  ],\n",
       "       [1132.1826  ],\n",
       "       [1095.1241  ],\n",
       "       [1095.795   ],\n",
       "       [1683.0759  ],\n",
       "       [1683.5408  ],\n",
       "       [1675.6655  ],\n",
       "       [1094.1923  ],\n",
       "       [1675.668   ],\n",
       "       [1675.8245  ],\n",
       "       [1254.2139  ],\n",
       "       [1095.795   ],\n",
       "       [1095.795   ],\n",
       "       [1683.5935  ],\n",
       "       [1682.9442  ],\n",
       "       [1683.6235  ],\n",
       "       [1683.0275  ],\n",
       "       [1704.561   ],\n",
       "       [1095.1241  ],\n",
       "       [1683.5459  ],\n",
       "       [1095.1241  ],\n",
       "       [1682.9272  ],\n",
       "       [1675.8809  ],\n",
       "       [1675.9208  ],\n",
       "       [1683.5334  ],\n",
       "       [1096.2456  ],\n",
       "       [1254.2892  ],\n",
       "       [1675.7129  ],\n",
       "       [1675.7463  ],\n",
       "       [1095.1241  ],\n",
       "       [1094.9021  ],\n",
       "       [1683.1216  ],\n",
       "       [1095.1241  ],\n",
       "       [1683.4644  ],\n",
       "       [1683.0442  ],\n",
       "       [1683.1707  ],\n",
       "       [1675.8682  ],\n",
       "       [1095.1241  ],\n",
       "       [1683.2313  ],\n",
       "       [ -21.624557],\n",
       "       [1683.1849  ],\n",
       "       [1683.5015  ],\n",
       "       [1096.0203  ],\n",
       "       [1095.1273  ],\n",
       "       [1057.6431  ],\n",
       "       [1095.1241  ],\n",
       "       [1096.4708  ],\n",
       "       [1675.8835  ],\n",
       "       [1095.1241  ],\n",
       "       [1697.1017  ],\n",
       "       [1683.5315  ],\n",
       "       [ -21.624557],\n",
       "       [1682.9138  ],\n",
       "       [1683.5298  ],\n",
       "       [1675.9401  ],\n",
       "       [1095.1273  ],\n",
       "       [1683.1986  ],\n",
       "       [1683.0796  ],\n",
       "       [1683.0604  ],\n",
       "       [1675.7499  ],\n",
       "       [1683.0186  ],\n",
       "       [1675.8525  ],\n",
       "       [1675.9564  ],\n",
       "       [1675.8799  ],\n",
       "       [1683.169   ],\n",
       "       [1094.1923  ],\n",
       "       [1057.42    ],\n",
       "       [1094.1539  ],\n",
       "       [1675.8809  ],\n",
       "       [1675.8975  ],\n",
       "       [ 418.17206 ],\n",
       "       [1095.3527  ],\n",
       "       [1675.9185  ],\n",
       "       [1095.3489  ],\n",
       "       [1096.0203  ],\n",
       "       [1095.1273  ],\n",
       "       [1675.9414  ],\n",
       "       [1094.1539  ],\n",
       "       [1675.9185  ],\n",
       "       [1094.1539  ],\n",
       "       [1675.9401  ],\n",
       "       [1095.1241  ],\n",
       "       [1682.9292  ],\n",
       "       [1675.5574  ],\n",
       "       [1675.831   ],\n",
       "       [1675.6819  ],\n",
       "       [1254.415   ],\n",
       "       [1682.9751  ],\n",
       "       [1683.501   ],\n",
       "       [1057.6431  ],\n",
       "       [1096.2456  ],\n",
       "       [1094.1923  ],\n",
       "       [1683.3362  ],\n",
       "       [1697.0969  ],\n",
       "       [1675.7231  ],\n",
       "       [1683.532   ],\n",
       "       [1093.9675  ],\n",
       "       [1675.9198  ],\n",
       "       [1683.6555  ],\n",
       "       [1096.4708  ],\n",
       "       [1096.2456  ],\n",
       "       [1683.075   ],\n",
       "       [1675.8167  ],\n",
       "       [1675.899   ],\n",
       "       [1683.4957  ],\n",
       "       [1095.1241  ],\n",
       "       [1683.5977  ],\n",
       "       [1683.0613  ],\n",
       "       [1096.2456  ],\n",
       "       [1675.7129  ],\n",
       "       [1675.8032  ],\n",
       "       [1683.6094  ],\n",
       "       [1095.1387  ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.0     |  0:00:00s\n",
      "epoch 1  | loss: 0.0     |  0:00:00s\n",
      "epoch 2  | loss: 0.0     |  0:00:00s\n",
      "epoch 3  | loss: 0.0     |  0:00:00s\n",
      "epoch 4  | loss: 0.0     |  0:00:00s\n",
      "epoch 5  | loss: 0.0     |  0:00:00s\n",
      "epoch 6  | loss: 0.0     |  0:00:00s\n",
      "epoch 7  | loss: 0.0     |  0:00:00s\n",
      "epoch 8  | loss: 0.0     |  0:00:00s\n",
      "epoch 9  | loss: 0.0     |  0:00:00s\n",
      "epoch 10 | loss: 0.0     |  0:00:00s\n",
      "epoch 11 | loss: 0.0     |  0:00:00s\n",
      "epoch 12 | loss: 0.0     |  0:00:00s\n",
      "epoch 13 | loss: 0.0     |  0:00:00s\n",
      "epoch 14 | loss: 0.0     |  0:00:00s\n",
      "epoch 15 | loss: 0.0     |  0:00:00s\n",
      "epoch 16 | loss: 0.0     |  0:00:00s\n",
      "epoch 17 | loss: 0.0     |  0:00:00s\n",
      "epoch 18 | loss: 0.0     |  0:00:00s\n",
      "epoch 19 | loss: 0.0     |  0:00:00s\n",
      "epoch 20 | loss: 0.0     |  0:00:00s\n",
      "epoch 21 | loss: 0.0     |  0:00:00s\n",
      "epoch 22 | loss: 0.0     |  0:00:00s\n",
      "epoch 23 | loss: 0.0     |  0:00:00s\n",
      "epoch 24 | loss: 0.0     |  0:00:00s\n",
      "epoch 25 | loss: 0.0     |  0:00:00s\n",
      "epoch 26 | loss: 0.0     |  0:00:00s\n",
      "epoch 27 | loss: 0.0     |  0:00:00s\n",
      "epoch 28 | loss: 0.0     |  0:00:00s\n",
      "epoch 29 | loss: 0.0     |  0:00:00s\n",
      "epoch 30 | loss: 0.0     |  0:00:00s\n",
      "epoch 31 | loss: 0.0     |  0:00:00s\n",
      "epoch 32 | loss: 0.0     |  0:00:00s\n",
      "epoch 33 | loss: 0.0     |  0:00:00s\n",
      "epoch 34 | loss: 0.0     |  0:00:00s\n",
      "epoch 35 | loss: 0.0     |  0:00:00s\n",
      "epoch 36 | loss: 0.0     |  0:00:00s\n",
      "epoch 37 | loss: 0.0     |  0:00:00s\n",
      "epoch 38 | loss: 0.0     |  0:00:00s\n",
      "epoch 39 | loss: 0.0     |  0:00:00s\n",
      "epoch 40 | loss: 0.0     |  0:00:00s\n",
      "epoch 41 | loss: 0.0     |  0:00:00s\n",
      "epoch 42 | loss: 0.0     |  0:00:00s\n",
      "epoch 43 | loss: 0.0     |  0:00:00s\n",
      "epoch 44 | loss: 0.0     |  0:00:00s\n",
      "epoch 45 | loss: 0.0     |  0:00:00s\n",
      "epoch 46 | loss: 0.0     |  0:00:00s\n",
      "epoch 47 | loss: 0.0     |  0:00:00s\n",
      "epoch 48 | loss: 0.0     |  0:00:00s\n",
      "epoch 49 | loss: 0.0     |  0:00:00s\n",
      "epoch 50 | loss: 0.0     |  0:00:00s\n",
      "epoch 51 | loss: 0.0     |  0:00:00s\n",
      "epoch 52 | loss: 0.0     |  0:00:00s\n",
      "epoch 53 | loss: 0.0     |  0:00:00s\n",
      "epoch 54 | loss: 0.0     |  0:00:00s\n",
      "epoch 55 | loss: 0.0     |  0:00:00s\n",
      "epoch 56 | loss: 0.0     |  0:00:00s\n",
      "epoch 57 | loss: 0.0     |  0:00:00s\n",
      "epoch 58 | loss: 0.0     |  0:00:00s\n",
      "epoch 59 | loss: 0.0     |  0:00:00s\n",
      "epoch 60 | loss: 0.0     |  0:00:00s\n",
      "epoch 61 | loss: 0.0     |  0:00:00s\n",
      "epoch 62 | loss: 0.0     |  0:00:00s\n",
      "epoch 63 | loss: 0.0     |  0:00:00s\n",
      "epoch 64 | loss: 0.0     |  0:00:00s\n",
      "epoch 65 | loss: 0.0     |  0:00:00s\n",
      "epoch 66 | loss: 0.0     |  0:00:00s\n",
      "epoch 67 | loss: 0.0     |  0:00:00s\n",
      "epoch 68 | loss: 0.0     |  0:00:00s\n",
      "epoch 69 | loss: 0.0     |  0:00:00s\n",
      "epoch 70 | loss: 0.0     |  0:00:00s\n",
      "epoch 71 | loss: 0.0     |  0:00:00s\n",
      "epoch 72 | loss: 0.0     |  0:00:00s\n",
      "epoch 73 | loss: 0.0     |  0:00:00s\n",
      "epoch 74 | loss: 0.0     |  0:00:00s\n",
      "epoch 75 | loss: 0.0     |  0:00:00s\n",
      "epoch 76 | loss: 0.0     |  0:00:00s\n",
      "epoch 77 | loss: 0.0     |  0:00:00s\n",
      "epoch 78 | loss: 0.0     |  0:00:00s\n",
      "epoch 79 | loss: 0.0     |  0:00:00s\n",
      "epoch 80 | loss: 0.0     |  0:00:00s\n",
      "epoch 81 | loss: 0.0     |  0:00:00s\n",
      "epoch 82 | loss: 0.0     |  0:00:00s\n",
      "epoch 83 | loss: 0.0     |  0:00:00s\n",
      "epoch 84 | loss: 0.0     |  0:00:00s\n",
      "epoch 85 | loss: 0.0     |  0:00:00s\n",
      "epoch 86 | loss: 0.0     |  0:00:00s\n",
      "epoch 87 | loss: 0.0     |  0:00:00s\n",
      "epoch 88 | loss: 0.0     |  0:00:00s\n",
      "epoch 89 | loss: 0.0     |  0:00:00s\n",
      "epoch 90 | loss: 0.0     |  0:00:00s\n",
      "epoch 91 | loss: 0.0     |  0:00:00s\n",
      "epoch 92 | loss: 0.0     |  0:00:00s\n",
      "epoch 93 | loss: 0.0     |  0:00:00s\n",
      "epoch 94 | loss: 0.0     |  0:00:00s\n",
      "epoch 95 | loss: 0.0     |  0:00:00s\n",
      "epoch 96 | loss: 0.0     |  0:00:00s\n",
      "epoch 97 | loss: 0.0     |  0:00:00s\n",
      "epoch 98 | loss: 0.0     |  0:00:00s\n",
      "epoch 99 | loss: 0.0     |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "unsupervised_model.fit(X_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192    14048.4\n",
       "227    14086.0\n",
       "555    14089.8\n",
       "340    13975.4\n",
       "560    13949.0\n",
       "        ...   \n",
       "139    13905.6\n",
       "68     13896.2\n",
       "618    14202.6\n",
       "536    14201.0\n",
       "513    14303.4\n",
       "Name: Y, Length: 128, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsupervised_model.prepare_target(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192    14048.4\n",
       "227    14086.0\n",
       "555    14089.8\n",
       "340    13975.4\n",
       "560    13949.0\n",
       "        ...   \n",
       "139    13905.6\n",
       "68     13896.2\n",
       "618    14202.6\n",
       "536    14201.0\n",
       "513    14303.4\n",
       "Name: Y, Length: 128, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
