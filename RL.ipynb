{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python3.7.9 (Python 3.7.9)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n Python3.7.9 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mdptoolbox\n",
    "\n",
    "# 示例数据\n",
    "num_samples = 130\n",
    "num_features = 9\n",
    "num_actions = 45\n",
    "num_targets = 50\n",
    "\n",
    "# 生成随机数据作为示例\n",
    "samples = []\n",
    "for i in range(num_samples):\n",
    "    features = np.random.random(num_features)\n",
    "    targets = np.random.random(num_targets)\n",
    "    samples.append({\"features\": features, \"targets\": targets})\n",
    "\n",
    "# 构建状态空间和动作空间\n",
    "state_space_size = num_features + num_targets\n",
    "action_space_size = num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 构建MDP的奖励矩阵\n",
    "R = np.zeros((action_space_size, state_space_size, state_space_size))\n",
    "\n",
    "for a in range(action_space_size):\n",
    "    for s in range(state_space_size):\n",
    "        features = samples[s][\"features\"]\n",
    "        targets = samples[s][\"targets\"]\n",
    "        \n",
    "        # 模拟动作对应的状态转移\n",
    "        next_features = features  # 假设状态转移不变\n",
    "        next_targets = targets + np.random.random(num_targets) * 0.1  # 添加一些随机噪声\n",
    "        \n",
    "        # 构建下一个状态的完整向量\n",
    "        next_state = np.concatenate((next_features, next_targets))\n",
    "        \n",
    "        # 计算奖励（负均方误差）\n",
    "        reward = -np.mean((next_targets - samples[s][\"targets\"]) ** 2)\n",
    "        \n",
    "        # 更新奖励矩阵\n",
    "        R[a, s, :] = reward  # 这里简化为直接使用 reward，实际应根据转移后状态评估\n",
    "        \n",
    "# 构建MDP的状态转移概率矩阵（这里简单假设为均匀随机）\n",
    "P = np.ones((action_space_size, state_space_size, state_space_size)) / state_space_size\n",
    "\n",
    "# 定义MDP问题\n",
    "mdp = mdptoolbox.mdp.QLearning(transitions=P, reward=R, discount=0.95)\n",
    "\n",
    "# 求解MDP\n",
    "mdp.run()\n",
    "\n",
    "# 获取最佳策略（最优动作组合）\n",
    "best_policy = mdp.policy\n",
    "\n",
    "print(\"最佳策略（最优动作组合）：\", best_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation: [ 0.97861979  0.01152329  0.4779264  -1.83788408 -0.44486296  0.06620423\n",
      "  0.09263808 -1.05460767 -1.29149072 -0.22474662 -0.64085572 -0.57818808\n",
      " -0.84344305  0.57539129 -0.19779026  0.37634929 -0.73086445  1.0314188\n",
      " -0.05513471 -0.95187143  1.73998975 -0.44423871 -0.04697355 -1.12994277\n",
      "  0.96105641  0.43651209  1.68673694 -0.08775295 -0.68293601  0.05227444\n",
      " -0.41181948  1.47696433 -1.31313351 -2.64185462 -0.48416617 -0.4042733\n",
      " -0.25261355  0.5002195   0.72872389  0.01056364  0.43492809  0.3651766\n",
      "  0.3704211   0.1639889   0.38544975 -0.05473639  0.45774094 -0.17031296\n",
      "  0.17690185 -1.18334491 -0.61336723  0.81345319 -0.16557842 -0.32215159\n",
      "  0.16133995  0.15931639 -1.86594397  0.72859471 -0.40704614]\n",
      "Sample action: [ 0.21683596 -0.51408833  0.443587   -0.40082827  0.68951046  0.7248425\n",
      "  0.24207471 -0.00493412 -0.46831512 -0.44285402  0.8019002  -0.39888823\n",
      " -0.18927142  0.21498124  0.5100797   0.3965886   0.20945281  0.87948954\n",
      " -0.5229363  -0.690321    0.24614167  0.7164841  -0.14903976 -0.53847563\n",
      " -0.66691643 -0.6294099  -0.9928228  -0.17740744 -0.18087587 -0.20568529\n",
      "  0.0938957  -0.6553876  -0.78870237 -0.98448396 -0.6564803  -0.8087398\n",
      " -0.5121685  -0.6287201  -0.01148195  0.5618235   0.13021569 -0.51220024\n",
      "  0.5014618   0.06970878 -0.26759878]\n",
      "Next observation: [ 0.97861979  0.01152329  0.4779264  -1.83788408 -0.44486296  0.06620423\n",
      "  0.09263808 -1.05460767 -1.29149072 -0.55683324 -0.75892138 -0.98841736\n",
      " -0.87480198  1.53545607  0.90890649  1.25705381 -1.1386312   0.53653718\n",
      "  0.56036651  1.21909705  1.38498548 -0.35508023  0.45576957  2.34920355\n",
      " -0.47919315 -0.67669178 -0.87951187  1.36790423  0.65424966  0.98204241\n",
      "  1.46361257 -2.11036933  0.39540667 -0.4176438   0.46067198  0.21956661\n",
      " -0.94173097 -0.42249816 -0.49953195 -0.76705199 -0.17355903 -0.77360939\n",
      " -0.85175375 -0.35196001  1.05067934 -0.3949783  -1.19950738 -1.67375114\n",
      " -0.91778922  2.23631909  0.34372918 -1.18782451 -1.65689918  1.50622059\n",
      " -0.31465926 -0.55745332 -0.51501013 -0.75075354  0.6266787 ]\n",
      "Reward: -103.04960342727563\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.s_dim = 9  # Number of state features\n",
    "        self.a_dim = 45 # Number of actions\n",
    "        self.y_dim = 50 # Number of outcome features\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.a_dim,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.s_dim + self.y_dim,), dtype=np.float32)\n",
    "\n",
    "        # Initialize state and target values\n",
    "        self.state = np.zeros(self.s_dim)\n",
    "        self.target_y = np.zeros(self.y_dim)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state and target values to some initial values\n",
    "        self.state = np.random.randn(self.s_dim)\n",
    "        self.target_y = np.random.randn(self.y_dim)\n",
    "        return np.concatenate((self.state, self.target_y))\n",
    "\n",
    "    def step(self, action):\n",
    "        # Simulate the environment step\n",
    "        current_y = np.random.randn(self.y_dim)  # This should be replaced by your own model to calculate Y values based on action\n",
    "        reward = -np.sum((current_y - self.target_y) ** 2)  # Negative squared error as the reward\n",
    "\n",
    "        done = False  # Define a condition for ending the episode\n",
    "        return np.concatenate((self.state, current_y)), reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "# Example usage\n",
    "env = CustomEnv()\n",
    "obs = env.reset()\n",
    "print(\"Initial observation:\", obs)\n",
    "action = env.action_space.sample()\n",
    "print(\"Sample action:\", action)\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"Next observation:\", obs)\n",
    "print(\"Reward:\", reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1845 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1121         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112253595 |\n",
      "|    clip_fraction        | 0.108        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -63.8        |\n",
      "|    explained_variance   | 0.00025      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.23e+06     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0534      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.45e+06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 987         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009013215 |\n",
      "|    clip_fraction        | 0.0576      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -63.8       |\n",
      "|    explained_variance   | 0.000112    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.21e+06    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0411     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.45e+06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 943         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008995274 |\n",
      "|    clip_fraction        | 0.0558      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -63.9       |\n",
      "|    explained_variance   | -2.74e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.2e+06     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0422     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.43e+06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 915         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007971749 |\n",
      "|    clip_fraction        | 0.0533      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -63.9       |\n",
      "|    explained_variance   | -1.16e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.2e+06     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.041      |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.43e+06    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "#from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Check the custom environment\n",
    "#check_env(env)\n",
    "\n",
    "# Create the RL model\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the model\n",
    "#model.save(\"ppo_custom_env\")\n",
    "\n",
    "# Load the model\n",
    "#model = PPO.load(\"ppo_custom_env\")\n",
    "\n",
    "# Test the trained model\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = np.array([0.5, -0.2, 1.0, -1.0, 0.3, -0.3, 0.8, -0.5, 0.2])  # 這是新的s1, s2, ..., s9\n",
    "new_target_y = np.array([1.2, -0.5, 0.7, -0.8, 1.0, -0.1, 0.5, -0.3, 0.4, 0.2, -0.4, 0.9, -0.6, 0.1, 0.7, -0.2, 0.3, 1.0, -1.0, 0.8,\n",
    "                         0.1, -0.1, 0.6, -0.4, 0.5, -0.7, 0.3, -0.2, 0.9, -0.5, 0.2, -0.3, 0.8, 0.0, -0.1, 0.7, -0.6, 0.4, 0.2, -0.8,\n",
    "                         1.1, -0.9, 0.3, -0.2, 0.5, -0.1, 0.4, 0.3, -0.7, 0.6])  # 這是新的Y1, Y2, ..., Y50\n",
    "\n",
    "# 合併狀態和目標值作為模型的觀測值\n",
    "new_obs = np.concatenate((new_state, new_target_y))\n",
    "\n",
    "# 通過模型進行預測以獲得最佳行動\n",
    "best_action, _states = model.predict(new_obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.s_dim = 9  # Number of state features\n",
    "        self.a_dim = 45 # Number of actions\n",
    "        self.y_dim = 50 # Number of outcome features\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.a_dim,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.s_dim + self.y_dim,), dtype=np.float32)\n",
    "\n",
    "        # Initialize state and target values\n",
    "        self.state = np.zeros(self.s_dim)\n",
    "        self.target_y = np.zeros(self.y_dim)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state and target values to some initial values\n",
    "        self.state = np.random.randn(self.s_dim)\n",
    "        self.target_y = np.random.randn(self.y_dim)\n",
    "        return np.concatenate((self.state, self.target_y))\n",
    "\n",
    "    def step(self, action):\n",
    "        # Simulate the environment step\n",
    "        current_y = np.random.randn(self.y_dim)  # This should be replaced by your own model to calculate Y values based on action\n",
    "        reward = -np.sum((current_y - self.target_y) ** 2)  # Negative squared error as the reward\n",
    "\n",
    "        done = False  # Define a condition for ending the episode\n",
    "        return np.concatenate((self.state, current_y)), reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "# Example usage\n",
    "env = CustomEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 744       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -64.3     |\n",
      "|    explained_variance | -0.00817  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -1.85e+04 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.01e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 758       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -64.4     |\n",
      "|    explained_variance | 0.000742  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.93e+04 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.16e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 763       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -64.5     |\n",
      "|    explained_variance | -0.000103 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -2.12e+04 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.26e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 753       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -64.7     |\n",
      "|    explained_variance | -0.000286 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -1.75e+04 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9e+04     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 767      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -64.7    |\n",
      "|    explained_variance | 0.000116 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -2.1e+04 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.25e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 773       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -64.7     |\n",
      "|    explained_variance | -4.74e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -1.95e+04 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.1e+05   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 784       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -64.9     |\n",
      "|    explained_variance | -0.000137 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -1.92e+04 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.65e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 793       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -64.9     |\n",
      "|    explained_variance | -4.77e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -1.79e+04 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.03e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 790       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -64.9     |\n",
      "|    explained_variance | 5.9e-06   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -1.88e+04 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.02e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 801       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -65       |\n",
      "|    explained_variance | -8.94e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -2.07e+04 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.12e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 803       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -65       |\n",
      "|    explained_variance | 3.99e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -1.81e+04 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 8.94e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 792       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -65       |\n",
      "|    explained_variance | 1.01e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -2.26e+04 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.41e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 785       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -65.1     |\n",
      "|    explained_variance | -4.17e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -1.66e+04 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 8.95e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 774       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -65.1     |\n",
      "|    explained_variance | -1.19e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -2.05e+04 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.16e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 767       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -65.2     |\n",
      "|    explained_variance | -4.77e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -1.79e+04 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 9.89e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 763       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -65.3     |\n",
      "|    explained_variance | -7.15e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -2.16e+04 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.19e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 767       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -65.3     |\n",
      "|    explained_variance | -2.38e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -1.7e+04  |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 8.3e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 769       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -65.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -1.9e+04  |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.1e+05   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 773       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -65.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -1.99e+04 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.16e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 773       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -65.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -2.11e+04 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.4e+05   |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "#from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Check the custom environment\n",
    "#check_env(env)\n",
    "\n",
    "# Create the RL model\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the model\n",
    "#model.save(\"a2c_custom_env\")\n",
    "\n",
    "# Load the model\n",
    "model = A2C.load(\"a2c_custom_env\")\n",
    "\n",
    "# Test the trained model\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.68797433, -0.23337502, -2.07955743, -0.23329905,  1.97636615,\n",
       "       -0.29753903,  1.19219976, -0.10995019,  0.62325934, -0.54087482,\n",
       "       -1.40126717,  0.09196068, -0.67318411,  0.25431874, -1.16918542,\n",
       "       -0.5776434 , -0.93913452,  1.27568959, -0.00548958,  0.70132658,\n",
       "        0.80368286,  0.40092364,  0.39354961,  0.44299191,  0.08730106,\n",
       "       -0.35556862,  0.97029919, -0.7308742 , -1.36132043, -0.52721031,\n",
       "        0.59720783, -1.1262289 ,  0.24854871, -0.1575808 ,  1.5099382 ,\n",
       "       -2.18117213, -0.39265746, -0.15779116,  1.59910209,  2.68928255,\n",
       "       -0.9504317 , -0.62282263, -1.52180531, -0.2859528 , -1.63001379,\n",
       "        0.48177264,  0.68613896, -2.4051463 , -1.01129154, -1.00361373,\n",
       "       -0.492331  ,  1.51119645, -0.49177073,  0.89907323,  1.34829124,\n",
       "        0.35403551,  0.98313499,  1.25623839, -1.93534993])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳行動 (A1, A2, ..., A45): [-0.0778634   0.14584479 -0.10855152  0.25720716  0.12717877  0.10408416\n",
      " -0.26208588  0.05341439 -0.35347003  0.22465476  0.34642455  0.0775558\n",
      "  0.06647099 -0.18065456 -0.05863594  0.09651533  0.17540209  0.23628968\n",
      "  0.09649915  0.02422079  0.2353704  -0.16184193  0.08210889  0.0280089\n",
      "  0.31701857  0.02462357 -0.17210303  0.07415044  0.12877241  0.05255285\n",
      " -0.14221765 -0.04528626 -0.1163014  -0.37283963 -0.64668757  0.3215596\n",
      " -0.01330615  0.02589626  0.02697727  0.11285277 -0.19151497  0.20494309\n",
      " -0.3835866   0.13402124 -0.11572623]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# 加載已訓練的模型\n",
    "model = A2C.load(\"a2c_custom_env\")\n",
    "\n",
    "# 設置新的狀態和目標值\n",
    "new_state = np.array([0.5, -0.2, 1.0, -1.0, 0.3, -0.3, 0.8, -0.5, 0.2])  # 這是新的s1, s2, ..., s9\n",
    "new_target_y = np.array([1.2, -0.5, 0.7, -0.8, 1.0, -0.1, 0.5, -0.3, 0.4, 0.2, -0.4, 0.9, -0.6, 0.1, 0.7, -0.2, 0.3, 1.0, -1.0, 0.8,\n",
    "                         0.1, -0.1, 0.6, -0.4, 0.5, -0.7, 0.3, -0.2, 0.9, -0.5, 0.2, -0.3, 0.8, 0.0, -0.1, 0.7, -0.6, 0.4, 0.2, -0.8,\n",
    "                         1.1, -0.9, 0.3, -0.2, 0.5, -0.1, 0.4, 0.3, -0.7, 0.6])  # 這是新的Y1, Y2, ..., Y50\n",
    "\n",
    "# 合併狀態和目標值作為模型的觀測值\n",
    "new_obs = np.concatenate((new_state, new_target_y))\n",
    "\n",
    "# 通過模型進行預測以獲得最佳行動\n",
    "best_action, _states = model.predict(new_obs, deterministic=True)\n",
    "\n",
    "# 打印最佳行動\n",
    "print(\"最佳行動 (A1, A2, ..., A45):\", best_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m new_obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((new_state, new_target_y))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 評估模型\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Standard deviation of reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 16\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, env, num_episodes)\u001b[0m\n\u001b[0;32m     13\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 16\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     18\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\policies.py:368\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[0;32m    370\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\policies.py:717\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[1;34m(self, observation, deterministic)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    710\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;124;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\policies.py:752\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    750\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpi_features_extractor)\n\u001b[0;32m    751\u001b[0m latent_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_actor(features)\n\u001b[1;32m--> 752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\policies.py:691\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[1;34m(self, latent_pi)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_action_dist_from_latent\u001b[39m(\u001b[38;5;28mself\u001b[39m, latent_pi: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Distribution:\n\u001b[0;32m    685\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;124;03m    Retrieve action distribution given the latent codes.\u001b[39;00m\n\u001b[0;32m    687\u001b[0m \n\u001b[0;32m    688\u001b[0m \u001b[38;5;124;03m    :param latent_pi: Latent code for the actor\u001b[39;00m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;124;03m    :return: Action distribution\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 691\u001b[0m     mean_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, DiagGaussianDistribution):\n\u001b[0;32m    694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(mean_actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# 加載已訓練的模型\n",
    "#model = A2C.load(\"a2c_custom_env\")\n",
    "\n",
    "def evaluate_model(model, env, num_episodes=100):\n",
    "    all_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "    \n",
    "    mean_reward = np.mean(all_rewards)\n",
    "    std_reward = np.std(all_rewards)\n",
    "    \n",
    "    return mean_reward, std_reward\n",
    "\n",
    "# 設置新的狀態和目標值\n",
    "env = CustomEnv()  # 初始化自定義環境\n",
    "new_state = np.array([0.5, -0.2, 1.0, -1.0, 0.3, -0.3, 0.8, -0.5, 0.2])  # 這是新的s1, s2, ..., s9\n",
    "new_target_y = np.array([1.2, -0.5, 0.7, -0.8, 1.0, -0.1, 0.5, -0.3, 0.4, 0.2, -0.4, 0.9, -0.6, 0.1, 0.7, -0.2, 0.3, 1.0, -1.0, 0.8,\n",
    "                         0.1, -0.1, 0.6, -0.4, 0.5, -0.7, 0.3, -0.2, 0.9, -0.5, 0.2, -0.3, 0.8, 0.0, -0.1, 0.7, -0.6, 0.4, 0.2, -0.8,\n",
    "                         1.1, -0.9, 0.3, -0.2, 0.5, -0.1, 0.4, 0.3, -0.7, 0.6])  # 這是新的Y1, Y2, ..., Y50\n",
    "\n",
    "# 合併狀態和目標值作為模型的觀測值\n",
    "new_obs = np.concatenate((new_state, new_target_y))\n",
    "\n",
    "# 評估模型\n",
    "mean_reward, std_reward = evaluate_model(model, env, num_episodes=100)\n",
    "print(f\"Mean reward: {mean_reward}, Standard deviation of reward: {std_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1360     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 400      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.353    |\n",
      "|    n_updates        | 74       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1217     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 800      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.349    |\n",
      "|    n_updates        | 174      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 1090     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 1200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.292    |\n",
      "|    n_updates        | 274      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1041     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 1600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.338    |\n",
      "|    n_updates        | 374      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1041     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 2000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.558    |\n",
      "|    n_updates        | 474      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1021     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 2400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.345    |\n",
      "|    n_updates        | 574      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1021     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 2800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.44     |\n",
      "|    n_updates        | 674      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 1015     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 3200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.558    |\n",
      "|    n_updates        | 774      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 1007     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 3600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.212    |\n",
      "|    n_updates        | 874      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 1007     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 4000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.308    |\n",
      "|    n_updates        | 974      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 1010     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 4400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.613    |\n",
      "|    n_updates        | 1074     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 1012     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 4800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.348    |\n",
      "|    n_updates        | 1174     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 1017     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 5200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.314    |\n",
      "|    n_updates        | 1274     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 1018     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 5600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.467    |\n",
      "|    n_updates        | 1374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 1014     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 6000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.401    |\n",
      "|    n_updates        | 1474     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 1011     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 6400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.391    |\n",
      "|    n_updates        | 1574     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 1007     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 6800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.486    |\n",
      "|    n_updates        | 1674     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 1006     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 7200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.334    |\n",
      "|    n_updates        | 1774     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 1005     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 7600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.463    |\n",
      "|    n_updates        | 1874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 1005     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.468    |\n",
      "|    n_updates        | 1974     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 1006     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 8400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.506    |\n",
      "|    n_updates        | 2074     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 1005     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 8800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.353    |\n",
      "|    n_updates        | 2174     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 1006     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 9200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.48     |\n",
      "|    n_updates        | 2274     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 1008     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 9600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.47     |\n",
      "|    n_updates        | 2374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 1008     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.441    |\n",
      "|    n_updates        | 2474     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1f0d83db430>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Define your custom Gym environment\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, S, A, Y):\n",
    "        self.S = S\n",
    "        self.A = A\n",
    "        self.Y = Y\n",
    "        self.num_samples = S.shape[0]\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Action space: assume discrete actions for simplicity\n",
    "        self.action_space = spaces.Discrete(A.shape[1])\n",
    "        \n",
    "        # Observation space: S and Y concatenated\n",
    "        obs_dim_s = S.shape[1]\n",
    "        obs_dim_y = Y.shape[1]\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'S': spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim_s,), dtype=np.float32),\n",
    "            'Y': spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim_y,), dtype=np.float32),\n",
    "        })\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply action and get next state and reward\n",
    "        # For simplicity, a random reward function is used here\n",
    "        next_state = self._get_observation()\n",
    "        reward = np.random.normal()  # Replace with your reward function\n",
    "        done = (self.current_step == self.num_samples - 1)\n",
    "        self.current_step += 1\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        return np.concatenate((self.S[self.current_step], self.Y[self.current_step]))\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "S = np.random.randn(100, 9)\n",
    "A = np.random.randn(100, 45)\n",
    "Y = np.random.randn(100, 23)\n",
    "\n",
    "# Create the custom environment\n",
    "env = CustomEnv(S, A, Y)\n",
    "\n",
    "# Wrap the environment in a vectorized form\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Initialize and train the RL agent (using PPO as an example)\n",
    "model = DQN(\"MlpPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save and reload the model (optional)\n",
    "#model.save(\"rl_model\")\n",
    "# model = PPO.load(\"rl_model\")\n",
    "\n",
    "# Test the trained model\n",
    "# obs = env.reset()\n",
    "# for _ in range(10):  # Test for 10 steps\n",
    "#     action, _ = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     print(f\"Action: {action}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 官網標準寫法\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    vec_env.render()\n",
    "    # VecEnv resets automatically\n",
    "    # if done:\n",
    "    #   obs = env.reset()\n",
    "\n",
    "env.close()\n",
    "#\n",
    "\n",
    "\n",
    "# 這邊是觀察內部\n",
    "for i_episode in range(5): #how many episodes you want to run\n",
    "    observation = env.reset() #reset() returns initial observation\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "\n",
    "# 這邊則是進行預測\n",
    "for _ in range(10):  # Test for 10 steps\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "obss = np.concatenate([np.random.randn(100, 9),np.random.randn(100, 23)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, _ = model.predict(obss, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 14,  3,  7, 33,  6, 27, 17, 42,  7,  6, 24,  6, 24, 17, 23, 33,\n",
       "       23, 14,  8,  3, 14, 24, 24, 23, 14,  7, 24, 23, 14, 23, 33, 24, 14,\n",
       "        9, 14, 35, 24, 35, 14, 14, 35, 14, 23, 23, 14, 14, 14, 41, 17, 14,\n",
       "        6, 14, 14,  6, 24,  6,  6,  6, 10, 14, 14,  9,  6, 14, 16, 24,  9,\n",
       "       17, 21, 14,  8,  6,  7, 23, 14, 16,  7, 14, 14,  8, 14,  6,  9, 17,\n",
       "       14,  6, 14, 23,  9,  6, 17, 23, 24, 24, 24, 14, 14,  8, 16],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.56    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 2.75     |\n",
      "|    value_loss         | 1.14     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 387      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.22    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -5.22    |\n",
      "|    value_loss         | 2.47     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 387      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.57    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 6.55     |\n",
      "|    value_loss         | 16.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 388      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -7.38    |\n",
      "|    value_loss         | 6.36     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 388      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.2     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.57     |\n",
      "|    value_loss         | 1.05     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 388      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.93    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -5.11    |\n",
      "|    value_loss         | 2.71     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.78    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -1.32    |\n",
      "|    value_loss         | 0.704    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 4.66     |\n",
      "|    value_loss         | 1.58     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.14    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.939    |\n",
      "|    value_loss         | 1.44     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -1.18    |\n",
      "|    value_loss         | 0.911    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.422   |\n",
      "|    value_loss         | 2.35     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.654   |\n",
      "|    value_loss         | 0.436    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 389      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -4.12    |\n",
      "|    value_loss         | 4.87     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 389      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 1.35     |\n",
      "|    value_loss         | 0.959    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 389      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.987    |\n",
      "|    value_loss         | 0.458    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.103    |\n",
      "|    value_loss         | 0.204    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 388      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.309    |\n",
      "|    value_loss         | 3.11     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 389      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.797    |\n",
      "|    value_loss         | 0.991    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 387       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2        |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -11.6     |\n",
      "|    value_loss         | 19.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 385       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.01     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 4.49      |\n",
      "|    value_loss         | 3.98      |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1f0c1e29040>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Wrap the environment to make it compatible with Stable Baselines\n",
    "\n",
    "# Create the DQN model\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: 0.2619406376685048\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example usage for evaluating the trained model\n",
    "# Assume `model` is the trained RL model\n",
    "\n",
    "# Test the trained model\n",
    "total_rewards = []\n",
    "num_episodes = 100\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "    \n",
    "    total_rewards.append(episode_reward)\n",
    "\n",
    "# Calculate average reward\n",
    "avg_reward = np.mean(total_rewards)\n",
    "print(f\"Average Reward over {num_episodes} episodes: {avg_reward}\")\n",
    "\n",
    "# Plotting the learning curve\n",
    "plt.plot(total_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Learning Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1572 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1042       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00271051 |\n",
      "|    clip_fraction        | 0.00806    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.81      |\n",
      "|    explained_variance   | -0.000301  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.2e+03    |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0251    |\n",
      "|    value_loss           | 1.24e+04   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 927          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009848682 |\n",
      "|    clip_fraction        | 0.00156      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.8         |\n",
      "|    explained_variance   | -0.0356      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.13e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0149      |\n",
      "|    value_loss           | 1.15e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 889          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007119263 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.8         |\n",
      "|    explained_variance   | -0.0239      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.8e+03      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0112      |\n",
      "|    value_loss           | 1.07e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 857         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000791863 |\n",
      "|    clip_fraction        | 0.000439    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.8        |\n",
      "|    explained_variance   | 0.00325     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.01e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 1.08e+04    |\n",
      "-----------------------------------------\n",
      "Action: 9, Reward: -7.059324569968333\n",
      "Action: 9, Reward: -6.30340323881022\n",
      "Action: 28, Reward: -7.409127959530675\n",
      "Action: 1, Reward: -6.098285931230457\n",
      "Action: 15, Reward: -6.698052947019561\n",
      "Action: 18, Reward: -8.455684792302824\n",
      "Action: 32, Reward: -6.495087073898289\n",
      "Action: 11, Reward: -8.605197314420831\n",
      "Action: 33, Reward: -7.285786624126287\n",
      "Action: 29, Reward: -5.555096322629754\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Define your custom Gym environment\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, S, A, Y, R=None):\n",
    "        self.S = S\n",
    "        self.A = A\n",
    "        self.Y = Y\n",
    "        self.R = R  # Optional reward matrix based on Y\n",
    "        self.num_samples = S.shape[0]\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Action space: assume discrete actions for simplicity\n",
    "        self.action_space = spaces.Discrete(A.shape[1])\n",
    "        \n",
    "        # Observation space: separate S and Y\n",
    "        obs_dim_s = S.shape[1]\n",
    "        obs_dim_y = Y.shape[1]\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'S': spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim_s,), dtype=np.float32),\n",
    "            'Y': spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim_y,), dtype=np.float32),\n",
    "        })\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply action and get next state and reward\n",
    "        next_state = self._get_observation()\n",
    "        reward = self._calculate_reward(action)\n",
    "        done = (self.current_step == self.num_samples - 1)\n",
    "        self.current_step += 1\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        return {\n",
    "            'S': self.S[self.current_step],\n",
    "            'Y': self.Y[self.current_step]\n",
    "        }\n",
    "    \n",
    "    def _calculate_reward(self, action):\n",
    "        # Example: using a reward matrix R based on expected Y values\n",
    "        if self.R is not None:\n",
    "            ideal_y = self.R[action]  # Ideal Y value for the chosen action\n",
    "            actual_y = self.Y[self.current_step]  # Actual Y value\n",
    "            reward = -np.linalg.norm(actual_y - ideal_y)  # Negative L2 norm difference as reward\n",
    "        else:\n",
    "            # Default reward function (replace with your own logic)\n",
    "            reward = np.sum(self.A[self.current_step] * action)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "S = np.random.randn(1000, 9)\n",
    "A = np.random.randn(1000, 45)\n",
    "Y = np.random.randn(1000, 23)\n",
    "\n",
    "# Example reward matrix R based on Y (replace with your own logic)\n",
    "R = np.random.randn(45, 23)  # Example: random R matrix for 45 actions and 23 Y features\n",
    "\n",
    "# Create the custom environment\n",
    "env = CustomEnv(S, A, Y, R)\n",
    "\n",
    "# Wrap the environment in a vectorized form\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Initialize and train the RL agent (using PPO as an example)\n",
    "model = PPO(\"MultiInputPolicy\", vec_env, verbose=1) #  MultiInputPolicy很重要\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save and reload the model (optional)\n",
    "model.save(\"rl_model\")\n",
    "# model = PPO.load(\"rl_model\")\n",
    "\n",
    "# Test the trained model\n",
    "obs = env.reset()\n",
    "for _ in range(10):  # Test for 10 steps\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': array([ 0.21612199, -0.2774042 ,  0.18650078, -0.03265162, -0.3022883 ,\n",
       "         0.08037136,  0.03816191, -0.26109779, -0.127289  ]),\n",
       " 'Y': array([-0.12420115, -0.09958756,  0.88847959,  1.25859574, -0.32624944,\n",
       "         0.83981638, -0.32178181, -0.38048443, -1.08656403, -0.28067518,\n",
       "         1.64137828, -0.06652959,  2.29549738, -0.47086446,  0.34647372,\n",
       "         1.55156816,  0.77892352, -1.18782682, -0.07429324,  0.26817989,\n",
       "         0.62560905, -0.73113462,  0.86687187])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 79   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 25   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037599713 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -207        |\n",
      "|    explained_variance   | 0.000357    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.75e+05    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.068      |\n",
      "|    value_loss           | 5.63e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012403216 |\n",
      "|    clip_fraction        | 0.0758      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -207        |\n",
      "|    explained_variance   | 0.00149     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.82e+05    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 5.57e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 59           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 137          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071466733 |\n",
      "|    clip_fraction        | 0.0504       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -207         |\n",
      "|    explained_variance   | 0.00316      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.75e+05     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0463      |\n",
      "|    value_loss           | 5.52e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 58           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 174          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070135687 |\n",
      "|    clip_fraction        | 0.0496       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -207         |\n",
      "|    explained_variance   | 0.000492     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+05     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0455      |\n",
      "|    value_loss           | 5.52e+05     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, S, A, Y, R=None):\n",
    "        self.S = S\n",
    "        self.A = A\n",
    "        self.Y = Y\n",
    "        self.R = R  # Optional reward matrix based on Y\n",
    "        self.num_samples = S.shape[0]\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Action space: assume discrete actions for simplicity\n",
    "        action_dim = A.shape[1]\n",
    "        self.action_space = spaces.MultiDiscrete([100]*action_dim)\n",
    "        \n",
    "        # Observation space: separate S and Y with specific labels\n",
    "        obs_dim_s = S.shape[1]\n",
    "        obs_dim_y = Y.shape[1]\n",
    "        observation_space = {}\n",
    "        for i in range(obs_dim_s):\n",
    "            observation_space[f'S_{i+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        for j in range(obs_dim_y):\n",
    "            observation_space[f'Y_{j+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Dict(observation_space)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply action and get next state and reward\n",
    "        next_state = self._get_observation()\n",
    "        reward = self._calculate_reward(action)\n",
    "        done = (self.current_step == self.num_samples - 1)\n",
    "        self.current_step += 1\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        obs = {}\n",
    "        for i in range(self.S.shape[1]):\n",
    "            obs[f'S_{i+1}'] = np.array([self.S[self.current_step, i]])\n",
    "        for j in range(self.Y.shape[1]):\n",
    "            obs[f'Y_{j+1}'] = np.array([self.Y[self.current_step, j]])\n",
    "        return obs\n",
    "    \n",
    "    def _calculate_reward(self, action):\n",
    "        # Example: using a reward matrix R based on expected Y values\n",
    "        if self.R is not None:\n",
    "            #print(action)\n",
    "            #print(self.R)\n",
    "            ideal_y = self.R[action]  # Ideal Y value for the chosen action\n",
    "            actual_y = self.Y[self.current_step]  # Actual Y value\n",
    "            reward = -np.linalg.norm(actual_y - ideal_y)  # Negative L2 norm difference as reward\n",
    "        else:\n",
    "            # Default reward function (replace with your own logic)\n",
    "            reward = np.sum(self.A[self.current_step] * action)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data (replace with your actual data)\n",
    "    S = np.random.randn(1000, 9)\n",
    "    A = np.random.randint(low=1, high=100,size=(1000, 45))\n",
    "    Y = np.random.randn(1000, 23)\n",
    "    \n",
    "    # Example reward matrix R based on Y (replace with your own logic)\n",
    "    R = np.random.randn(100, 23)  # Example: random R matrix for 45 actions and 23 Y features\n",
    "    \n",
    "    # Create the custom environment\n",
    "    env = CustomEnv(S, A, Y, R)\n",
    "    \n",
    "    # Wrap the environment in a vectorized form\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    # Initialize and train the RL agent (using PPO as an example)\n",
    "    model = PPO(\"MultiInputPolicy\", vec_env, verbose=1)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    \n",
    "    # Save and reload the model (optional)\n",
    "    #model.save(\"rl_model\")\n",
    "    # model = PPO.load(\"rl_model\")\n",
    "    \n",
    "    # Test the trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "discrete_action_max = 1000\n",
    "\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, S, A, Y, R, discrete_action_max):\n",
    "        self.S = torch.tensor(S, dtype=torch.float32)  # Convert S to torch tensor\n",
    "        self.A = torch.tensor(A, dtype=torch.float32)  # Convert A to torch tensor\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)  # Convert Y to torch tensor\n",
    "        self.R = torch.tensor(R, dtype=torch.float32) if R is not None else None  # Convert R to torch tensor if provided\n",
    "        self.num_samples = S.shape[0]\n",
    "        self.current_step = 0\n",
    "        self.discrete_action_max = discrete_action_max\n",
    "        \n",
    "        # Action space: assume discrete actions for simplicity\n",
    "        action_dim = A.shape[1]\n",
    "        self.action_space = spaces.MultiDiscrete([self.discrete_action_max]*action_dim)\n",
    "        \n",
    "        # Observation space: separate S and Y with specific labels\n",
    "        obs_dim_s = S.shape[1]\n",
    "        obs_dim_y = Y.shape[1]\n",
    "        observation_space = {}\n",
    "        for i in range(obs_dim_s):\n",
    "            observation_space[f'S_{i+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        for j in range(obs_dim_y):\n",
    "            observation_space[f'Y_{j+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Dict(observation_space)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply action and get next state and reward\n",
    "        next_state = self._get_observation()\n",
    "        reward = self._calculate_reward(action)\n",
    "        done = (self.current_step == self.num_samples - 1)\n",
    "        self.current_step += 1\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        obs = {}\n",
    "        for i in range(self.S.shape[1]):\n",
    "            obs[f'S_{i+1}'] = np.array([self.S[self.current_step, i]])\n",
    "        for j in range(self.Y.shape[1]):\n",
    "            obs[f'Y_{j+1}'] = np.array([self.Y[self.current_step, j]])\n",
    "        return obs\n",
    "    \n",
    "    def _calculate_reward(self, action):\n",
    "        # Example: using a reward matrix R based on expected Y values\n",
    "        if self.R is not None:\n",
    "            ideal_y = self.R[action]  # Ideal Y value for the chosen action\n",
    "            actual_y = self.Y[self.current_step]  # Actual Y value\n",
    "            reward = -torch.norm(actual_y - ideal_y)  # Negative L2 norm difference as reward\n",
    "        else:\n",
    "            # Default reward function (replace with your own logic)\n",
    "            reward = torch.sum(self.A[self.current_step] * action)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.ppo.ppo.PPO'>\n",
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 54   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 37   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 37            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 110           |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059329194 |\n",
      "|    clip_fraction        | 0.0043        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -311          |\n",
      "|    explained_variance   | -5.72e-06     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.69e+08      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.0187       |\n",
      "|    value_loss           | 9.43e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 184          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.256879e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -311         |\n",
      "|    explained_variance   | -1.51e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.69e+08     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00646     |\n",
      "|    value_loss           | 9.42e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 30            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 264           |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4589262e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -311          |\n",
      "|    explained_variance   | -9.18e-06     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.73e+08      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00376      |\n",
      "|    value_loss           | 9.42e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 29            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 346           |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6604877e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -311          |\n",
      "|    explained_variance   | 5.07e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.7e+08       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00385      |\n",
      "|    value_loss           | 9.42e+08      |\n",
      "-------------------------------------------\n",
      "Action: [[257 696 371 515 563 203 621 745 273 897 417 652 473 813 439  55 645 428\n",
      "  391 543 428 586 598 758 265 874 218 661 392  32 471 289 361 722 780   9\n",
      "  856 128 647 152 151 545 112 505 570]], Reward: -1832.8330078125, 14\n",
      "Action: [[420 776 135 900 558 204 189 408 262 855 825 304 232 857 230 944 610  28\n",
      "  588 476 102 946 913  53  54 805 630 741 686 963 528 395 391 150 138 812\n",
      "  207 286 285 260 890 942 645 579 882]], Reward: -1824.3770751953125, 24\n",
      "Action: [[608 138 815  89 628 499 941 365 917 273 344 683 726  86 901 154  54 742\n",
      "  812 908 260 902 723  48  42 627 295 614  10 848 894 862 140 520  92 371\n",
      "  933 986 881 728 808 503 561 685 352]], Reward: -1804.5111083984375, 38\n",
      "Action: [[407 192 383   7 782 982 972  61 185 603 672 647 233 535 718 500 677 221\n",
      "  212 808 421 352 247 410 696 258 113 575 868   7 126 133 959  75 601 310\n",
      "  537 645 899 480 462 240  92 579 960]], Reward: -1802.0101318359375, 92\n",
      "Action: [[323 215 964 417 589 933 891 999 178 518 347 324 177 913 285  57 760 172\n",
      "  443 709 996 292 593 529 235 663 358 149 959 847 115 838 995 553 637 678\n",
      "  762 314 724 972 633  52 139 952 739]], Reward: -1799.919189453125, 93\n",
      "<class 'stable_baselines3.a2c.a2c.A2C'>\n",
      "Using cpu device\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 31        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -311      |\n",
      "|    explained_variance | 0.000225  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -1.71e+06 |\n",
      "|    value_loss         | 3.68e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 30        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -311      |\n",
      "|    explained_variance | 0.00162   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.71e+06 |\n",
      "|    value_loss         | 3.67e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 30        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -311      |\n",
      "|    explained_variance | 0.0012    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -1.69e+06 |\n",
      "|    value_loss         | 3.61e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 30        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -311      |\n",
      "|    explained_variance | 0.000826  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -1.71e+06 |\n",
      "|    value_loss         | 3.7e+07   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 29        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -311      |\n",
      "|    explained_variance | 0.000211  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -1.71e+06 |\n",
      "|    value_loss         | 3.71e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 29        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -311      |\n",
      "|    explained_variance | 9.44e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -1.71e+06 |\n",
      "|    value_loss         | 3.72e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 30        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -310      |\n",
      "|    explained_variance | 3.28e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -1.72e+06 |\n",
      "|    value_loss         | 3.71e+07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 30       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 132      |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -311     |\n",
      "|    explained_variance | 1.82e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -1.7e+06 |\n",
      "|    value_loss         | 3.65e+07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 30        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 148       |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -310      |\n",
      "|    explained_variance | 7.75e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -1.72e+06 |\n",
      "|    value_loss         | 3.76e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 30        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 164       |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -311      |\n",
      "|    explained_variance | 5.01e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -1.72e+06 |\n",
      "|    value_loss         | 3.74e+07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 30       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 180      |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -310     |\n",
      "|    explained_variance | 2.26e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -1.7e+06 |\n",
      "|    value_loss         | 3.66e+07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 30        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 195       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -310      |\n",
      "|    explained_variance | 1.67e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -1.71e+06 |\n",
      "|    value_loss         | 3.7e+07   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 30       |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 212      |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -310     |\n",
      "|    explained_variance | 9.54e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -1.7e+06 |\n",
      "|    value_loss         | 3.69e+07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 30        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 228       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -310      |\n",
      "|    explained_variance | 5.96e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -1.73e+06 |\n",
      "|    value_loss         | 3.76e+07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 30       |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 244      |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -310     |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -1.7e+06 |\n",
      "|    value_loss         | 3.66e+07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 30        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 260       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -310      |\n",
      "|    explained_variance | 2.98e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -1.72e+06 |\n",
      "|    value_loss         | 3.72e+07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 30       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 277      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -309     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -1.7e+06 |\n",
      "|    value_loss         | 3.64e+07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 30       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 294      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -310     |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -1.7e+06 |\n",
      "|    value_loss         | 3.68e+07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 30        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 311       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -309      |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -1.69e+06 |\n",
      "|    value_loss         | 3.66e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 30        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 328       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -310      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -1.71e+06 |\n",
      "|    value_loss         | 3.68e+07  |\n",
      "-------------------------------------\n",
      "Action: [[684 972 617 182 703 975 238 340 138 130 878 253 365 973 889 290 887 259\n",
      "  368 801 816 241 544 811 836 293 165 736 438 819 317 563 530 636 724 120\n",
      "  568 578 371 789 773 765 680 658 766]], Reward: -1857.158447265625, 1\n",
      "Action: [[710  70 382 946 939 849 597 893 117 122 840 813 740 570 815 661   5 275\n",
      "  881 276 544   2   0 551 243 931 726 500 514   8 154 793 142 727 970 935\n",
      "  932 962 553 781 603 951 480 254 979]], Reward: -1849.896240234375, 14\n",
      "Action: [[481 870  45 482 323  94  24 842   6 761 185 450 214 872 684 195  73 414\n",
      "  751 333 259 952 504 931  61 246 478  16  58 612 840 773 126 957 566 344\n",
      "  750 426 921 707 434 360 966   5 538]], Reward: -1847.2413330078125, 16\n",
      "Action: [[590 789 793 299  21 987  75 841 292 828 617 298 616 590 757 259 853 566\n",
      "  118 779 802 429 206 841 277 312 406 388  16 630 117 913 745 506 866 974\n",
      "  871 583 246 950  46  67 946 396 887]], Reward: -1814.2288818359375, 17\n",
      "Action: [[847 652 636 497 448 348 761 801 575 823  31 543 388 666 421 304 825  52\n",
      "  624 923 823 705 606 297 307 162 421 399 724 388 324 284 450 657 229 727\n",
      "  503   5 995 973  79 576 221 656 293]], Reward: -1805.2562255859375, 98\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "S = np.random.randn(1000, 9)\n",
    "A = np.random.randint(low=10, high=discrete_action_max, size=(1000, 45))\n",
    "Y = np.random.randn(1000, 23)\n",
    "\n",
    "# Example reward matrix R based on Y (replace with your own logic)\n",
    "R = np.random.uniform(low=-100,high=100,size=(discrete_action_max, 23))  # Example: random R matrix for 45 actions and 23 Y features\n",
    "\n",
    "# Create the custom environment\n",
    "env = CustomEnv(S, A, Y, R,discrete_action_max=discrete_action_max)\n",
    "\n",
    "# Wrap the environment in a vectorized form\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "\n",
    "from stable_baselines3 import PPO,A2C\n",
    "\n",
    "\n",
    "for RL_model in [PPO,A2C]:\n",
    "    print(RL_model)\n",
    "    # Initialize and train the RL agent (using PPO as an example)\n",
    "    model = RL_model(\"MultiInputPolicy\", vec_env, verbose=1)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    new_obs = dict()\n",
    "    for i in range(S.shape[1]):\n",
    "        new_obs[f'S_{i+1}'] = np.array([np.random.randn(1)])\n",
    "    for j in range(Y.shape[1]):\n",
    "        new_obs[f'Y_{j+1}'] = np.array([np.random.randn(1)])\n",
    "    r = None\n",
    "    best_action = None\n",
    "    for i in range(100):  # Test for 10 steps\n",
    "        action, _ = model.predict(new_obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if r == None:\n",
    "            r = reward\n",
    "        elif r < reward:\n",
    "            r = reward\n",
    "            best_action = action\n",
    "\n",
    "            print(f\"Action: {best_action}, Reward: {r}, {i}\")\n",
    "# Save and reload the model (optional)\n",
    "#model.save(\"rl_model\")\n",
    "# model = PPO.load(\"rl_model\")\n",
    "\n",
    "# Test the trained model\n",
    "\n",
    "\n",
    "# # 官網標準寫法\n",
    "# env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# model.learn(total_timesteps=10_000)\n",
    "\n",
    "# vec_env = model.get_env()\n",
    "# obs = vec_env.reset()\n",
    "# for i in range(1000):\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, info = vec_env.step(action)\n",
    "#     vec_env.render()\n",
    "#     # VecEnv resets automatically\n",
    "#     # if done:\n",
    "#     #   obs = env.reset()\n",
    "\n",
    "# env.close()\n",
    "# #\n",
    "\n",
    "\n",
    "# # 這邊是觀察內部\n",
    "# for i_episode in range(5): #how many episodes you want to run\n",
    "#     observation = env.reset() #reset() returns initial observation\n",
    "#     for t in range(200):\n",
    "#         env.render()\n",
    "#         print(observation)\n",
    "#         action = env.action_space.sample()\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         if done:\n",
    "#             print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "#             break\n",
    "\n",
    "# # 這邊則是進行預測\n",
    "# for _ in range(10):  # Test for 10 steps\n",
    "#     action, _ = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     print(f\"Action: {action}, Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.ppo.ppo.PPO'>\n",
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 53   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 38   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 37            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 110           |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00063626096 |\n",
      "|    clip_fraction        | 0.00483       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -311          |\n",
      "|    explained_variance   | 1.04e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.77e+08      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.0191       |\n",
      "|    value_loss           | 9.42e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 185          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.029446e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -311         |\n",
      "|    explained_variance   | -8.23e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.75e+08     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00677     |\n",
      "|    value_loss           | 9.43e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 31            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 258           |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6353013e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -311          |\n",
      "|    explained_variance   | 3.08e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.74e+08      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00385      |\n",
      "|    value_loss           | 9.42e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 30           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 330          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.764988e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -311         |\n",
      "|    explained_variance   | 1.71e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.75e+08     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00393     |\n",
      "|    value_loss           | 9.42e+08     |\n",
      "------------------------------------------\n",
      "Action: [[859 977 119 775 801 632 946 969  61 803 906 900 693 181 488  39 291  74\n",
      "  769 654 977 538 295 682 486 308 140 966 352 338 970 392 173 655 555 870\n",
      "  848 603 214 328 310 433 459 142 297]], Reward: -1821.205322265625, 6\n",
      "Action: [[607 465 915 657 193 270 664 325  93 924 428 457 233  85 996  63 899 174\n",
      "  754 793 555 331 967 870 343 261 651 870 706 307 149 621  81 455 154 313\n",
      "  564 456 464 295 291  21 859 729 142]], Reward: -1818.2998046875, 15\n",
      "Action: [[116 279 164 482  48 335 185   4  98 862 628 715 423 640 972  55 656 985\n",
      "  656 100 468 879  14 398 882 310 859 920 883 511 948 685 624  52 479 940\n",
      "  292 116 537 667 204 309 892 804 159]], Reward: -1799.9613037109375, 22\n",
      "Action: [[939 742 465 490 994 961 776 622 990 721 894 817 881 644 230 780  84 613\n",
      "  788 978 129 716 577 693  52 949 843 649 972 902 852 556 550 354 896 478\n",
      "  853 233 288 911  73 919 285 944 220]], Reward: -1798.02783203125, 146\n",
      "Action: [[898 335  39 524 865 675 479 410 412 785 355 733 682 891 666 407  78 342\n",
      "  768 736 778 501 939 636  39 470 778 272 276 901 541 844 307 753 513 252\n",
      "  954 501 372 635 954 734 715 858 214]], Reward: -1791.9622802734375, 156\n",
      "Action: [[688 699 412 256  78 152 865 582 540 563 362  28 502 533   4 140 282 404\n",
      "   20 728 481 974  75 879  92 604 396 879 895   2 547 348 638 571 779 785\n",
      "  342 995  38 682 519 228 276 787  16]], Reward: -1773.173828125, 261\n",
      "Action: [[442 131 669 557 343 537 303 270 308 447 500 611 488 762 335 704 744 214\n",
      "  153 644 320 840 564 368 203 366 922 198 987   5 525 584 731 693 184  80\n",
      "  144 609 483 647 213 164 173 864 576]], Reward: -1767.9849853515625, 459\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1000 is out of bounds for dimension 0 with size 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kkkkkk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):  \u001b[38;5;66;03m# Test for 1000 steps\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(new_obs)\n\u001b[1;32m---> 15\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m         r \u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[6], line 42\u001b[0m, in \u001b[0;36mCustomEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Apply action and get next state and reward\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_reward(action)\n\u001b[0;32m     44\u001b[0m     done \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 51\u001b[0m, in \u001b[0;36mCustomEnv._get_observation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m obs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m---> 51\u001b[0m     obs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m])\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m     53\u001b[0m     obs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step, j]])\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1000 is out of bounds for dimension 0 with size 1000"
     ]
    }
   ],
   "source": [
    "for RL_model in [PPO,A2C]:\n",
    "    print(RL_model)\n",
    "    # Initialize and train the RL agent (using PPO as an example)\n",
    "    model = RL_model(\"MultiInputPolicy\", vec_env, verbose=1)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    new_obs = dict()\n",
    "    for i in range(S.shape[1]):\n",
    "        new_obs[f'S_{i+1}'] = np.array([np.random.randn(1)])\n",
    "    for j in range(Y.shape[1]):\n",
    "        new_obs[f'Y_{j+1}'] = np.array([np.random.randn(1)])\n",
    "    r = None\n",
    "    best_action = None\n",
    "    for kkkkkk in range(1000):  # Test for 1000 steps\n",
    "        action, _ = model.predict(new_obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if r == None:\n",
    "            r = reward\n",
    "        elif r < reward:\n",
    "            r = reward\n",
    "            best_action = action\n",
    "\n",
    "            print(f\"Action: {best_action}, Reward: {r}, {kkkkkk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x20519a62430>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [[ 346 3301 4950  231 9982 1267 5578 5405 1448 1306 2188 2240 1389 8888\n",
      "  6087 4519 4598 6955 5826 6926 1892 4558 9558 6319 6369  645 5675 3830\n",
      "  1653 8167 8493 9182 9381 3805 2296 2930 8986 5164 8139 3192 4044 2616\n",
      "  7993 3491 4640]], Reward: -1868.0452880859375, 1\n",
      "Action: [[5731 1093  339 6056 2416 1782 1185 3732 4084 2192 2422 8413 6818  426\n",
      "   712 4277 7799 2483 9223 8081 2218  873 9848 5805 9006 6084 1019 3598\n",
      "  2319 9289  281 1525 1718 8860 2941 4219 4577  433 4621 3706  423 1926\n",
      "  2014 7846 9479]], Reward: -1834.482177734375, 2\n",
      "Action: [[8201 8043 2316 7600 1062 5547 8239 4554 6959 3334 4054 9622 5016 8152\n",
      "  3100 9848 7208 1884 8049 7013 8937 5607 1401 3083 9326  572 2671 4930\n",
      "  3719 4051 4072 4431 2420 7958 7405 2688 2433 9568 3683 8967 1163 8205\n",
      "  5306 4664 8452]], Reward: -1815.260986328125, 3\n",
      "Action: [[8496 7805  664 4239 4329 4717 7510 6100 7084 2302 4518 1603 5710 4234\n",
      "   492 3521 5464 6494 8944 2600 9533 8562 8949 3140 5086 6788 9110 7318\n",
      "  4995 3643 3876 8940 1280 5825 4914 8305 5647 9997 9927  532 8746 6298\n",
      "  9912 7290 9584]], Reward: -1815.188720703125, 29\n",
      "Action: [[5266 5913 5867 5107 8073  220 7191 1267 2864 2559 2058 6739 9777 1401\n",
      "  2460 3004 2803 8077 3295 8970 2348 2388 1491 6960 4081 7127 9522 4747\n",
      "  4999 8109  290 8230 5876 3982 5296 6146   26  896 2784 2445 2341 8638\n",
      "  2886 5811 8441]], Reward: -1805.8834228515625, 41\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The algorithm only supports (<class 'gymnasium.spaces.box.Box'>, <class 'gymnasium.spaces.discrete.Discrete'>, <class 'gymnasium.spaces.multi_discrete.MultiDiscrete'>, <class 'gymnasium.spaces.multi_binary.MultiBinary'>) as action spaces but Dict('A_1': Box(-1.0, 1.0, (1,), float32), 'A_10': Box(-1.0, 1.0, (1,), float32), 'A_11': Box(-1.0, 1.0, (1,), float32), 'A_12': Box(-1.0, 1.0, (1,), float32), 'A_13': Box(-1.0, 1.0, (1,), float32), 'A_14': Box(-1.0, 1.0, (1,), float32), 'A_15': Box(-1.0, 1.0, (1,), float32), 'A_16': Box(-1.0, 1.0, (1,), float32), 'A_17': Box(-1.0, 1.0, (1,), float32), 'A_18': Box(-1.0, 1.0, (1,), float32), 'A_19': Box(-1.0, 1.0, (1,), float32), 'A_2': Box(-1.0, 1.0, (1,), float32), 'A_20': Box(-1.0, 1.0, (1,), float32), 'A_21': Box(-1.0, 1.0, (1,), float32), 'A_22': Box(-1.0, 1.0, (1,), float32), 'A_23': Box(-1.0, 1.0, (1,), float32), 'A_24': Box(-1.0, 1.0, (1,), float32), 'A_25': Box(-1.0, 1.0, (1,), float32), 'A_26': Box(-1.0, 1.0, (1,), float32), 'A_27': Box(-1.0, 1.0, (1,), float32), 'A_28': Box(-1.0, 1.0, (1,), float32), 'A_29': Box(-1.0, 1.0, (1,), float32), 'A_3': Box(-1.0, 1.0, (1,), float32), 'A_30': Box(-1.0, 1.0, (1,), float32), 'A_31': Box(-1.0, 1.0, (1,), float32), 'A_32': Box(-1.0, 1.0, (1,), float32), 'A_33': Box(-1.0, 1.0, (1,), float32), 'A_34': Box(-1.0, 1.0, (1,), float32), 'A_35': Box(-1.0, 1.0, (1,), float32), 'A_36': Box(-1.0, 1.0, (1,), float32), 'A_37': Box(-1.0, 1.0, (1,), float32), 'A_38': Box(-1.0, 1.0, (1,), float32), 'A_39': Box(-1.0, 1.0, (1,), float32), 'A_4': Box(-1.0, 1.0, (1,), float32), 'A_40': Box(-1.0, 1.0, (1,), float32), 'A_41': Box(-1.0, 1.0, (1,), float32), 'A_42': Box(-1.0, 1.0, (1,), float32), 'A_43': Box(-1.0, 1.0, (1,), float32), 'A_44': Box(-1.0, 1.0, (1,), float32), 'A_45': Box(-1.0, 1.0, (1,), float32), 'A_5': Box(-1.0, 1.0, (1,), float32), 'A_6': Box(-1.0, 1.0, (1,), float32), 'A_7': Box(-1.0, 1.0, (1,), float32), 'A_8': Box(-1.0, 1.0, (1,), float32), 'A_9': Box(-1.0, 1.0, (1,), float32)) was provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: env])\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Initialize and train the RL agent (using PPO as an example)\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Save and reload the model (optional)\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#model.save(\"rl_model\")\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# model = PPO.load(\"rl_model\")\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Test the trained model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:109\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     82\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m     _init_setup_model: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    108\u001b[0m ):\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgae_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgae_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43ment_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ment_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvf_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvf_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_buffer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_buffer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_buffer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_buffer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_init_setup_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDiscrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiDiscrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiBinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# because of the advantage normalization\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize_advantage:\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:85\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     62\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[38;5;241m.\u001b[39mSpace], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     84\u001b[0m ):\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupport_multi_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msupported_action_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps \u001b[38;5;241m=\u001b[39m n_steps\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m gamma\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:180\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vec_normalize_env \u001b[38;5;241m=\u001b[39m unwrap_vec_normalize(env)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m supported_action_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, supported_action_spaces), (\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe algorithm only supports \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_action_spaces\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as action spaces \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was provided\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m support_multi_env \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: the model does not support multiple envs; it requires \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma single vectorized environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: The algorithm only supports (<class 'gymnasium.spaces.box.Box'>, <class 'gymnasium.spaces.discrete.Discrete'>, <class 'gymnasium.spaces.multi_discrete.MultiDiscrete'>, <class 'gymnasium.spaces.multi_binary.MultiBinary'>) as action spaces but Dict('A_1': Box(-1.0, 1.0, (1,), float32), 'A_10': Box(-1.0, 1.0, (1,), float32), 'A_11': Box(-1.0, 1.0, (1,), float32), 'A_12': Box(-1.0, 1.0, (1,), float32), 'A_13': Box(-1.0, 1.0, (1,), float32), 'A_14': Box(-1.0, 1.0, (1,), float32), 'A_15': Box(-1.0, 1.0, (1,), float32), 'A_16': Box(-1.0, 1.0, (1,), float32), 'A_17': Box(-1.0, 1.0, (1,), float32), 'A_18': Box(-1.0, 1.0, (1,), float32), 'A_19': Box(-1.0, 1.0, (1,), float32), 'A_2': Box(-1.0, 1.0, (1,), float32), 'A_20': Box(-1.0, 1.0, (1,), float32), 'A_21': Box(-1.0, 1.0, (1,), float32), 'A_22': Box(-1.0, 1.0, (1,), float32), 'A_23': Box(-1.0, 1.0, (1,), float32), 'A_24': Box(-1.0, 1.0, (1,), float32), 'A_25': Box(-1.0, 1.0, (1,), float32), 'A_26': Box(-1.0, 1.0, (1,), float32), 'A_27': Box(-1.0, 1.0, (1,), float32), 'A_28': Box(-1.0, 1.0, (1,), float32), 'A_29': Box(-1.0, 1.0, (1,), float32), 'A_3': Box(-1.0, 1.0, (1,), float32), 'A_30': Box(-1.0, 1.0, (1,), float32), 'A_31': Box(-1.0, 1.0, (1,), float32), 'A_32': Box(-1.0, 1.0, (1,), float32), 'A_33': Box(-1.0, 1.0, (1,), float32), 'A_34': Box(-1.0, 1.0, (1,), float32), 'A_35': Box(-1.0, 1.0, (1,), float32), 'A_36': Box(-1.0, 1.0, (1,), float32), 'A_37': Box(-1.0, 1.0, (1,), float32), 'A_38': Box(-1.0, 1.0, (1,), float32), 'A_39': Box(-1.0, 1.0, (1,), float32), 'A_4': Box(-1.0, 1.0, (1,), float32), 'A_40': Box(-1.0, 1.0, (1,), float32), 'A_41': Box(-1.0, 1.0, (1,), float32), 'A_42': Box(-1.0, 1.0, (1,), float32), 'A_43': Box(-1.0, 1.0, (1,), float32), 'A_44': Box(-1.0, 1.0, (1,), float32), 'A_45': Box(-1.0, 1.0, (1,), float32), 'A_5': Box(-1.0, 1.0, (1,), float32), 'A_6': Box(-1.0, 1.0, (1,), float32), 'A_7': Box(-1.0, 1.0, (1,), float32), 'A_8': Box(-1.0, 1.0, (1,), float32), 'A_9': Box(-1.0, 1.0, (1,), float32)) was provided"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "class CustomBulletEnv(gym.Env):\n",
    "    def __init__(self, S, A, Y, R=None):\n",
    "        super(CustomBulletEnv, self).__init__()\n",
    "        self.S = S\n",
    "        self.A = A\n",
    "        self.Y = Y\n",
    "        self.R = R  # Optional reward matrix based on Y\n",
    "        self.num_samples = S.shape[0]\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Action space: A_1 to A_45\n",
    "        action_dim = A.shape[1]\n",
    "        self.action_space = spaces.Dict({\n",
    "            f'A_{i+1}': spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32) for i in range(action_dim)\n",
    "        })\n",
    "\n",
    "        # Observation space: S_1 to S_9 and Y_1 to Y_23\n",
    "        obs_dim_s = S.shape[1]\n",
    "        obs_dim_y = Y.shape[1]\n",
    "        observation_space = {}\n",
    "        for i in range(obs_dim_s):\n",
    "            observation_space[f'S_{i+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        for j in range(obs_dim_y):\n",
    "            observation_space[f'Y_{j+1}'] = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Dict(observation_space)\n",
    "\n",
    "        # Initialize PyBullet\n",
    "        self.physicsClient = p.connect(p.DIRECT)  # Use p.GUI for graphical version\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply action and get next state and reward\n",
    "        next_state = self._get_observation()\n",
    "        reward = self._calculate_reward(action)\n",
    "        done = (self.current_step == self.num_samples - 1)\n",
    "        self.current_step += 1\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        obs = {}\n",
    "        for i in range(self.S.shape[1]):\n",
    "            obs[f'S_{i+1}'] = np.array([self.S[self.current_step, i]])\n",
    "        for j in range(self.Y.shape[1]):\n",
    "            obs[f'Y_{j+1}'] = np.array([self.Y[self.current_step, j]])\n",
    "        return obs\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        # Example: using a reward matrix R based on expected Y values\n",
    "        if self.R is not None:\n",
    "            ideal_y = self.R[action]  # Ideal Y value for the chosen action\n",
    "            actual_y = self.Y[self.current_step]  # Actual Y value\n",
    "            reward = -np.linalg.norm(actual_y - ideal_y)  # Negative L2 norm difference as reward\n",
    "        else:\n",
    "            # Default reward function (replace with your own logic)\n",
    "            reward = np.sum(self.A[self.current_step] * action)\n",
    "        return reward\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect(self.physicsClient)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data (replace with your actual data)\n",
    "    S = np.random.randn(1000, 9)\n",
    "    A = np.random.randn(1000, 45)\n",
    "    Y = np.random.randn(1000, 23)\n",
    "    \n",
    "    # Example reward matrix R based on Y (replace with your own logic)\n",
    "    R = np.random.randn(45, 23)  # Example: random R matrix for 45 actions and 23 Y features\n",
    "    \n",
    "    # Create the custom environment\n",
    "    env = CustomBulletEnv(S, A, Y, R)\n",
    "    \n",
    "    # Wrap the environment in a vectorized form\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    # Initialize and train the RL agent (using PPO as an example)\n",
    "    model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    \n",
    "    # Save and reload the model (optional)\n",
    "    #model.save(\"rl_model\")\n",
    "    # model = PPO.load(\"rl_model\")\n",
    "    \n",
    "    # Test the trained model\n",
    "    obs = env.reset()\n",
    "    for _ in range(10):  # Test for 10 steps\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        print(f\"Action: {action}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 13:54:57,951\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\foresight_User\\anaconda3\\envs\\Python3.9.13\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray.rllib.agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tune\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ppo\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_agent_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiAgentEnv\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomMultiAgentEnv\u001b[39;00m(MultiAgentEnv):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ray.rllib.agents'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "class CustomMultiAgentEnv(MultiAgentEnv):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.S = config['S']  # Fixed matrix S\n",
    "        self.A = config['A']  # Matrix A, actions that affect Y\n",
    "        self.Y = config['Y']  # Desired outcomes\n",
    "\n",
    "        self.num_agents = self.A.shape[1]  # Number of agents/actions\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.max_steps = self.Y.shape[0]  # Assuming Y has 1000 samples\n",
    "\n",
    "        # Define action space and observation space for each agent\n",
    "        self.action_space = gym.spaces.Discrete(self.num_agents)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=self.S.shape, dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.S[self.current_step]\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        # action_dict maps agent_id to action index\n",
    "        actions = np.array([action_dict[f'agent_{i}'] for i in range(self.num_agents)])\n",
    "        \n",
    "        # Simulate the effect of actions A on Y\n",
    "        current_Y = self.Y[self.current_step]\n",
    "        predicted_Y = np.dot(self.A[self.current_step], actions)\n",
    "\n",
    "        # Reward function (example: negative mean squared error)\n",
    "        rewards = -np.mean((current_Y - predicted_Y) ** 2)\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "\n",
    "        # Return observations, rewards, done flag, and info\n",
    "        obs = self.S[self.current_step] if not done else None\n",
    "        info = {}  # Additional info, if needed\n",
    "        return obs, rewards, done, info\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'S': np.random.rand(1000, 9),  # Replace with your actual S matrix\n",
    "        'A': np.random.rand(1000, 45),  # Replace with your actual A matrix\n",
    "        'Y': np.random.rand(1000, 23),  # Replace with your actual Y matrix\n",
    "    }\n",
    "\n",
    "    tune.register_env(\"custom_multi_agent_env\", lambda config: CustomMultiAgentEnv(config))\n",
    "    trainer = ppo.PPOTrainer(env=\"custom_multi_agent_env\")\n",
    "\n",
    "    # Training loop (you can adjust num_iterations as needed)\n",
    "    num_iterations = 1000\n",
    "    for i in range(num_iterations):\n",
    "        result = trainer.train()\n",
    "        print(f\"Iteration {i}: {result}\")\n",
    "\n",
    "    # Example of using the trained policy to predict actions for new S and Y\n",
    "    trained_policy = trainer.get_policy()\n",
    "    new_S = np.random.rand(9)  # Replace with your new S vector\n",
    "    new_Y = np.random.rand(23)  # Replace with your new Y vector\n",
    "    action_dict = trained_policy.compute_actions([new_S])\n",
    "    print(f\"Predicted actions for new S and Y: {action_dict}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
