{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foresight_User\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "C:\\Users\\foresight_User\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.019386024128794865}\n",
      "{'score': 0.052194863796759194}\n",
      "{'score': 0.16755671983323128}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNote: This returns a distance score, meaning that the lower the number, \\nthe more similar the prediction is to the reference, according to their embedded representation.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = \"thenlper/gte-large-zh\"\n",
    "gpu = 0\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)#, model_kwargs={\"device\": f\"cuda:{gpu}\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "<EmbeddingDistance.COSINE: 'cosine'>,\n",
    " <EmbeddingDistance.EUCLIDEAN: 'euclidean'>,\n",
    " <EmbeddingDistance.MANHATTAN: 'manhattan'>,\n",
    " <EmbeddingDistance.CHEBYSHEV: 'chebyshev'>,\n",
    " <EmbeddingDistance.HAMMING: 'hamming'>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "evaluator = load_evaluator(\"embedding_distance\", embeddings=embeddings,distance_metric=\"cosine\")#, distance_metric=EmbeddingDistance.EUCLIDEAN\n",
    "\n",
    "a1 = evaluator.evaluate_strings(prediction=\"社團與法人有什麼不同\", reference=\"社團與法人差異有哪些\")\n",
    "a2 = evaluator.evaluate_strings(prediction=\"社團與法人有什麼不同\", reference=\"社團與法人相同點\")\n",
    "a3 = evaluator.evaluate_strings(prediction=\"社團與法人有什麼不同\", reference=\"社團與公司相同點\")\n",
    "\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(a3)\n",
    "\"\"\"\n",
    "Note: This returns a distance score, meaning that the lower the number, \n",
    "the more similar the prediction is to the reference, according to their embedded representation.\n",
    "\"\"\"\n",
    "# print(_euclidean_relevance_score_fn(a1['score']))\n",
    "# print(_euclidean_relevance_score_fn(a2['score']))\n",
    "# print(_euclidean_relevance_score_fn(a3['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name nicolay/prompt-optimizer. Creating a new one with mean pooling.\n",
      "C:\\Users\\foresight_User\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d7a61450fa43a892924e93300b32da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foresight_User\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\foresight_User\\.cache\\huggingface\\hub\\models--nicolay--prompt-optimizer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c5848c953d40bba32dbc44a070f392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'nicolay/prompt-optimizer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'nicolay/prompt-optimizer' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnicolay/prompt-optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:72\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer(\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\n\u001b[0;32m     74\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:298\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[0;32m    286\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[0;32m    287\u001b[0m             model_name_or_path,\n\u001b[0;32m    288\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m             config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    296\u001b[0m         )\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[0;32m    299\u001b[0m             model_name_or_path,\n\u001b[0;32m    300\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    301\u001b[0m             cache_folder\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[0;32m    302\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    303\u001b[0m             trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    304\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    305\u001b[0m             model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[0;32m    306\u001b[0m             tokenizer_kwargs\u001b[38;5;241m=\u001b[39mtokenizer_kwargs,\n\u001b[0;32m    307\u001b[0m             config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    308\u001b[0m         )\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[0;32m    311\u001b[0m     modules \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1312\u001b[0m, in \u001b[0;36mSentenceTransformer._load_auto_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[0;32m   1309\u001b[0m tokenizer_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs}\n\u001b[0;32m   1310\u001b[0m config_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs}\n\u001b[1;32m-> 1312\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m Transformer(\n\u001b[0;32m   1313\u001b[0m     model_name_or_path,\n\u001b[0;32m   1314\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[0;32m   1315\u001b[0m     model_args\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[0;32m   1316\u001b[0m     tokenizer_args\u001b[38;5;241m=\u001b[39mtokenizer_kwargs,\n\u001b[0;32m   1317\u001b[0m     config_args\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m   1318\u001b[0m )\n\u001b[0;32m   1319\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m Pooling(transformer_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_card_data\u001b[38;5;241m.\u001b[39mset_base_model(model_name_or_path, revision\u001b[38;5;241m=\u001b[39mrevision)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:57\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[0;32m     56\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     58\u001b[0m     tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path,\n\u001b[0;32m     59\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args,\n\u001b[0;32m     61\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# No max_seq_length set. Try to infer from model\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:899\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    896\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2094\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2091\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2095\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2096\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2097\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2098\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2099\u001b[0m     )\n\u001b[0;32m   2101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'nicolay/prompt-optimizer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'nicolay/prompt-optimizer' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"nicolay/prompt-optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name Parth/mT5-question-generator. Creating a new one with mean pooling.\n",
      "C:\\Users\\foresight_User\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153ddcd881bc49b68cd4a195658c5ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foresight_User\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\foresight_User\\.cache\\huggingface\\hub\\models--Parth--mT5-question-generator. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b3c987a27544d4bc4d4b751c21201f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m HuggingFaceEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParth/mT5-question-generator\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:72\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer(\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\n\u001b[0;32m     74\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:298\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[0;32m    286\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[0;32m    287\u001b[0m             model_name_or_path,\n\u001b[0;32m    288\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m             config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    296\u001b[0m         )\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[0;32m    299\u001b[0m             model_name_or_path,\n\u001b[0;32m    300\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    301\u001b[0m             cache_folder\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[0;32m    302\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    303\u001b[0m             trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    304\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    305\u001b[0m             model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[0;32m    306\u001b[0m             tokenizer_kwargs\u001b[38;5;241m=\u001b[39mtokenizer_kwargs,\n\u001b[0;32m    307\u001b[0m             config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    308\u001b[0m         )\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[0;32m    311\u001b[0m     modules \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1312\u001b[0m, in \u001b[0;36mSentenceTransformer._load_auto_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[0;32m   1309\u001b[0m tokenizer_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs}\n\u001b[0;32m   1310\u001b[0m config_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs}\n\u001b[1;32m-> 1312\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m Transformer(\n\u001b[0;32m   1313\u001b[0m     model_name_or_path,\n\u001b[0;32m   1314\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[0;32m   1315\u001b[0m     model_args\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[0;32m   1316\u001b[0m     tokenizer_args\u001b[38;5;241m=\u001b[39mtokenizer_kwargs,\n\u001b[0;32m   1317\u001b[0m     config_args\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m   1318\u001b[0m )\n\u001b[0;32m   1319\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m Pooling(transformer_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_card_data\u001b[38;5;241m.\u001b[39mset_base_model(model_name_or_path, revision\u001b[38;5;241m=\u001b[39mrevision)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:53\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[0;32m     50\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     52\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[0;32m     56\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:82\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[1;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_t5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, MT5Config):\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     85\u001b[0m         model_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args\n\u001b[0;32m     86\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:102\u001b[0m, in \u001b[0;36mTransformer._load_mt5_model\u001b[1;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MT5EncoderModel\n\u001b[0;32m    101\u001b[0m MT5EncoderModel\u001b[38;5;241m.\u001b[39m_keys_to_ignore_on_load_unexpected \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder.*\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m MT5EncoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    103\u001b[0m     model_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args\n\u001b[0;32m    104\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:3380\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3378\u001b[0m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[0;32m   3379\u001b[0m         filename \u001b[38;5;241m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[1;32m-> 3380\u001b[0m         resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   3381\u001b[0m             pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs\n\u001b[0;32m   3382\u001b[0m         )\n\u001b[0;32m   3383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[0;32m   3384\u001b[0m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[0;32m   3385\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   3386\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   3387\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[0;32m   3388\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs,\n\u001b[0;32m   3389\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    400\u001b[0m         path_or_repo_id,\n\u001b[0;32m    401\u001b[0m         filename,\n\u001b[0;32m    402\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    403\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    404\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    405\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    406\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    407\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    408\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    409\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    410\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    411\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    414\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1219\u001b[0m     )\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[0;32m   1222\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1224\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[0;32m   1225\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1226\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1227\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   1228\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1231\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1232\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1233\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m   1234\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1236\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1237\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1365\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1367\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1368\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1369\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[0;32m   1370\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1371\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1372\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1373\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1374\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1375\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1376\u001b[0m     )\n\u001b[0;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1884\u001b[0m     http_get(\n\u001b[0;32m   1885\u001b[0m         url_to_download,\n\u001b[0;32m   1886\u001b[0m         f,\n\u001b[0;32m   1887\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1888\u001b[0m         resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[0;32m   1889\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1890\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1891\u001b[0m     )\n\u001b[0;32m   1893\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:539\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    537\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 539\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    541\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "HuggingFaceEmbeddings(model_name=\"Parth/mT5-question-generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmlingua import PromptCompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"給我好的food information\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foresight_User\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be21395dcf64ce7824653d73f0931ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41beeb511ef494487289c20efd09f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   2%|1         | 189M/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm_lingua \u001b[38;5;241m=\u001b[39m PromptCompressor()\u001b[38;5;66;03m#\"microsoft/phi-2\", model_config={\"revision\": \"main\"}\u001b[39;00m\n\u001b[0;32m      2\u001b[0m compressed_prompt \u001b[38;5;241m=\u001b[39m llm_lingua\u001b[38;5;241m.\u001b[39mcompress_prompt(prompt, instruction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\llmlingua\\prompt_compressor.py:89\u001b[0m, in \u001b[0;36mPromptCompressor.__init__\u001b[1;34m(self, model_name, device_map, model_config, open_api_config, use_llmlingua2, llmlingua2_config)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix_bos_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moai_tokenizer \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mencoding_for_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model(model_name, device_map, model_config)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_llmlingua2:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_llmlingua2(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllmlingua2_config)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\llmlingua\\prompt_compressor.py:140\u001b[0m, in \u001b[0;36mPromptCompressor.load_model\u001b[1;34m(self, model_name, device_map, model_config)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    135\u001b[0m     device_map\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m device_map \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m )\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map:\n\u001b[1;32m--> 140\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_CLASS\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    141\u001b[0m         model_name,\n\u001b[0;32m    142\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mpop(\n\u001b[0;32m    143\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    144\u001b[0m         ),\n\u001b[0;32m    145\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m    146\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    147\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_config,\n\u001b[0;32m    149\u001b[0m     )\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_CLASS\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    152\u001b[0m         model_name,\n\u001b[0;32m    153\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_config,\n\u001b[0;32m    157\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:3511\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3508\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[0;32m   3509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[0;32m   3510\u001b[0m     \u001b[38;5;66;03m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[1;32m-> 3511\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m get_checkpoint_shard_files(\n\u001b[0;32m   3512\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   3513\u001b[0m         resolved_archive_file,\n\u001b[0;32m   3514\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   3515\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   3516\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   3517\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m   3518\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   3519\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   3520\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m   3521\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   3522\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m   3523\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   3524\u001b[0m     )\n\u001b[0;32m   3526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3527\u001b[0m     is_safetensors_available()\n\u001b[0;32m   3528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m   3529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3530\u001b[0m ):\n\u001b[0;32m   3531\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\hub.py:1040\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[1;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1039\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   1041\u001b[0m             pretrained_model_name_or_path,\n\u001b[0;32m   1042\u001b[0m             shard_filename,\n\u001b[0;32m   1043\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1044\u001b[0m             force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1045\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1046\u001b[0m             resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m   1047\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1048\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1049\u001b[0m             user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m   1050\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1051\u001b[0m             subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m   1052\u001b[0m             _commit_hash\u001b[38;5;241m=\u001b[39m_commit_hash,\n\u001b[0;32m   1053\u001b[0m         )\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    400\u001b[0m         path_or_repo_id,\n\u001b[0;32m    401\u001b[0m         filename,\n\u001b[0;32m    402\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    403\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    404\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    405\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    406\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    407\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    408\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    409\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    410\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    411\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    414\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1219\u001b[0m     )\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[0;32m   1222\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1224\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[0;32m   1225\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1226\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1227\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   1228\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1231\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1232\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1233\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m   1234\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1236\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1237\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1365\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1367\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1368\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1369\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[0;32m   1370\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1371\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1372\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1373\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1374\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1375\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1376\u001b[0m     )\n\u001b[0;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1884\u001b[0m     http_get(\n\u001b[0;32m   1885\u001b[0m         url_to_download,\n\u001b[0;32m   1886\u001b[0m         f,\n\u001b[0;32m   1887\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1888\u001b[0m         resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[0;32m   1889\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1890\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1891\u001b[0m     )\n\u001b[0;32m   1893\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:539\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    537\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 539\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    541\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "llm_lingua = PromptCompressor()#\"microsoft/phi-2\", model_config={\"revision\": \"main\"}\n",
    "compressed_prompt = llm_lingua.compress_prompt(prompt, instruction=\"\", question=\"\", target_token=200)\n",
    "\n",
    "# > {'compressed_prompt': 'Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He reanged five of boxes into packages of sixlters each and sold them $3 per. He sold the rest theters separately at the of three pens $2. How much did make in total, dollars?\\nLets think step step\\nSam bought 1 boxes x00 oflters.\\nHe bought 12 * 300ters in total\\nSam then took 5 boxes 6ters0ters.\\nHe sold these boxes for 5 *5\\nAfterelling these  boxes there were 3030 highlighters remaining.\\nThese form 330 / 3 = 110 groups of three pens.\\nHe sold each of these groups for $2 each, so made 110 * 2 = $220 from them.\\nIn total, then, he earned $220 + $15 = $235.\\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\\nThe answer is 115',\n",
    "#  'origin_tokens': 2365,\n",
    "#  'compressed_tokens': 211,\n",
    "#  'ratio': '11.2x',\n",
    "#  'saving': ', Saving $0.1 in GPT-4.'}\n",
    "\n",
    "# ## Or use the phi-2 model,\n",
    "# llm_lingua = PromptCompressor(\"microsoft/phi-2\")\n",
    "\n",
    "# ## Or use the quantation model, like TheBloke/Llama-2-7b-Chat-GPTQ, only need <8GB GPU memory.\n",
    "# ## Before that, you need to pip install optimum auto-gptq\n",
    "# llm_lingua = PromptCompressor(\"TheBloke/Llama-2-7b-Chat-GPTQ\", model_config={\"revision\": \"main\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pctoolkit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpctoolkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptCompressor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pctoolkit'"
     ]
    }
   ],
   "source": [
    "from pctoolkit.compressors import PromptCompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#from langchain.chat_models import ChatOpenAI\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QAGenerationChain\n\u001b[1;32m----> 3\u001b[0m chain \u001b[38;5;241m=\u001b[39m QAGenerationChain\u001b[38;5;241m.\u001b[39mfrom_llm(embeddings)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\langchain\\chains\\qa_generation\\base.py:52\u001b[0m, in \u001b[0;36mQAGenerationChain.from_llm\u001b[1;34m(cls, llm, prompt, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03mCreate a QAGenerationChain from a language model.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    a QAGenerationChain class\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m _prompt \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;129;01mor\u001b[39;00m PROMPT_SELECTOR\u001b[38;5;241m.\u001b[39mget_prompt(llm)\n\u001b[1;32m---> 52\u001b[0m chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39m_prompt)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(llm_chain\u001b[38;5;241m=\u001b[39mchain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:183\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     emit_warning()\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import QAGenerationChain\n",
    "chain = QAGenerationChain.from_llm(embeddings)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2024-06-06 14:40:57,190 - prompt_optimizer.poptim.logger - INFO - Prompt Data Before: 給我好的food information\n",
      "2024-06-06 14:40:57,191 - prompt_optimizer.poptim.logger - INFO - Prompt Data After: [UNK] [UNK] [UNK] food information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '[UNK] [UNK] [UNK] food information', 'metrics': []}\n"
     ]
    }
   ],
   "source": [
    "llm_lingua = PromptCompressor(\n",
    "    model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n",
    "    use_llmlingua2=True, # Whether to use llmlingua-2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '[UNK] [UNK] [UNK] food information', 'metrics': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"pairwise_embedding_distance\", embeddings=embeddings,distance_metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.21706318305563443}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"Seattle is hot in June\", prediction_b=\"Seattle is cool in June.\"\n",
    ")\n",
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"Seattle is warm in June\", prediction_b=\"Seattle is cool in June.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"embedding_distance\", embeddings=embeddings)\n",
    "a1 = evaluator.evaluate_strings(prediction=\"社團與法人有什麼不同\", reference=\"社團與法人差異有哪些\")\n",
    "a2 = evaluator.evaluate_strings(prediction=\"社團與法人有什麼不同\", reference=\"社團與法人相同點\")\n",
    "a3 = evaluator.evaluate_strings(prediction=\"社團與法人有什麼不同\", reference=\"社團與公司相同點\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.019386024128794865} {'score': 0.052194863796759194} {'score': 0.16755671983323128}\n"
     ]
    }
   ],
   "source": [
    "print(a1,a2,a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.functional.pairwise import pairwise_cosine_similarity\n",
    "pairwise_cosine_similarity(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datasets\n",
    "print(datasets.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 59.398771349561265, 'char_order': 6, 'word_order': 0, 'beta': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe chrF(++) score can be any value between 0.0 and 100.0, inclusive.\\n分數越高，表示質量越好。\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ChrF and ChrF++ are two MT evaluation metrics. \n",
    "They both use the F-score statistic for character n-gram matches, \n",
    "and ChrF++ adds word n-grams as well which correlates more strongly with direct assessment. \n",
    "We use the implementation that is already present in sacrebleu. \n",
    "The implementation here is slightly different from sacrebleu in terms of the required input format. \n",
    "The length of the references and hypotheses lists need to be the same, \n",
    "so you may need to transpose your references compared to sacrebleu's required input format. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ChrF and ChrF++ are two MT evaluation metrics that use the F-score statistic for character n-gram matches.\n",
    "ChrF++ additionally includes word n-grams, which correlate more strongly with direct assessment. \n",
    "We use the implementation that is already present in sacrebleu.\n",
    "While this metric is included in sacreBLEU, \n",
    "the implementation here is slightly different from sacreBLEU in terms of the required input format. \n",
    "Here, the length of the references and hypotheses lists need to be the same, \n",
    "so you may need to transpose your references compared to sacrebleu's required input format. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "char_order (int): Character n-gram order. Defaults to 6.\n",
    "word_order (int): Word n-gram order. If equals to 2, the metric is referred to as chrF++. Defaults to 0.\n",
    "beta (int): Determine the importance of recall w.r.t precision. Defaults to 2.\n",
    "lowercase (bool): If True, enables case-insensitivity. Defaults to False.\n",
    "whitespace (bool): If True, include whitespaces when extracting character n-grams. Defaults to False.\n",
    "eps_smoothing (bool): If True, applies epsilon smoothing similar to reference chrF++.py, NLTK, and Moses implementations. If False, takes into account effective match order similar to sacreBLEU < 2.0.0. Defaults to False.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Limitation\n",
    "According to Popović 2017, \n",
    "chrF+ (where word_order=1) and chrF++ (where word_order=2) \n",
    "produce scores that correlate better with human judgements than chrF (where word_order=0) does.\n",
    "\"\"\"\n",
    "prediction = [\"The relationship between cats and dogs is not exactly friendly.\", \" 好的書城 is just a genteel black hole that knows how to read.\"]\n",
    "reference = [\"貓和狗的relationship不怎麼friendly\", \"A good bookshop is just a genteel Black Hole that knows how to read.\"]\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "results = chrf.compute(predictions=prediction, references=reference,char_order=6,word_order=0,beta=2,lowercase=False,whitespace=False,eps_smoothing=False)\n",
    "print(results)\n",
    "\n",
    "\"\"\"\n",
    "The chrF(++) score can be any value between 0.0 and 100.0, inclusive.\n",
    "分數越高，表示質量越好。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-measurement--toxicity.2390290fa0bf6d78480143547c6b08f3d4f8805b249df8c7a8e80d0ce8e3778b.toxicity:Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toxicity': [0.0001379283203277737, 0.000788058212492615, 0.001533533213660121]}\n",
      "{'toxicity': [0.00040979203185997903, 0.0001410811091773212, 0.00014824386744294316]}\n"
     ]
    }
   ],
   "source": [
    "######## 這不能使用toxicity\n",
    "# male_model_completions = [\n",
    "# 'was working so hard at an enterprise that he needed his own quarters',\n",
    "# 'needs the answer',\n",
    "# 'the pregnancy and the woman’s condition.'\n",
    "# ]\n",
    "\n",
    "# female_model_completions = [\n",
    "# 'got up after 3 and gave him a few \"fucks\"',\n",
    "# 'usually doesn’t have any money',\n",
    "# 'the course and it would be a great opportunity to meet with patients during her time at this hospital.'\n",
    "# ]\n",
    "# toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
    "# male_results = toxicity.compute(predictions=male_model_completions)\n",
    "\n",
    "# female_results = toxicity.compute(predictions=female_model_completions)\n",
    "\n",
    "# print(male_results)\n",
    "# print(female_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [0.68645840883255], 'recall': [0.6680428981781006], 'f1': [0.6771254539489746], 'hashcode': 'bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.41.2)'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BERTScore leverages the pre-trained contextual embeddings from BERT and matches words \n",
    "in candidate and reference sentences by cosine similarity. \n",
    "It has been shown to correlate with human judgment on sentence-level and system-level evaluation. \n",
    "Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful \n",
    "for evaluating different language generation tasks. \n",
    "\"\"\"\n",
    "\n",
    "# 不用做segmentation\n",
    "sentence1 = [\"一隻貓稅在mattress上，cute cat\"]\n",
    "sentence2 = [\"那有 cat 睡在墊子上，可怕的貓!!!\"]\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "result = bertscore.compute(predictions=sentence1, references=sentence2, model_type = \"bert-base-multilingual-cased\",device=None,nthreads=4,rescale_with_baseline=False,) # (bertscore lang=\"en\" \"zh\" \"jp\") #有中英夾雜使用jp 則是bert-base-multilingual-cased_L9_no-idf_version=0.3.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1489: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6119fdd02ab84a02b59efec832321dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2444ab0593e545d8ae930ea48d816b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': [0.436601, 0.8199912]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FrugalScore is a reference-based metric for NLG models evaluation. \n",
    "It is based on a distillation approach that allows to learn a fixed,\n",
    "low cost version of any expensive NLG metric, while retaining most of its original performance.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "FrugalScore is a reference-based metric for Natural Language Generation (NLG) model evaluation. \n",
    "It is based on a distillation approach that allows to learn a fixed, \n",
    "low cost version of any expensive NLG metric, while retaining most of its original performance.\n",
    "The FrugalScore models are obtained by continuing the pretraining of small models on a synthetic \n",
    "dataset constructed using summarization, backtranslation and denoising models. \n",
    "During the training, the small models learn the internal mapping of the expensive metric, \n",
    "including any similarity function.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Limitation and Bias\n",
    "FrugalScore is based on BertScore and MoverScore, \n",
    "and the models used are based on the original models used for these scores.\n",
    "\"\"\"\n",
    "\n",
    "prediction = [\"The relationship between cats and dogs is not exactly friendly.\", \" 好的書城 is just a genteel black hole that knows how to read.\"]\n",
    "reference = [\"貓和狗的relationship不怎麼friendly\", \"A good bookshop is just a genteel Black Hole that knows how to read.\"]\n",
    "frugalscore = evaluate.load(\"frugalscore\") #evaluate.load(\"frugalscore\", \"moussaKam/frugalscore_medium_bert-base_mover-score\")\n",
    "results = frugalscore.compute(predictions=prediction, references=reference, batch_size=16, max_length=64,device=None)\n",
    "print(results)\n",
    "# 分數越高越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "Featurizing tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3eff79be1741b18f00cabecee5d88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "Featurizing tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe46af2ba6c94d93b99423217256431c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 25\n",
      "performing clustering in lower dimension = 2\n",
      "kmeans time: 0.36 s\n",
      "total discretization time: 0.36 seconds\n"
     ]
    }
   ],
   "source": [
    "# pip install mauve-text\n",
    "\n",
    "\"\"\"\n",
    "MAUVE is a measure of the gap between neural text and human text. \n",
    "It is computed using the Kullback–Leibler (KL) divergences between the two distributions of text in a quantized embedding \n",
    "space of a large language model. \n",
    "MAUVE can identify differences in quality arising from model sizes and decoding algorithms.\n",
    "\"\"\"\n",
    "\n",
    "mauve = evaluate.load('mauve')\n",
    "prediction = [\"The relationship between cats and dogs is not exactly friendly.\", \" 好的書城 is just a genteel black hole that knows how to read.\"]\n",
    "reference = [\"貓和狗的relationship不怎麼friendly\", \"A good bookshop is just a genteel Black Hole that knows how to read.\"]\n",
    "mauve_results = mauve.compute(predictions=predictions, references=references)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "The metric takes two lists of strings of tokens separated by spaces: \n",
    "one representing predictions (i.e. the text generated by the model) \n",
    " the second representing references (a reference text for each prediction)\n",
    "\n",
    "\n",
    "mauve: MAUVE score, which ranges between 0 and 1. Larger values indicate that P and Q are closer.\n",
    "\n",
    "The original MAUVE paper reported values ranging from 0.88 to 0.94 for \n",
    "open-ended text generation using a text completion task in the web text domain. \n",
    "The authors found that bigger models resulted in higher MAUVE scores and that MAUVE is correlated with human judgments.\n",
    "\n",
    "\n",
    "frontier_integral: Frontier Integral, which ranges between 0 and 1. Smaller values indicate that P and Q are closer.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(mauve_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\FORESI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.669 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'world', '.', ' ', '你好', '，', '世界', '。', 'This', ' ', 'is', ' ', 'a', ' ', 'test', ' ', 'sentence', '.', ' ', '這是', '一個', '測試', '句子', '。']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# 加载多语言模型\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import jieba\n",
    "\n",
    "#text = \"你好，世界。這是一個測試句子。\"\n",
    "words = jieba.lcut(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world.\n",
      "你好，世界。This is a test sentence.\n",
      "這是一個測試句子。\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      ".\n",
      "你好，世界。This\n",
      "is\n",
      "a\n",
      "test\n",
      "sentence\n",
      ".\n",
      "這是一個測試句子\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "# 詞語分割\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello world. 你好，世界。This is a test sentence. 這是一個測試句子。"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis metric outputs a dictionary with the perplexity scores for the text input in the list, \\nand the average perplexity. \\nIf one of the input texts is longer than the max input length of the model, \\nthen it is truncated to the max length for the perplexity computation.\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity 不可使用，主要評估訓練模型後的好壞\n",
    "\"\"\"\n",
    "This metric outputs a dictionary with the perplexity scores for the text input in the list, \n",
    "and the average perplexity. \n",
    "If one of the input texts is longer than the max input length of the model, \n",
    "then it is truncated to the max length for the perplexity computation.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.3333333333333333, 'rouge2': 0.0, 'rougeL': 0.3333333333333333, 'rougeLsum': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, \n",
    "is a set of metrics and a software package used for evaluating automatic summarization \n",
    "and machine translation software in natural language processing. \n",
    "The metrics compare an automatically produced summary or translation \n",
    "against a reference or a set of references (human-produced) summary or translation.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"rouge1\": unigram (1-gram) based scoring\n",
    "\"rouge2\": bigram (2-gram) based scoring\n",
    "\"rougeL\": Longest common subsequence based scoring.\n",
    "\"rougeLSum\": splits text using \"\\n\"\n",
    "\n",
    "\"\"\"\n",
    "# 要做segmentation\n",
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=['你好啊', 'general kenobi'],\n",
    "                         references=['hello 你好啊', 'kenobi將軍'])#, tokenizer=lambda x: x.split()\n",
    "print(results)\n",
    "# UNIGEN 用 'rougeL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.3333333333333333, 'precisions': [0.3333333333333333], 'brevity_penalty': 1.0, 'length_ratio': 1.5, 'translation_length': 3, 'reference_length': 2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. \n",
    "Quality is considered to be the correspondence between a machine's output and that of a human: \n",
    "\"the closer a machine translation is to a professional human translation, the better it is\" – this is the central idea behind BLEU. \n",
    "BLEU was one of the first metrics to claim a high correlation with human judgements of quality, \n",
    "and remains one of the most popular automated and inexpensive metrics.\n",
    "\n",
    "Scores are calculated for individual translated segments—generally sentences—by comparing them with a set of good quality reference translations. \n",
    "Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. \n",
    "Neither intelligibility nor grammatical correctness are not taken into account.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "max_order (int): Maximum n-gram order to use when computing BLEU score. Defaults to 4.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "BLEU's output is always a number between 0 and 1. \n",
    "This value indicates how similar the candidate text is to the reference texts,\n",
    "with values closer to 1 representing more similar texts. \n",
    "Few human translations will attain a score of 1, \n",
    "since this would indicate that the candidate is identical to one of the reference translations. \n",
    "For this reason, it is not necessary to attain a score of 1. \n",
    "Because there are more opportunities to match, adding additional reference translations will increase the BLEU score.\n",
    "\"\"\"\n",
    "predictions = ['你好啊', 'general kenobi']\n",
    "references = [['hello there', 'kenobi將軍'],\n",
    "                [\"你好啊\", \"general 肯諾比\"]]\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references,  max_order=1)#, 用 1 grame\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 65.39657130693087, 'counts': [13, 12, 11, 10], 'totals': [16, 14, 12, 10], 'precisions': [81.25, 85.71428571428571, 91.66666666666667, 100.0], 'bp': 0.7316156289466418, 'sys_len': 16, 'ref_len': 21}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SacreBLEU provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. \n",
    "Inspired by Rico Sennrich's `multi-bleu-detok.perl`, \n",
    "it produces the official WMT scores but works with plain text. \n",
    "It also knows all the standard test sets and handles downloading, processing, and tokenization for you.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "SacreBLEU provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. \n",
    "Inspired by Rico Sennrich's multi-bleu-detok.perl, \n",
    "it produces the official Workshop on Machine Translation (WMT) \n",
    "scores but works with plain text. \n",
    "It also knows all the standard test sets and handles downloading, processing, and tokenization.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tokenize (str): \n",
    "Tokenization method to use for BLEU. \n",
    "If not provided, defaults to 'zh' for Chinese, 'ja-mecab' for Japanese and '13a' (mteval) otherwise. Possible values are:\n",
    "   'none': No tokenization.\n",
    "   'zh': Chinese tokenization.\n",
    "   '13a': mimics the mteval-v13a script from Moses.\n",
    "   'intl': International tokenization, mimics the mteval-v14 script from Moses\n",
    "   'char': Language-agnostic character-level tokenization.\n",
    "   'ja-mecab': Japanese tokenization. Uses the MeCab tokenizer.\n",
    "\n",
    "   intl 方法適合：大多數印歐語系語言（如英語、法語、德語等），這些語言的單詞邊界明確，使用國際標準的單詞級分詞更能反映翻譯質量。\n",
    "   char 方法適合：漢藏語系語言（如中文、日語、韓語等），這些語言單詞邊界不明顯，字符級分詞更能準確評估翻譯質量。\n",
    "   !! 這部分需要好好review\n",
    "\n",
    "   intl (International tokenization)：\n",
    "    基於國際標準的分詞方法，模仿 Moses mteval-v14 腳本。\n",
    "    主要針對單詞級別的匹配，考慮單詞邊界、標點符號和其他語言特性。\n",
    "    適合處理大多數語言，特別是那些詞彙邊界清晰的語言，如英語、法語等。\n",
    "   char (Character-level tokenization)：\n",
    "    基於字符級別的分詞方法，不依賴於單詞邊界。\n",
    "    將文本分解為單個字符進行匹配。\n",
    "    更加語言無關，適合處理那些單詞邊界不明顯或形態複雜的語言，如中文、日語等。\n",
    "\n",
    "lowercase (bool): \n",
    "   If True, lowercases the input, enabling case-insensitivity. Defaults to False.\n",
    "force (bool): \n",
    "   If True, insists that your tokenized input is actually detokenized. Defaults to False.\n",
    "use_effective_order (bool): \n",
    "   If True, stops including n-gram orders for which precision is 0. \n",
    "   This should be True, if sentence-level BLEU will be computed. Defaults to False.\n",
    "\n",
    "   use_effective_order 參數決定了在計算 BLEU 分數時是否忽略精確度為 0 的 n-gram 順序。\n",
    "   設為 True 時，有助於提高句子級別 BLEU 分數的準確性，特別適用於短句子的評估。\n",
    "   設為 False 時，則適合評估長文本的整體翻譯質量\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Limitation and Bias\n",
    "Because what this metric calculates is BLEU scores, \n",
    "it has the same limitations as that metric, except that sacreBLEU is more easily reproducible.\n",
    "\n",
    "\"\"\"\n",
    "predictions = ['你好啊', 'general kenobi']\n",
    "references = [['hello there', 'kenobi將軍'],\n",
    "                [\"你好啊\", \"general kenobi\"]]\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "results = sacrebleu.compute(predictions=predictions, \n",
    "                            references=references,tokenize=\"char\",lowercase=True,use_effective_order=False) # 要調整tokenize，為何用char\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sacremoses\n",
    "sari = evaluate.load(\"sari\")\n",
    "sources=[\"'kenobi將軍，您好!\"]\n",
    "predictions = ['你好啊! general kenobi']\n",
    "references = [['hello there! kenobi將軍',\n",
    "                \"你好啊! general 肯諾比\"]]\n",
    "sari_score = sari.compute(sources=sources, predictions=predictions, references=references)\n",
    "\"\"\"\n",
    "\n",
    "SARI (system output against references and against the input sentence) is \n",
    "a metric used for evaluating automatic text simplification systems.\n",
    "The metric compares the predicted simplified sentences against the reference and the source sentences. \n",
    "It explicitly measures the goodness of words that are added, deleted and kept by the system.\n",
    "\n",
    "The range of values for the SARI score is between 0 and 100 -- \n",
    "the higher the value, the better the performance of the model being evaluated,\n",
    "with a SARI of 100 being a perfect score.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\"這句話對嗎??\",\n",
    "               \"那這句話呢?\",\n",
    "          \"這笑話很難笑\"     ]\n",
    "references = [[\"這句話對嗎\", \"這句話對嗎!?!\"],\n",
    "               [\"這句話怎麼樣?\", \"wHaT aBoUt ThIs SeNtEnCe?\"],\n",
    "               [\"你的笑話...\", \"...TERrible\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ArithmeticError\\nThe metric can take on any value 0 and above. 0 is a perfect score, \\nmeaning the predictions exactly match the references and no edits were necessary.\\n Higher scores are worse. \\n Scores above 100 mean that the cumulative number of edits, num_edits, \\n is higher than the cumulative length of the references, ref_length.\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ter = evaluate.load(\"ter\")\n",
    "results = ter.compute(predictions=predictions,\n",
    "                        references=references,\n",
    "                         case_sensitive=True,support_zh_ja_chars=True,ignore_punct=True,normalized=True)\n",
    "\n",
    "\n",
    "\"\"\"ArithmeticError\n",
    "The metric can take on any value 0 and above. 0 is a perfect score, \n",
    "meaning the predictions exactly match the references and no edits were necessary.\n",
    " Higher scores are worse. \n",
    " Scores above 100 mean that the cumulative number of edits, num_edits, \n",
    " is higher than the cumulative length of the references, ref_length.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3333333333333333\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Word error rate (WER) is a common metric of the performance of an automatic speech recognition (ASR) system.\n",
    "\n",
    "The general difficulty of measuring the performance of ASR systems lies in the fact that the recognized word sequence can have a different \n",
    "length from the reference word sequence (supposedly the correct one). The WER is derived from the Levenshtein distance, working at the word level.\n",
    "\n",
    "This problem is solved by first aligning the recognized word sequence with the reference (spoken) \n",
    "word sequence using dynamic string alignment. \n",
    "Examination of this issue is seen through a theory called the power law that states the correlation between \n",
    "perplexity and word error rate (see this article for further information).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The lower the value, the better the performance of the ASR system, with a WER of 0 being a perfect score.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# !pip install jiwer\n",
    "wer = evaluate.load(\"wer\")\n",
    "predictions = ['你好啊', 'general kenobi']\n",
    "references1 = ['hello there', 'kenobi將軍']\n",
    "references2 = [\"你好啊\", \"general 肯諾比\"]\n",
    "wer_score1 = wer.compute(predictions=predictions, references=references1)\n",
    "wer_score2 = wer.compute(predictions=predictions, references=references2)\n",
    "\n",
    "print(wer_score1)\n",
    "print(wer_score2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wer': 1.3333333333333333, 'cer': 1.105263157894737}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://huggingface.co/datasets/google/xtreme_s\n",
    "\n",
    "wer: \n",
    "Word error rate (WER) is a common metric of the performance of an automatic speech recognition system. \n",
    "The lower the value, the better the performance of the ASR system, with a WER of 0 being a perfect score (see WER score for more information). \n",
    "It is returned for the mls, fleurs-asr, voxpopuli and babel subsets of the benchmark.\n",
    "\n",
    "cer: \n",
    "Character error rate (CER) is similar to WER, but operates on character instead of word. \n",
    "The lower the CER value, the better the performance of the ASR system, \n",
    "with a CER of 0 being a perfect score (see CER score for more information). \n",
    "It is returned for the mls, fleurs-asr, voxpopuli and babel subsets of the benchmark.\n",
    "\n",
    "多語言語音基準 (Multilingual Speech Benchmark, MLS) \n",
    "是一個用於評估多語言自動語音識別 (ASR) 模型性能的數據集。\n",
    "MLS 數據集包含多種語言的語音數據，旨在促進多語言語音技術的發展。評估模型性能時，\n",
    "常用的指標包括字錯誤率 (WER) 和語音片段的準確性。\n",
    "\n",
    "\n",
    "Fleurs-ASR：\n",
    "這代表的是 Fleurs 的自動語音識別（Automatic Speech Recognition, ASR）任務。\n",
    "Fleurs 是一個多語種語音數據集，\n",
    "用於訓練和評估語音識別系統。在這個任務中，模型需要將語音轉錄為相應的文本，並在多種語言上進行評估。\n",
    "\"\"\"\n",
    "\n",
    "xtreme_s_metric = evaluate.load('xtreme_s', 'mls')\n",
    "predictions = ['你好啊', 'general kenobi']\n",
    "references = ['hello there', 'kenobi將軍']\n",
    "results = xtreme_s_metric.compute(predictions=predictions, references=references)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a machine translation evaluation metric, \n",
    "which is calculated based on the harmonic mean of precision and recall, with recall weighted more than precision.\n",
    "METEOR is based on a generalized concept of unigram matching between the machine-produced translation and human-produced reference translations. \n",
    "Unigrams can be matched based on their surface forms, stemmed forms, and meanings. \n",
    "Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, \n",
    "unigram-recall, and a measure of fragmentation that is designed to directly capture \n",
    "how well-ordered the matched words in the machine translation are in relation to the reference.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Limitations and bias\n",
    "While the correlation between METEOR and human judgments was measured \n",
    "for Chinese and Arabic and found to be significant, further experimentation is needed to check its correlation for other languages.\n",
    "Furthermore, while the alignment and matching done in METEOR is based on unigrams, \n",
    "using multiple word entities (e.g. bigrams) could contribute to improving its accuracy \n",
    "– this has been proposed in more recent publications on the subject.\n",
    "\n",
    "\"\"\"\n",
    "# 語英文顯著差距，無法捕捉中文質量\n",
    "# meteor = evaluate.load('meteor')\n",
    "# results = meteor.compute(predictions=[\" \".join(prediction_ws)], references=[\" \".join(reference_ws)],gamma=1,alpha=1)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# 加载预训练的模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# 加载模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:919: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:3027: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:00<00:00, 479.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('What is COVID-19?',\n",
      "  'Epidemiological investigations, Virological analyses, Disease surveillance'),\n",
      " ('What is COVID-19?', 'Epidemiological investigations, Virological analyses'),\n",
      " ('What is COVID-19?', 'Epidemiological investigations, Virological analyses'),\n",
      " ('What is COVID-19?', 'Epidemiological investigations, Virological analyses')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from lmqg import TransformersQG\n",
    "\n",
    "# initialize model\n",
    "model = TransformersQG('lmqg/t5-base-squad-qag') # or TransformersQG(model='lmqg/t5-base-squad-qg-ae') \n",
    "# paragraph to generate pairs of question and answer\n",
    "context = \"在當今社會，公共衛生扮演著至關重要的角色。COVID-19等新興傳染病的爆發再次彰顯了防疫與預防的重要性。\\\n",
    "    疫苗接種率、社區防控措施、醫療資源等因素都直接影響著疾病的傳播和控制。\\\n",
    "        Epidemiological investigations, Virological analyses, Disease surveillance等是預防控制疾病的核心手段。\\\n",
    "            政府應該加強對公共衛生體系的投入，提高基礎設施和醫療資源水平，加強科學宣傳，提高民眾的健康意識。\\\n",
    "                透過跨學科合作，共同應對全球公共衛生挑戰。\"\n",
    "# model prediction\n",
    "question_answer = model.generate_qa(context)\n",
    "# the output is a list of tuple (question, answer)\n",
    "pprint(question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '什么被认为是预防疾病的重要性?'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", \"lmqg/mt5-small-zhquad-qg\")\n",
    "output = pipe(context)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'question: 在当今社会公共卫生组织的?, answer: 公共卫生资源'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = pipeline(\"text2text-generation\", \"lmqg/mt5-base-zhquad-qag\")\n",
    "output = pipe(context)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '在当今社会,公共卫生扮演着至关重要的角色?'}]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text2text-generation\", \"lmqg/mt5-base-zhquad-qg-ae\")\n",
    "output = pipe(context)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '疫苗接种率、社区防施和医疗资源等因素都直接影响'}]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text2text-generation\", \"lmqg/mt5-base-zhquad-ae\")\n",
    "output = pipe(context)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'疫苗接种率、社区防施和医疗资源等因素都直接影响'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chinese-converter XXXXXXXXXXXXXXXXXXXXX\n",
    "#! pip install opencc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'鼠標'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import opencc\n",
    "converter = opencc.OpenCC('s2t.json')\n",
    "converter.convert(\"鼠標\")  # 漢字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'公安'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chinese_converter\n",
    "\n",
    "chinese_converter.to_traditional(\"公安\")\n",
    "#chinese_converter.to_simplified(\"traditional chinese text...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"ArithmeticError\n",
    "pip uninstall spacy\n",
    "pip uninstall srsly\n",
    "pip uninstall blis\n",
    "pip uninstall cymem\n",
    "pip uninstall preshed\n",
    "pip uninstall murmurhash\n",
    "\n",
    "pip install thinc==8.2.2\n",
    "pip install blis==0.7.8\n",
    "pip install cymem\n",
    "pip install preshed\n",
    "pip install murmurhash\n",
    "pip install spacy\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download ja_core_news_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "python -m spacy download es_core_news_sm\n",
    "python -m spacy download it_core_news_sm\n",
    "python -m spacy download ko_core_news_sm\n",
    "python -m spacy download ru_core_news_sm\n",
    "python -m spacy download fr_core_news_sm\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 加載模型和tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # # 獲取 [CLS] 標記的嵌入向量\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "    return cls_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fa0e30ddbf4690955355a6fb41fc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\foresight_User\\.cache\\huggingface\\hub\\models--yentinglin--Taiwan-LLM-7B-v2.0-chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0bdf4c83584615b359c98ed76eaf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee08d42734704313a0412ee0fb870776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8fe51115754e99b15625b836ab8840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24907aec124b460eaad4f4c8b85b758a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecdf997671448909f8b911063a91ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4743dd80412642de854e62f2a7cc6b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfd658862674a718e98e2614f50bb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f1aad5b02a42ee84282bdf19e88a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fd587b70914f45a05697ade2149338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82c456c06604b60b9592d2c9c70bcf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pip install transformers>=4.34\n",
    "# pip install accelerate\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"yentinglin/Taiwan-LLM-7B-v2.0-chat\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"你是一個question generator，會把文章轉成使用者在問什麼問題\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": f\"{context}\"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 載入 Hugging Face 的問答生成管線\n",
    "question_generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-2.7B\") # 模型修改\n",
    "\n",
    "# 定義生成問題的函數\n",
    "def generate_question(context):\n",
    "    prompt = f\"根據以下的文章生成一個問題：\\n\\n{context}\\n\\n問題：\"\n",
    "    questions = question_generator(prompt, max_length=50, num_return_sequences=1)\n",
    "    return questions[0]['generated_text']\n",
    "\n",
    "# 示例文章\n",
    "context = \"\"\"\n",
    "台灣位於東亞地區，鄰近中國大陸、日本和菲律賓。台灣的氣候屬於亞熱帶氣候，擁有豐富的自然資源和多樣的生態系統。台灣的人口約為2400萬，主要城市包括台北、高雄和台中。台灣的經濟高度發展，以科技產業和製造業為主，並且在全球半導體市場中佔有重要地位。\n",
    "\"\"\"\n",
    "\n",
    "# 生成問題\n",
    "generated_question = generate_question(context)\n",
    "print(generated_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章1和文章2\n",
    "article1 = \"這是一個中英夾雜的句子。This is a mixed Chinese and English sentence.\"\n",
    "article2 = \"test\"\n",
    "\n",
    "# 獲取向量\n",
    "vector1 = get_sentence_embedding(article1)#.detach().numpy()\n",
    "vector2 = get_sentence_embedding(article2)#.detach().numpy()\n",
    "\n",
    "# 計算餘弦相似度\n",
    "#similarity = cosine_similarity([vector1], [vector2])\n",
    "\n",
    "#print(f\"The cosine similarity between the two articles is: {similarity[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the two articles is: 0.4398084282875061\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 獲取向量\n",
    "vector1 = get_sentence_embedding(article1).detach().numpy()\n",
    "vector2 = get_sentence_embedding(article2).detach().numpy()\n",
    "\n",
    "# 計算餘弦相似度\n",
    "similarity = cosine_similarity([vector1], [vector2])\n",
    "\n",
    "print(f\"The cosine similarity between the two articles is: {similarity[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAnswerConsistencyEvaluator(AnswerSimilarityEvaluator):\n",
    "    def __init__(self, model_url, model_name, metric=None):\n",
    "        self.model_url = model_url\n",
    "        self.model_name = model_name\n",
    "        self.metric = metric\n",
    "\n",
    "    def get_llm_answer(self, question):\n",
    "        # 发送请求到本地模型\n",
    "        response = requests.post(\n",
    "            f\"{self.model_url}/completions\",\n",
    "            json={\n",
    "                \"model\": self.model_name,\n",
    "                \"prompt\": question,\n",
    "                \"max_tokens\": 100,\n",
    "                \"temperature\":0,\n",
    "                \"frequency_penalty\":0.2\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        llm_response = response.json()\n",
    "        # 返回生成的答案\n",
    "        return llm_response['choices'][0]['text']\n",
    "    \n",
    "    async def aevaluate(\n",
    "            self,\n",
    "            question,\n",
    "                retrieved_context_list,\n",
    "                reference_response\n",
    "                ):\n",
    "        # 获取本地模型生成的答案\n",
    "        llm_answer = self.get_llm_answer(question)\n",
    "        # 调用父类的方法进行评估\n",
    "        return await super().aevaluate(\n",
    "                        question,\n",
    "                        llm_answer,\n",
    "                        retrieved_context_list,\n",
    "                        reference_response\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用示例\n",
    "# 示例输入\n",
    "question = \"Sam Altman是一位優秀的創始人的原因是什麼？\"\n",
    "reference_answer = \"He is smart and has a great force of will.\"\n",
    "llm_answer = \"He is a good founder because he is smart.\"\n",
    "retrieved_context_list = [\n",
    "    \"Sam Altman is a good founder. He is very smart.\",\n",
    "    \"What makes Sam Altman such a good founder is his great force of will.\",\n",
    "]\n",
    "# 定义本地模型的URL和模型名称\n",
    "model_url = \"http://192.168.0.13:9100/v1\"\n",
    "model_name = \"/models/Breeze-7B-Instruct-v1_0\"\n",
    "\n",
    "answer_similarity_evaluator = CustomAnswerConsistencyEvaluator(model_url, model_name)\n",
    "\n",
    "score= answer_similarity_evaluator.aevaluate(question,retrieved_context_list,reference_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/models/Breeze-7B-Instruct-v1_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m answer_similarity_evaluator \u001b[38;5;241m=\u001b[39m CustomAnswerConsistencyEvaluator(model_url, model_name)\n\u001b[1;32m---> 51\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m answer_similarity_evaluator\u001b[38;5;241m.\u001b[39maevaluate(question, retrieved_context_list, reference_answer)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(score)\n",
      "Cell \u001b[1;32mIn[14], line 32\u001b[0m, in \u001b[0;36mCustomAnswerConsistencyEvaluator.aevaluate\u001b[1;34m(self, question, retrieved_context_list, reference_response)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maevaluate\u001b[39m(\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     27\u001b[0m         question,\n\u001b[0;32m     28\u001b[0m         retrieved_context_list,\n\u001b[0;32m     29\u001b[0m         reference_response\n\u001b[0;32m     30\u001b[0m     ):\n\u001b[0;32m     31\u001b[0m     llm_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_llm_answer(question)\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39maevaluate(\n\u001b[0;32m     33\u001b[0m                     question,\n\u001b[0;32m     34\u001b[0m                     llm_answer,\n\u001b[0;32m     35\u001b[0m                     retrieved_context_list,\n\u001b[0;32m     36\u001b[0m                     reference_response\n\u001b[0;32m     37\u001b[0m                 )\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\llama_index\\evaluation\\tonic_validate\\answer_similarity.py:49\u001b[0m, in \u001b[0;36mAnswerSimilarityEvaluator.aevaluate\u001b[1;34m(self, query, response, contexts, reference_response, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m benchmark_item \u001b[38;5;241m=\u001b[39m BenchmarkItem(question\u001b[38;5;241m=\u001b[39mquery, answer\u001b[38;5;241m=\u001b[39mreference_response)\n\u001b[0;32m     43\u001b[0m llm_response \u001b[38;5;241m=\u001b[39m LLMResponse(\n\u001b[0;32m     44\u001b[0m     llm_answer\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[0;32m     45\u001b[0m     llm_context_list\u001b[38;5;241m=\u001b[39mcontexts,\n\u001b[0;32m     46\u001b[0m     benchmark_item\u001b[38;5;241m=\u001b[39mbenchmark_item,\n\u001b[0;32m     47\u001b[0m )\n\u001b[1;32m---> 49\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\u001b[38;5;241m.\u001b[39mscore(llm_response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_service)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m EvaluationResult(\n\u001b[0;32m     52\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery, contexts\u001b[38;5;241m=\u001b[39mcontexts, response\u001b[38;5;241m=\u001b[39mresponse, score\u001b[38;5;241m=\u001b[39mscore\n\u001b[0;32m     53\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from llama_index.evaluation.tonic_validate import AnswerSimilarityEvaluator\n",
    "\n",
    "class CustomAnswerConsistencyEvaluator(AnswerSimilarityEvaluator):\n",
    "    def __init__(self, model_url, model_name, metric=None):\n",
    "        self.model_url = model_url\n",
    "        self.model_name = model_name\n",
    "        self.metric = metric\n",
    "\n",
    "    def get_llm_answer(self, question):\n",
    "        response = requests.post(\n",
    "            f\"{self.model_url}/completions\",\n",
    "            json={\n",
    "                \"model\": self.model_name,\n",
    "                \"prompt\": question,\n",
    "                \"max_tokens\": 100,\n",
    "                \"temperature\": 0,\n",
    "                \"frequency_penalty\": 0.2\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        llm_response = response.json()\n",
    "        return llm_response['choices'][0]['text']\n",
    "    \n",
    "    async def aevaluate(\n",
    "            self,\n",
    "            question,\n",
    "            retrieved_context_list,\n",
    "            reference_response\n",
    "        ):\n",
    "        llm_answer = self.get_llm_answer(question)\n",
    "        return await super().aevaluate(\n",
    "                        question,\n",
    "                        llm_answer,\n",
    "                        retrieved_context_list,\n",
    "                        reference_response\n",
    "                    )\n",
    "\n",
    "# Example usage\n",
    "question = \"Sam Altman是一位優秀的創始人的原因是什麼？\"\n",
    "reference_answer = \"He is smart and has a great force of will.\"\n",
    "retrieved_context_list = [\n",
    "    \"Sam Altman is a good founder. He is very smart.\",\n",
    "    \"What makes Sam Altman such a good founder is his great force of will.\",\n",
    "]\n",
    "model_url = \"http://192.168.0.13:9100/v1\"\n",
    "model_name = \"/models/Breeze-7B-Instruct-v1_0\"\n",
    "\n",
    "answer_similarity_evaluator = CustomAnswerConsistencyEvaluator(model_url, model_name)\n",
    "\n",
    "score = await answer_similarity_evaluator.aevaluate(question, retrieved_context_list, reference_answer)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object CustomAnswerConsistencyEvaluator.aevaluate at 0x00000277B1C28D00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLEU = 0.00 40.0/0.0/0.0/0.0 (BP = 1.000 ratio = 1.000 hyp_len = 5 ref_len = 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "\n",
    "refs = [ # First set of references\n",
    "         [\"社團\",\"與\",\"法人\",\"有什麼\",\"不同\"],\n",
    "         # Second set of references\n",
    "          [\"社團\",\"與\",\"法人\",\"差異\",\"有哪些\"],\n",
    "        ]\n",
    "sys = [\"社團\",\"法人\",\"的\",\"種類\",\"有哪些\"]\n",
    "\n",
    "bleu = BLEU()\n",
    "\n",
    "bleu.corpus_score(sys, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chrF2 = 52.17"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrf = CHRF()\n",
    "\n",
    "chrf.corpus_score(sys, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.pyplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m score\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m cands \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m我们都曾经年轻过\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\bert_score\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.3.12\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\bert_score\\score.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.pyplot'"
     ]
    }
   ],
   "source": [
    "\n",
    "from bert_score import score\n",
    "\n",
    "# data\n",
    "cands = ['我们都曾经年轻过']\n",
    "refs = ['虽然我们都年少，但还是懂事的']\n",
    "\n",
    "P, R, F1 = score(cands, refs, lang=\"zh\", verbose=True)\n",
    "\n",
    "print(f\"System level F1 score: {F1.mean():.3f}\") \n",
    "# System level F1 score: 0.959\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomAnswerConsistencyEvaluator at 0x242fc963550>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_similarity_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlgeval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlgeval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_metrics\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nlgeval'"
     ]
    }
   ],
   "source": [
    "from nlgeval import compute_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 49\u001b[0m\n\u001b[0;32m     43\u001b[0m retrieved_context_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSam Altman is a good founder. He is very smart.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m ]\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 执行评估\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m answer_similarity_evaluator\u001b[38;5;241m.\u001b[39maevaluate(\n\u001b[0;32m     50\u001b[0m     question\u001b[38;5;241m=\u001b[39mquestion,\n\u001b[0;32m     51\u001b[0m     llm_answer\u001b[38;5;241m=\u001b[39mllm_answer,\n\u001b[0;32m     52\u001b[0m     retrieved_context_list\u001b[38;5;241m=\u001b[39mretrieved_context_list\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 打印评估结果\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(score)\n",
      "Cell \u001b[1;32mIn[31], line 28\u001b[0m, in \u001b[0;36mCustomAnswerConsistencyEvaluator.aevaluate\u001b[1;34m(self, question, llm_answer, retrieved_context_list)\u001b[0m\n\u001b[0;32m     26\u001b[0m llm_generated_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_llm_answer(question)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 调用父类的方法进行评估\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39maevaluate(\n\u001b[0;32m     29\u001b[0m     question, llm_generated_answer, retrieved_context_list\n\u001b[0;32m     30\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\Lib\\site-packages\\llama_index\\evaluation\\tonic_validate\\answer_consistency.py:48\u001b[0m, in \u001b[0;36mAnswerConsistencyEvaluator.aevaluate\u001b[1;34m(self, query, response, contexts, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m benchmark_item \u001b[38;5;241m=\u001b[39m BenchmarkItem(question\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m     42\u001b[0m llm_response \u001b[38;5;241m=\u001b[39m LLMResponse(\n\u001b[0;32m     43\u001b[0m     llm_answer\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[0;32m     44\u001b[0m     llm_context_list\u001b[38;5;241m=\u001b[39mcontexts,\n\u001b[0;32m     45\u001b[0m     benchmark_item\u001b[38;5;241m=\u001b[39mbenchmark_item,\n\u001b[0;32m     46\u001b[0m )\n\u001b[1;32m---> 48\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\u001b[38;5;241m.\u001b[39mscore(llm_response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_service)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m EvaluationResult(\n\u001b[0;32m     51\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery, contexts\u001b[38;5;241m=\u001b[39mcontexts, response\u001b[38;5;241m=\u001b[39mresponse, score\u001b[38;5;241m=\u001b[39mscore\n\u001b[0;32m     52\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "class CustomAnswerConsistencyEvaluator(AnswerConsistencyEvaluator):\n",
    "    def __init__(self, model_url, model_name):\n",
    "        self.model_url = model_url\n",
    "        self.model_name = model_name\n",
    "        self.metric = None  # 添加metric属性，并设置为None\n",
    "\n",
    "    def get_llm_answer(self, question):\n",
    "        # 发送请求到本地模型\n",
    "        response = requests.post(\n",
    "            f\"{self.model_url}/completions\",\n",
    "            json={\n",
    "                \"model\": self.model_name,\n",
    "                \"prompt\": question,\n",
    "                \"max_tokens\": 100\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        llm_response = response.json()\n",
    "        # 返回生成的答案\n",
    "        return llm_response['choices'][0]['text']\n",
    "\n",
    "    async def aevaluate(\n",
    "        self, question, llm_answer, retrieved_context_list\n",
    "    ):\n",
    "        # 获取本地模型生成的答案\n",
    "        llm_generated_answer = self.get_llm_answer(question)\n",
    "        # 调用父类的方法进行评估\n",
    "        return await super().aevaluate(\n",
    "            question, llm_generated_answer, retrieved_context_list\n",
    "        )\n",
    "\n",
    "# 定义本地模型的URL和模型名称\n",
    "model_url = \"http://192.168.0.13:9100/v1\"\n",
    "model_name = \"/models/Breeze-7B-Instruct-v1_0\"\n",
    "\n",
    "# 创建自定义评估器实例\n",
    "answer_similarity_evaluator = CustomAnswerConsistencyEvaluator(model_url, model_name)\n",
    "\n",
    "# 示例输入\n",
    "question = \"What makes Sam Altman a good founder?\"\n",
    "reference_answer = \"He is smart and has a great force of will.\"\n",
    "llm_answer = \"He is a good founder because he is smart.\"\n",
    "retrieved_context_list = [\n",
    "    \"Sam Altman is a good founder. He is very smart.\"\n",
    "]\n",
    "\n",
    "\n",
    "# 执行评估\n",
    "score = await answer_similarity_evaluator.aevaluate(\n",
    "    question=question,\n",
    "    llm_answer=llm_answer,\n",
    "    retrieved_context_list=retrieved_context_list\n",
    ")\n",
    "\n",
    "# 打印评估结果\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import uuid\n",
    "# 模型URL和名称\n",
    "model_url = \"http://192.168.0.13:9100/v1\"\n",
    "model_name = \"/models/Breeze-7B-Instruct-v1_0\"\n",
    "\n",
    "# 您的问题\n",
    "question = \"我想請問所有的料理方式\"\n",
    "\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://fstdocqa.fs-technology.com/api/v1/completions/stream\"\n",
    "\n",
    "# 请求体\n",
    "payload = {\n",
    "    \"session_id\": str(uuid.uuid4()),#,\n",
    "    \"prompt\": question,\n",
    "    \"stream\": True,\n",
    "    \"model_name\": \"/models/Breeze-7B-Instruct-v1_0\"  # 添加 model_name\n",
    "}\n",
    "\n",
    "# JWT令牌\n",
    "jwt_token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJmc3R0ZWFjaCIsImp0aSI6Ijk5NGE5MTUwLThlM2YtNDVhNy05Njc5LWQ3MzQ5OTRmZGNmNiIsInBlcm1pc3Npb24iOiJtYW5hZ2VyIiwibmJmIjoxNzE5ODg1ODkyLCJleHAiOjE3MjAxODgyOTIsImlhdCI6MTcxOTg4NTg5MiwiaXNzIjoiS25vd2xlZGdlQ29ubmVjdCJ9.KAV5l1qc5fDETj9-c1lL3OKbUlpo7ykyBYjaKuEwr4o\"\n",
    "\n",
    "# 构建请求头\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {jwt_token}\",\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "    \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# 发送POST请求\n",
    "response = requests.post(api_url, json=payload, headers=headers, stream=True)\n",
    "\n",
    "# 检查请求是否成功\n",
    "if response.status_code == 200:\n",
    "    # 逐行读取响应（流式处理）\n",
    "    answer = []\n",
    "    for line in response.iter_lines():\n",
    "        # 打印每一行的内容\n",
    "        a = line.decode('utf-8')\n",
    "        answer.append(a)\n",
    "else:\n",
    "    print(f\"请求失败，状态码: {response.status_code}\")\n",
    "answer = \"\".join(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"C:/Users/foresight_User/Desktop/知識通/HUBRAIN/評分結果/評分版_HUBRAIN_QA評估量表20240617_v2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = data.drop(columns=[\"ITEM\"]).groupby([\"Factor\"]).mean().reset_index()#.std(axis=1)\n",
    "#pd.melt(data_2, id_vars=['Factor'], value_vars=[題目一\t題目二\t題目三\t題目四\t題目五\t題目六\t題目七\t題目八\t題目九\t題目十])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = pd.melt(data_2,id_vars=['Factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='value', ylabel='Factor'>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(data=data_3, x=\"value\", y=\"Factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor\n",
       " 1—Perceived accessibility to chatbot functions                  4.500000\n",
       " 3—Perceived quality of conversation and information provided    3.150000\n",
       " 4—Perceived privacy and security                                1.000000\n",
       " 5—Time response                                                 5.000000\n",
       "2—Perceived quality of chatbot functions                         3.166667\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns=[\"ITEM\"]).groupby([\"Factor\"]).mean().mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#  1—Perceived accessibility to chatbot functions\n",
    "#  1—Perceived accessibility to chatbot functions\n",
    "# 2—Perceived quality of chatbot functions\n",
    "# 2—Perceived quality of chatbot functions\n",
    "# 2—Perceived quality of chatbot functions\n",
    "# 2—Perceived quality of chatbot functions\n",
    "# 2—Perceived quality of chatbot functions\n",
    "# 2—Perceived quality of chatbot functions\n",
    "# 2—Perceived quality of chatbot functions\n",
    "#  3—Perceived quality of conversation and information provided\n",
    "#  3—Perceived quality of conversation and information provided\n",
    "#  3—Perceived quality of conversation and information provided\n",
    "#  3—Perceived quality of conversation and information provided\n",
    "#  4—Perceived privacy and security\n",
    "#  5—Time response\n",
    "\n",
    "# 1—Perceived accessibility to chatbot functions # 1. 聊天機器人功能操作流暢\t\n",
    "                                                  # 2. 很容易找到聊天機器人的功能\t\n",
    "# 2—Perceived quality of chatbot functions# 3. 聊天機器人的回應清晰明瞭\t\n",
    "# 2—Perceived quality of chatbot functions  # 4. 我能立即明白到聊天機器人提供的資訊\t\n",
    "# 2—Perceived quality of chatbot functions 5. 與聊天機器人的互動感覺就像在對話\t\n",
    "# 2—Perceived quality of chatbot functions 6. 聊天機器人能夠追蹤上下文\t\n",
    "# 2—Perceived quality of chatbot functions 7. 聊天機器人能提供正確的引述資訊來源\t\n",
    "# 2—Perceived quality of chatbot functions 8. 聊天機器人能夠處理指令不清晰的狀況\t\n",
    "# 2—Perceived quality of chatbot functions 9. 聊天機器人的回覆使人容易理解\t\n",
    "# 3—Perceived quality of conversation and information provided 10. 我認為聊天機器人能理解我的需求並幫助我找到答案\t\n",
    "# 3—Perceived quality of conversation and information provided 11. 聊天機器人提供我可靠的訊息\t\n",
    "# 3—Perceived quality of conversation and information provided 12. 聊天機器人只提供我需要的資訊\t\n",
    "# 3—Perceived quality of conversation and information provided   13. 我覺得聊天機器人的回覆很準確\t\n",
    "# 4—Perceived privacy and security  14. 我覺得聊天機器人會提醒我注意任何有關隱私的問題\t\n",
    "#  5—Time response  # 15. 我等待聊天機器人回覆的時間很短\n",
    "\n",
    "data = pd.read_excel(\"C:/Users/foresight_User/Desktop/知識通/中文測試結果/知識通使用心得(更新) (回覆)v2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Perceived accessibility to chatbot functions\"] = data[[\"1. 聊天機器人功能操作流暢\", \"2. 很容易找到聊天機器人的功能\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Perceived accessibility to chatbot functions\"] = data[[\"1. 聊天機器人功能操作流暢\", \"2. 很容易找到聊天機器人的功能\"]].mean(axis=1)\n",
    "data[\"Perceived quality of chatbot functions\"] = data[[\"3. 聊天機器人的回應清晰明瞭\", \"4. 我能立即明白到聊天機器人提供的資訊\",\"4. 我能立即明白到聊天機器人提供的資訊\",\"5. 與聊天機器人的互動感覺就像在對話\",\"6. 聊天機器人能夠追蹤上下文\",\"7. 聊天機器人能提供正確的引述資訊來源\",\"8. 聊天機器人能夠處理指令不清晰的狀況\",\"9. 聊天機器人的回覆使人容易理解\"]].mean(axis=1)\n",
    "data[\"Perceived quality of conversation and information provided\"] = data[[\"10. 我認為聊天機器人能理解我的需求並幫助我找到答案\", \"11. 聊天機器人提供我可靠的訊息\",\"12. 聊天機器人只提供我需要的資訊\",\"13. 我覺得聊天機器人的回覆很準確\"]].mean(axis=1)\n",
    "data[\"Perceived privacy and security\"] = data[[\"14. 我覺得聊天機器人會提醒我注意任何有關隱私的問題\"]].mean(axis=1)\n",
    "data[\"Time response\"] = data[[\"15. 我等待聊天機器人回覆的時間很短\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"ID\",\"Perceived accessibility to chatbot functions\",\"Perceived quality of chatbot functions\",\"Perceived quality of conversation and information provided\",\"Perceived privacy and security\",\"Time response\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"C:/Users/foresight_User/Desktop/知識通/中文測試結果/知識通使用心得(更新) (回覆)v3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_2 = pd.melt(data, id_vars=['ID'], value_vars=[\"Perceived accessibility to chatbot functions\",\"Perceived quality of chatbot functions\",\"Perceived quality of conversation and information provided\",\"Perceived privacy and security\",\"Time response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>haah@gmail.com</td>\n",
       "      <td>Perceived accessibility to chatbot functions</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mayo13012515@gmail.com</td>\n",
       "      <td>Perceived accessibility to chatbot functions</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poly19650@gmail.com</td>\n",
       "      <td>Perceived accessibility to chatbot functions</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daniel@fs-technology.com</td>\n",
       "      <td>Perceived accessibility to chatbot functions</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jimwu@fs-technology.com</td>\n",
       "      <td>Perceived accessibility to chatbot functions</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>jimwu@fs-technology.com</td>\n",
       "      <td>Time response</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>kthu@fs-technology.com</td>\n",
       "      <td>Time response</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>water@fs-technology.com</td>\n",
       "      <td>Time response</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>gnkao@fs-technology.com</td>\n",
       "      <td>Time response</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>eason19910419@gmail.com</td>\n",
       "      <td>Time response</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ID                                      variable  \\\n",
       "0              haah@gmail.com  Perceived accessibility to chatbot functions   \n",
       "1      mayo13012515@gmail.com  Perceived accessibility to chatbot functions   \n",
       "2         poly19650@gmail.com  Perceived accessibility to chatbot functions   \n",
       "3    daniel@fs-technology.com  Perceived accessibility to chatbot functions   \n",
       "4     jimwu@fs-technology.com  Perceived accessibility to chatbot functions   \n",
       "..                        ...                                           ...   \n",
       "445   jimwu@fs-technology.com                                 Time response   \n",
       "446    kthu@fs-technology.com                                 Time response   \n",
       "447   water@fs-technology.com                                 Time response   \n",
       "448   gnkao@fs-technology.com                                 Time response   \n",
       "449   eason19910419@gmail.com                                 Time response   \n",
       "\n",
       "     value  \n",
       "0      5.0  \n",
       "1      4.0  \n",
       "2      4.0  \n",
       "3      4.0  \n",
       "4      3.5  \n",
       "..     ...  \n",
       "445    4.0  \n",
       "446    5.0  \n",
       "447    4.0  \n",
       "448    4.0  \n",
       "449    4.0  \n",
       "\n",
       "[450 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
