{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "dir = \"C:/Users/foresight_User/Desktop/公司文件/4.CIT季賽/測試資料/\"\n",
    "# CH_TO = pd.read_csv(dir+\"CH_TO.csv\")\n",
    "# X = CH_TO.drop(columns=[\"Y\",\"Context Name\"])#.to_numpy()\n",
    "# y = CH_TO[\"Y\"]#.to_numpy()\n",
    "# #加入CROSS項目，有時候會顯著，有時候不會顯著，明顯受到切資料的影響， 但CROSS BEST的模型確實會比較好\n",
    "\n",
    "\n",
    "#\n",
    "X = pd.read_csv(dir+\"wb/Indicator_Data(1).csv\").drop(columns=[\"Unnamed: 74\",\"Context Name\"])#.to_numpy()\n",
    "y = pd.read_csv(dir+\"wb/Metrology_Data(1).csv\")[\"Point1\"]#.to_numpy()\n",
    "#\n",
    "\n",
    "\n",
    "data = pd.read_excel(dir+\"Array7_N808測試資料集/Model_TJN808XK_SAMP75.xlsx\").drop(columns=[\"CONTEXTID\"])\n",
    "X = data.drop(columns=list(filter(lambda x: \"T2_CD\" in x,data.columns)))#.to_numpy()\n",
    "X = X.fillna(value=0)\n",
    "y = data[\"T2_CD01_Mean\"]#.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#X = torch.FloatTensor(X).requires_grad_(True)\n",
    "#y = torch.FloatTensor(y).requires_grad_(True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   X, y, test_size=0.3,random_state=np.random.randint(0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.2'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "reg = RandomForestRegressor()\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.23910509084044484"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error,mean_absolute_percentage_error\n",
    "# 計算 Deviance\n",
    "from sklearn.metrics import mean_tweedie_deviance\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "def deviance(real, predicted):\n",
    "    if isinstance(predicted, pd.Series):\n",
    "        predicted = predicted.to_numpy()\n",
    "    real = real.to_numpy().reshape(-1)\n",
    "    predicted =  predicted.reshape(-1)\n",
    "    scaler =  MinMaxScaler().fit(np.concatenate((real, predicted),axis=0).reshape(-1,1))\n",
    "    real_std = scaler.transform(real.reshape(-1, 1))\n",
    "    predicted_std = scaler.transform(predicted.reshape(-1, 1))\n",
    "    if sum(predicted_std < 0) or sum(real_std < 0):\n",
    "        nag_idx = np.concatenate([np.where(predicted_std < 0)[0], np.where(real_std < 0)[0]])\n",
    "        predicted_std = np.delete(predicted_std, nag_idx, 0)\n",
    "        real_std = np.delete(real_std, nag_idx, 0)\n",
    "    return mean_tweedie_deviance(real_std, predicted_std)\n",
    "\n",
    "# 計算 RMSLE (Root Mean Squared Logarithmic Error)\n",
    "def root_mean_squared_log_error(real, predicted):\n",
    "    if isinstance(predicted, pd.Series):\n",
    "        predicted = predicted.to_numpy()\n",
    "    real = real.to_numpy().reshape(-1)\n",
    "    predicted =  predicted.reshape(-1)\n",
    "    scaler =  MinMaxScaler().fit(np.concatenate((real, predicted),axis=0).reshape(-1,1))\n",
    "    real_std = scaler.transform(real.reshape(-1, 1))\n",
    "    predicted_std = scaler.transform(predicted.reshape(-1, 1))\n",
    "    if sum(predicted_std < 0) or sum(real_std < 0):\n",
    "        nag_idx = np.concatenate([np.where(predicted_std < 0)[0], np.where(real_std < 0)[0]])\n",
    "        predicted_std = np.delete(predicted_std, nag_idx, 0)\n",
    "        real_std = np.delete(real_std, nag_idx, 0)\n",
    "    return mean_squared_log_error(real_std, predicted_std, squared=False)\n",
    "\n",
    "\n",
    "def _95_error(real, predicted):\n",
    "    if isinstance(predicted, pd.Series):\n",
    "        predicted = predicted.to_numpy()\n",
    "    real = real.to_numpy().reshape(-1)\n",
    "    predicted = predicted.reshape(-1)\n",
    "    residual = abs(real - predicted)\n",
    "    sorted_residual = sorted(residual)\n",
    "    ninety_five_err = sorted_residual[: int(len(sorted_residual) * 0.95)][-1]\n",
    "    return ninety_five_err\n",
    "def adjusted_r2_score(real, predicted, n, p):\n",
    "    r2 = r2_score(real, predicted)\n",
    "    adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1))\n",
    "    return adjusted_r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>confidence</th>\n",
       "      <th>metrics</th>\n",
       "      <th>rando_guess_lower_bound</th>\n",
       "      <th>rando_guess_upper_bound</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.239105</td>\n",
       "      <td>0.95</td>\n",
       "      <td>R2</td>\n",
       "      <td>-0.017808</td>\n",
       "      <td>0.09933</td>\n",
       "      <td>Extremely Poor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  confidence metrics  rando_guess_lower_bound  \\\n",
       "0 -0.239105        0.95      R2                -0.017808   \n",
       "\n",
       "   rando_guess_upper_bound          status  \n",
       "0                  0.09933  Extremely Poor  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "\n",
    "def confidence_interval(data, confidence):\n",
    "    n = len(data)\n",
    "    mean_value = np.mean(data)\n",
    "    std_error = np.std(data, ddof=1) / np.sqrt(n)\n",
    "    margin_error = std_error * t.ppf((1 + confidence) / 2., n - 1)\n",
    "    return mean_value, mean_value - margin_error, mean_value + margin_error\n",
    "\n",
    "def monte_carlo_simulation(y_true, n_simulations):\n",
    "    y_preds_random = []\n",
    "    for _ in range(n_simulations):\n",
    "        noise = np.random.normal(0, np.std(y_true) * 1, size=len(y_true))\n",
    "        y_pred_random = y_true + noise\n",
    "        y_preds_random.append(y_pred_random)\n",
    "    return np.array(y_preds_random)\n",
    "def random_guess(metric_fuc, y_true, y_preds_random):\n",
    "    values = [metric_fuc(y_true, y_pred_random) for y_pred_random in y_preds_random]\n",
    "    return values\n",
    "\n",
    "def evaluation_metric(metric_fuc,y_true,y_pred):\n",
    "    return metric_fuc(y_true, y_pred)\n",
    "\n",
    "def generate_evaluation_dataframe(y_true: Union[pd.Series, np.ndarray] = None, y_pred: Union[pd.Series, np.ndarray] = None, n_simulations:int =100, confidence:float = 0.95, metrics_info:dict ={},direction:str=None):\n",
    "    if not isinstance(y_true, (pd.Series, np.ndarray)):\n",
    "        raise TypeError(\"Only pd.Series and np.ndarray are allowed\")\n",
    "\n",
    "    y_preds_random = monte_carlo_simulation(y_true, n_simulations)\n",
    "    # 構建DataFrame\n",
    "    data = []\n",
    "    for metric_name,metric_fuc in metrics_info.items():\n",
    "        \n",
    "        values = random_guess(metric_fuc,y_true, y_preds_random)\n",
    "        _,lower_bound, upper_bound = confidence_interval(values, confidence)\n",
    "        score = evaluation_metric(metric_fuc,y_true,y_pred)\n",
    "        if direction == \"maximum\":\n",
    "            if score > upper_bound:\n",
    "                status = 'Fair' #\n",
    "            elif (score <= upper_bound) and (score >= lower_bound):\n",
    "                status = 'Poor' #\n",
    "            elif score < lower_bound:\n",
    "                status = 'Extremely Poor' #\n",
    "            else:\n",
    "                status = np.nan  # 預防未定義的情況\n",
    "        elif direction == \"minimum\":\n",
    "            if score < lower_bound:\n",
    "                status = 'Fair' #\n",
    "            elif (score <= upper_bound) and (score >= lower_bound):\n",
    "                status = 'Poor' #\n",
    "            elif score > upper_bound:\n",
    "                status = 'Extremely Poor' #\n",
    "            else:\n",
    "                status = np.nan  # 預防未定義的情況\n",
    "        data.append([score, confidence, metric_name, lower_bound, upper_bound, status])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['score', 'confidence', 'metrics', 'rando_guess_lower_bound', 'rando_guess_upper_bound', 'status'])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# 範例一\n",
    "metric_minimum ={'MAE':mean_absolute_error, 'MAPE':mean_absolute_percentage_error, 'MSE':mean_squared_error, 'Deviance':deviance, 'RMSLE':root_mean_squared_log_error}# 這邊放哪個評估分數越小越好\n",
    "\n",
    "generate_evaluation_dataframe(y_true=y_test, y_pred=y_pred,metrics_info=metric_minimum,direction = \"minimum\")#\n",
    "\n",
    "\n",
    "# 範例二\n",
    "metric_maximum ={'R2':r2_score} # 這邊放哪個評估分數越大越號\n",
    "generate_evaluation_dataframe(y_true=y_test, y_pred=y_pred,metrics_info=metric_maximum,direction = \"maximum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>confidence</th>\n",
       "      <th>metrics</th>\n",
       "      <th>rando_guess_lower_bound</th>\n",
       "      <th>rando_guess_upper_bound</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.121376</td>\n",
       "      <td>0.95</td>\n",
       "      <td>MAE</td>\n",
       "      <td>0.098982</td>\n",
       "      <td>0.104923</td>\n",
       "      <td>Extremely Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028918</td>\n",
       "      <td>0.95</td>\n",
       "      <td>MAPE</td>\n",
       "      <td>0.023560</td>\n",
       "      <td>0.024978</td>\n",
       "      <td>Extremely Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020507</td>\n",
       "      <td>0.95</td>\n",
       "      <td>MSE</td>\n",
       "      <td>0.015430</td>\n",
       "      <td>0.017303</td>\n",
       "      <td>Extremely Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.082140</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Deviance</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>0.037790</td>\n",
       "      <td>Extremely Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.204425</td>\n",
       "      <td>0.95</td>\n",
       "      <td>RMSLE</td>\n",
       "      <td>0.126507</td>\n",
       "      <td>0.135704</td>\n",
       "      <td>Extremely Poor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  confidence   metrics  rando_guess_lower_bound  \\\n",
       "0  0.121376        0.95       MAE                 0.098982   \n",
       "1  0.028918        0.95      MAPE                 0.023560   \n",
       "2  0.020507        0.95       MSE                 0.015430   \n",
       "3  0.082140        0.95  Deviance                 0.032707   \n",
       "4  0.204425        0.95     RMSLE                 0.126507   \n",
       "\n",
       "   rando_guess_upper_bound          status  \n",
       "0                 0.104923  Extremely Poor  \n",
       "1                 0.024978  Extremely Poor  \n",
       "2                 0.017303  Extremely Poor  \n",
       "3                 0.037790  Extremely Poor  \n",
       "4                 0.135704  Extremely Poor  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_minimum ={'MAE':mean_absolute_error, 'MAPE':mean_absolute_percentage_error, 'MSE':mean_squared_error, 'Deviance':deviance, 'RMSLE':root_mean_squared_log_error}# 這邊放哪個評估分數越小越好\n",
    "\n",
    "generate_evaluation_dataframe(y_true=y_test, y_pred=y_pred,metrics_info=metric_minimum,direction = \"minimum\")#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>confidence</th>\n",
       "      <th>metrics</th>\n",
       "      <th>rando_guess_lower_bound</th>\n",
       "      <th>rando_guess_upper_bound</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.239105</td>\n",
       "      <td>0.95</td>\n",
       "      <td>R2</td>\n",
       "      <td>-0.064532</td>\n",
       "      <td>0.064059</td>\n",
       "      <td>Extremely Poor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  confidence metrics  rando_guess_lower_bound  \\\n",
       "0 -0.239105        0.95      R2                -0.064532   \n",
       "\n",
       "   rando_guess_upper_bound          status  \n",
       "0                 0.064059  Extremely Poor  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_maximum ={'R2':r2_score} # 這邊放哪個評估分數越大越號\n",
    "generate_evaluation_dataframe(y_true=y_test, y_pred=y_pred,metrics_info=metric_maximum,direction = \"maximum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only pd.Series and np.ndarray are allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_evaluation_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetrics_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_minimum\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mminimum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[131], line 29\u001b[0m, in \u001b[0;36mgenerate_evaluation_dataframe\u001b[1;34m(y_true, y_pred, n_simulations, confidence, metrics_info, direction)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_evaluation_dataframe\u001b[39m(y_true: Union[pd\u001b[38;5;241m.\u001b[39mSeries, np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, y_pred: Union[pd\u001b[38;5;241m.\u001b[39mSeries, np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, n_simulations:\u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, confidence:\u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m, metrics_info:\u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m{},direction:\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y_true, (pd\u001b[38;5;241m.\u001b[39mSeries, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[1;32m---> 29\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly pd.Series and np.ndarray are allowed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m     y_preds_random \u001b[38;5;241m=\u001b[39m monte_carlo_simulation(y_true, n_simulations)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# 構建DataFrame\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Only pd.Series and np.ndarray are allowed"
     ]
    }
   ],
   "source": [
    "generate_evaluation_dataframe(y_true=y_test.to_list(), y_pred=y_pred,metrics_info=metric_minimum,direction = \"minimum\")#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.19401993, 4.23459339, 4.23224582, 4.27229757, 4.24244304,\n",
       "       4.20158293, 4.3247526 , 4.19568755, 4.21431532, 4.24016197,\n",
       "       4.24225062, 4.24905322, 4.15974196, 4.24481582, 4.2317389 ,\n",
       "       4.26326687, 4.23491014, 4.2474046 , 4.25394009, 4.17030641,\n",
       "       4.18797309, 4.28438436, 4.25293465])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_evaluation_dataframe(y_true, y_pred, n_simulations:int =100, confidence:list =[0.95, 0.997], metrics_info:dict ={}):\n",
    "#     y_preds_random = monte_carlo_simulation(y_true, n_simulations)\n",
    "#     # 構建DataFrame\n",
    "#     data = []\n",
    "#     for metric_name,metric_fuc in metrics_info.items():\n",
    "        \n",
    "#         values = random_guess(metric_fuc,y_true, y_preds_random)\n",
    "#         for conf in confidence:\n",
    "#             _,lower_bound, upper_bound = confidence_interval(values, conf)\n",
    "#             score = evaluation_metric(metric_fuc,y_true,y_pred)\n",
    "#             if score < lower_bound:\n",
    "#                 status = 'below_CI'\n",
    "#             elif score > upper_bound:\n",
    "#                 status = 'above_CI'\n",
    "#             else:\n",
    "#                 status = 'within_CI'\n",
    "#             data.append([score, conf, metric_name, lower_bound, upper_bound, status])\n",
    "    \n",
    "#     return pd.DataFrame(data, columns=['score', 'confidence', 'metrics', 'lower_bound', 'upper_bound', 'status'])\n",
    "\n",
    "# def generate_evaluation_dataframe(y_true, y_pred, n_simulations:int =100, confidence:list =[0.95, 0.997], metrics_info:dict ={},direction:str=None,transform=None):\n",
    "#     y_preds_random = monte_carlo_simulation(y_true, n_simulations)\n",
    "#     # 構建DataFrame\n",
    "#     data = []\n",
    "#     for metric_name,metric_fuc in metrics_info.items():\n",
    "        \n",
    "#         values = random_guess(metric_fuc,y_true, y_preds_random)\n",
    "#         for conf in confidence:\n",
    "#             _,lower_bound, upper_bound = confidence_interval(values, conf)\n",
    "#             score = evaluation_metric(metric_fuc,y_true,y_pred)\n",
    "#             if score < lower_bound:\n",
    "#                 status = 'below_CI'\n",
    "#             elif score > upper_bound:\n",
    "#                 status = 'above_CI'\n",
    "#             else:\n",
    "#                 status = 'within_CI'\n",
    "#             data.append([score, conf, metric_name, lower_bound, upper_bound, status])\n",
    "#     result = pd.DataFrame(data, columns=['score', 'confidence', 'metrics', 'lower_bound', 'upper_bound', 'status'])\n",
    "\n",
    "#     if transform !=True:\n",
    "#         return result\n",
    "#     else:\n",
    "#         data = []\n",
    "#         for metric in metrics_info.keys():\n",
    "#             df = result[result[\"metrics\"] == metric]\n",
    "#             if direction == \"minimum\":\n",
    "#                 if ((df.confidence == 0.997) & (df.status == \"below_CI\")).all():\n",
    "#                     state = 'Good'\n",
    "#                 elif ((df.confidence == 0.997) &  (df.status == \"within_CI\")).all() and ((df.confidence == 0.95) & (df.status == \"below_CI\")).all():\n",
    "#                     state = 'Fair'\n",
    "#                 elif ((df.confidence == 0.997) & (df.status == \"within_CI\")).all() and ((df.confidence == 0.95) & (df.status == \"within_CI\")).all():\n",
    "#                     state = 'Poor'\n",
    "#                 elif ((df.confidence == 0.997) & df.status == \"within_CI\").all() and ((df.confidence == 0.95) & (df.status == \"above_CI\")).all():\n",
    "#                     state = 'Very Poor'\n",
    "#                 elif ((df.confidence == 0.997) & (df.status == \"above_CI\")).all():\n",
    "#                     state = 'Extremely Poor'\n",
    "#                 else:\n",
    "#                     state = np.nan\n",
    "#             elif direction == \"maximum\":\n",
    "#                 if ((df.confidence == 0.997) & (df.status == \"above_CI\")).all():\n",
    "#                     state = 'Good'\n",
    "#                 elif ((df.confidence == 0.997) and (df.status == \"within_CI\")).all() and ((df.confidence == 0.95) and (df.status == \"above_CI\")).all():\n",
    "#                     state = 'Fair'\n",
    "#                 elif ((df.confidence == 0.997) and (df.status == \"within_CI\")).all() and ((df.confidence == 0.95) and (df.status == \"within_CI\")).all():\n",
    "#                     state = 'Poor'\n",
    "#                 elif ((df.confidence == 0.997) and (df.status == \"within_CI\")).all() and (df.confidence == 0.95 and (df.status == \"below_CI\")).all():\n",
    "#                     state = 'Very Poor'\n",
    "#                 elif ((df.confidence == 0.997) and (df.status == \"below_CI\")).all():\n",
    "#                     state = 'Extremely Poor'\n",
    "#                 else:\n",
    "#                     state = np.nan\n",
    "#             data.append([metric,state])\n",
    "#         result = pd.DataFrame(data, columns=[ 'metrics','state'])\n",
    "#         return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from optuna.pruners import ThresholdPruner,PercentilePruner,SuccessiveHalvingPruner\n",
    "import optuna\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "random_state = 3\n",
    "n_train_iter = 1\n",
    "X, y = make_regression(n_samples=100000,n_features=40,n_informative=40, noise=0.5,random_state=random_state)\n",
    "# X, y = load_iris(return_X_y=True)\n",
    "X_valid, _, y_valid, _ = train_test_split(X,y)\n",
    "# classes = np.unique(y)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    auto_tuning_params = {\n",
    "        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 0.0, 9.0),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-6, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 5),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 2.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        'random_state': random_state\n",
    "    }\n",
    "    clf = xgb.XGBRegressor(**auto_tuning_params)\n",
    "\n",
    "    clf.fit(X_valid, y_valid)\n",
    "    score = clf.score(X_valid, y_valid)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-21 08:23:55,020] A new study created in memory with name: no-name-4d8a8300-a246-472e-bc58-4554333e83a2\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:27:25,157] Trial 0 finished with value: -1.0738938765298764e+24 and parameters: {'colsample_bytree': 0.5990505973530461, 'gamma': 0.7038913828895049, 'learning_rate': 0.6846363438204282, 'max_depth': 11, 'min_child_weight': 4, 'n_estimators': 5800, 'reg_alpha': 1.4855655100548075, 'reg_lambda': 0.837369033452857, 'subsample': 0.2554770461607322}. Best is trial 0 with value: -1.0738938765298764e+24.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:27:41,723] Trial 1 finished with value: 0.9984731923844108 and parameters: {'colsample_bytree': 0.4159281783539778, 'gamma': 5.415975768536057, 'learning_rate': 0.5657261586824908, 'max_depth': 4, 'min_child_weight': 4, 'n_estimators': 3400, 'reg_alpha': 1.4091289939061526, 'reg_lambda': 0.2533751172162346, 'subsample': 0.910189896042414}. Best is trial 1 with value: 0.9984731923844108.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:28:51,299] Trial 2 finished with value: 0.9999951471753669 and parameters: {'colsample_bytree': 0.7326854063118725, 'gamma': 6.89576861021518, 'learning_rate': 0.41116624514903327, 'max_depth': 11, 'min_child_weight': 4, 'n_estimators': 8000, 'reg_alpha': 0.3654161579789291, 'reg_lambda': 0.5614960642753386, 'subsample': 0.2893275178209165}. Best is trial 2 with value: 0.9999951471753669.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:28:54,199] Trial 3 finished with value: 0.9652128751079465 and parameters: {'colsample_bytree': 0.8509977343146691, 'gamma': 8.452674137882159, 'learning_rate': 0.4454338499286435, 'max_depth': 5, 'min_child_weight': 5, 'n_estimators': 400, 'reg_alpha': 1.5334821087345707, 'reg_lambda': 0.5713569766059639, 'subsample': 0.4547445722643617}. Best is trial 2 with value: 0.9999951471753669.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:29:19,686] Trial 4 finished with value: 0.9999966281129418 and parameters: {'colsample_bytree': 0.273328377304636, 'gamma': 2.342002155915538, 'learning_rate': 0.7465667759512754, 'max_depth': 12, 'min_child_weight': 4, 'n_estimators': 4100, 'reg_alpha': 0.2383196136356333, 'reg_lambda': 0.8083550831163521, 'subsample': 0.8948508469853538}. Best is trial 4 with value: 0.9999966281129418.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:30:18,858] Trial 5 finished with value: 0.9999956997563018 and parameters: {'colsample_bytree': 0.8709363347677694, 'gamma': 4.827024980259381, 'learning_rate': 0.5579802482471281, 'max_depth': 17, 'min_child_weight': 3, 'n_estimators': 3300, 'reg_alpha': 1.3512723281309118, 'reg_lambda': 0.4315938003206553, 'subsample': 0.6459745076174153}. Best is trial 4 with value: 0.9999966281129418.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:30:51,989] Trial 6 finished with value: 0.999993144661133 and parameters: {'colsample_bytree': 0.6461256636912402, 'gamma': 8.679988056550775, 'learning_rate': 0.7449582997627582, 'max_depth': 13, 'min_child_weight': 5, 'n_estimators': 4800, 'reg_alpha': 0.7417340740846806, 'reg_lambda': 0.7717870005916583, 'subsample': 0.56594178777392}. Best is trial 4 with value: 0.9999966281129418.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:31:13,883] Trial 7 finished with value: 0.9712357727344484 and parameters: {'colsample_bytree': 0.40874911713181106, 'gamma': 5.979671659317736, 'learning_rate': 0.8505587695458331, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 5400, 'reg_alpha': 0.9143257909694847, 'reg_lambda': 0.5555856781248119, 'subsample': 0.4009552361758287}. Best is trial 4 with value: 0.9999966281129418.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:31:54,888] Trial 8 finished with value: 0.9999950422934363 and parameters: {'colsample_bytree': 0.565528101160434, 'gamma': 3.982328494926734, 'learning_rate': 0.23451704337510274, 'max_depth': 11, 'min_child_weight': 3, 'n_estimators': 6600, 'reg_alpha': 0.986604290901652, 'reg_lambda': 0.32615318521102565, 'subsample': 0.703441854285521}. Best is trial 4 with value: 0.9999966281129418.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:33:13,398] Trial 9 finished with value: 0.9999941258661932 and parameters: {'colsample_bytree': 0.13420210555119064, 'gamma': 0.12999112487862352, 'learning_rate': 0.11867395983936939, 'max_depth': 7, 'min_child_weight': 4, 'n_estimators': 6600, 'reg_alpha': 0.2560031928102213, 'reg_lambda': 0.12444387018949477, 'subsample': 0.4370552442353017}. Best is trial 4 with value: 0.9999966281129418.\n"
     ]
    }
   ],
   "source": [
    "# tolerence = 1,2,3,4,5,6,7,8,9,10 .....\n",
    "n_trials = 10\n",
    "tolerence = 1\n",
    "if tolerence > 1:\n",
    "    # n_trials = int(n_trials/tolerence**(2/5))\n",
    "    # if n_trials == 0:\n",
    "    #     n_trials = 1\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\", pruner=SuccessiveHalvingPruner(reduction_factor=tolerence)\n",
    "    )\n",
    "else:\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\"\n",
    "    )    \n",
    "\n",
    "\n",
    "study.optimize(objective, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-21 08:52:01,177] A new study created in memory with name: no-name-29562e06-1ce5-43ab-ac39-52cf6f08046b\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:52:54,370] Trial 0 finished with value: 0.9999758879990331 and parameters: {'colsample_bytree': 0.28650608188464566, 'gamma': 4.188009165882256, 'learning_rate': 0.0836222192698602, 'max_depth': 8, 'min_child_weight': 4, 'n_estimators': 3200, 'reg_alpha': 1.415757023380406, 'reg_lambda': 0.10982493369013135, 'subsample': 0.5461107455861622}. Best is trial 0 with value: 0.9999758879990331.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:53:19,711] Trial 1 finished with value: 0.9999908957605316 and parameters: {'colsample_bytree': 0.31333062141315793, 'gamma': 5.134944575626767, 'learning_rate': 0.9479261835689776, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 5600, 'reg_alpha': 0.2739753668308631, 'reg_lambda': 0.18478393598869325, 'subsample': 0.95990093934548}. Best is trial 1 with value: 0.9999908957605316.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:55:27,593] Trial 2 finished with value: 0.9999878576309271 and parameters: {'colsample_bytree': 0.10001731322386666, 'gamma': 5.863924750202872, 'learning_rate': 0.48179002906222174, 'max_depth': 14, 'min_child_weight': 3, 'n_estimators': 2200, 'reg_alpha': 1.1879626408752464, 'reg_lambda': 0.9940140119961073, 'subsample': 0.2463116634357647}. Best is trial 1 with value: 0.9999908957605316.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:55:29,713] Trial 3 finished with value: 0.9709465304536908 and parameters: {'colsample_bytree': 0.2613580714010387, 'gamma': 8.04277011683711, 'learning_rate': 0.07510043439530531, 'max_depth': 4, 'min_child_weight': 3, 'n_estimators': 400, 'reg_alpha': 1.4278956063916775, 'reg_lambda': 0.46094526089492804, 'subsample': 0.21571081007802428}. Best is trial 1 with value: 0.9999908957605316.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 08:59:44,598] Trial 4 finished with value: -2.0600746340177434e+17 and parameters: {'colsample_bytree': 0.29261547022446033, 'gamma': 1.6130338281087808, 'learning_rate': 0.6875616637139496, 'max_depth': 12, 'min_child_weight': 4, 'n_estimators': 5300, 'reg_alpha': 0.8084626777438386, 'reg_lambda': 0.6478883721403099, 'subsample': 0.27828623206734715}. Best is trial 1 with value: 0.9999908957605316.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 09:00:40,790] Trial 5 finished with value: 0.9999970594216133 and parameters: {'colsample_bytree': 0.9431517436102087, 'gamma': 0.24303402525576612, 'learning_rate': 0.5846205738442246, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 3800, 'reg_alpha': 1.7507683600480208, 'reg_lambda': 0.2570648913816608, 'subsample': 0.37753991291535094}. Best is trial 5 with value: 0.9999970594216133.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 09:02:24,678] Trial 6 finished with value: 0.999996893946829 and parameters: {'colsample_bytree': 0.424574198830982, 'gamma': 2.240985796864903, 'learning_rate': 0.07677039659175117, 'max_depth': 13, 'min_child_weight': 2, 'n_estimators': 4100, 'reg_alpha': 1.633988119468218, 'reg_lambda': 0.01582918381493703, 'subsample': 0.6370705014204265}. Best is trial 5 with value: 0.9999970594216133.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 09:03:07,846] Trial 7 finished with value: 0.9999917540859111 and parameters: {'colsample_bytree': 0.6634215013994202, 'gamma': 7.9725034982902345, 'learning_rate': 0.27230721432181304, 'max_depth': 10, 'min_child_weight': 5, 'n_estimators': 6200, 'reg_alpha': 0.49925746469520393, 'reg_lambda': 0.6155154946655141, 'subsample': 0.5889342330112841}. Best is trial 5 with value: 0.9999970594216133.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 09:03:26,037] Trial 8 finished with value: 0.9962148040541503 and parameters: {'colsample_bytree': 0.6950892585814746, 'gamma': 3.702056252890046, 'learning_rate': 0.12507587771806591, 'max_depth': 4, 'min_child_weight': 4, 'n_estimators': 3300, 'reg_alpha': 0.5160757282037794, 'reg_lambda': 0.8969253111252075, 'subsample': 0.9018429818648586}. Best is trial 5 with value: 0.9999970594216133.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:25: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\1708608048.py:27: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-21 09:04:05,785] Trial 9 finished with value: -266582131124.36734 and parameters: {'colsample_bytree': 0.2390340884726062, 'gamma': 4.354758329146864, 'learning_rate': 0.8970663477930209, 'max_depth': 10, 'min_child_weight': 5, 'n_estimators': 1800, 'reg_alpha': 0.9237812117561115, 'reg_lambda': 0.41567457491848026, 'subsample': 0.30056343349145725}. Best is trial 5 with value: 0.9999970594216133.\n"
     ]
    }
   ],
   "source": [
    "# tolerence = 1,2,3,4,5,6,7,8,9,10 .....\n",
    "n_trials = 10\n",
    "tolerence = 10 # 1 速度, 3 平衡, 7 準確\n",
    "if tolerence > 1:\n",
    "    n_trials = int(n_trials/tolerence**(2/5))\n",
    "    if n_trials == 0:\n",
    "        n_trials = 1\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\", pruner=SuccessiveHalvingPruner(reduction_factor=tolerence)\n",
    "    )\n",
    "else:\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\"\n",
    "    )    \n",
    "study.optimize(objective, n_trials=n_trials,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 16:41:08,104] A new study created in memory with name: no-name-f3d034cc-5d59-432e-bcba-56abb032dc16\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\3674623702.py:7: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\3674623702.py:9: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[I 2024-06-19 16:41:44,177] Trial 0 finished with value: 0.9999982904523037 and parameters: {'colsample_bytree': 0.4571502946746342, 'gamma': 1.6200436964351117, 'learning_rate': 0.39821741104297426, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 5200, 'reg_alpha': 0.7415140738282958, 'reg_lambda': 0.12049155386041477, 'subsample': 0.4892590913398624}. Best is trial 0 with value: 0.9999982904523037.\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\3674623702.py:7: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
      "C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\3674623702.py:9: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
      "[W 2024-06-19 16:44:20,756] Trial 1 failed with parameters: {'colsample_bytree': 0.38153744303272197, 'gamma': 7.616401262658536, 'learning_rate': 0.6141717855967389, 'max_depth': 14, 'min_child_weight': 3, 'n_estimators': 2400, 'reg_alpha': 1.6488368756854141, 'reg_lambda': 0.18911604930599268, 'subsample': 0.3012570457580755} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\foresight_User\\AppData\\Local\\Temp\\ipykernel_2500\\3674623702.py\", line 17, in objective\n",
      "    clf.fit(X_valid, y_valid)\n",
      "  File \"c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 730, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 1090, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 730, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 2051, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2024-06-19 16:44:20,758] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 25\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n\u001b[0;32m     22\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m     23\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m, pruner\u001b[38;5;241m=\u001b[39mThresholdPruner(lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,interval_steps \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 25\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[69], line 17\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      3\u001b[0m auto_tuning_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1.0\u001b[39m),\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m9.0\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: random_state\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     15\u001b[0m clf \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mauto_tuning_params)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m score \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mscore(X_valid, y_valid)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1090\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1079\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m (\n\u001b[0;32m   1082\u001b[0m     model,\n\u001b[0;32m   1083\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1089\u001b[0m )\n\u001b[1;32m-> 1090\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\xgboost\\core.py:2051\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2050\u001b[0m     _check_call(\n\u001b[1;32m-> 2051\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2052\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2054\u001b[0m     )\n\u001b[0;32m   2055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2056\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    auto_tuning_params = {\n",
    "        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 0.0, 9.0),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-6, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 18, 1),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 5),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 8000, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 2.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        'random_state': random_state\n",
    "    }\n",
    "    clf = xgb.XGBRegressor(**auto_tuning_params)\n",
    "\n",
    "    clf.fit(X_valid, y_valid)\n",
    "    score = clf.score(X_valid, y_valid)\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\", pruner=PercentilePruner(percentile=0.25,interval_steps =10)\n",
    ")\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
