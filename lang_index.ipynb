{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.evaluation import load_evaluator\n",
    "# from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# embedding_model = \"thenlper/gte-large-zh\"\n",
    "# gpu = 0\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=embedding_model)#, model_kwargs={\"device\": f\"cuda:{gpu}\"\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# <EmbeddingDistance.COSINE: 'cosine'>,\n",
    "#  <EmbeddingDistance.EUCLIDEAN: 'euclidean'>,\n",
    "#  <EmbeddingDistance.MANHATTAN: 'manhattan'>,\n",
    "#  <EmbeddingDistance.CHEBYSHEV: 'chebyshev'>,\n",
    "#  <EmbeddingDistance.HAMMING: 'hamming'>\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# evaluator = load_evaluator(\"embedding_distance\", embeddings=embeddings,distance_metric=\"cosine\")#, distance_metric=EmbeddingDistance.EUCLIDEAN\n",
    "\n",
    "# a1 = evaluator.evaluate_strings(prediction=\"ç¤¾åœ˜èˆ‡æ³•äººæœ‰ä»€éº¼ä¸åŒ\", reference=\"ç¤¾åœ˜èˆ‡æ³•äººå·®ç•°æœ‰å“ªäº›\")\n",
    "# a2 = evaluator.evaluate_strings(prediction=\"ç¤¾åœ˜èˆ‡æ³•äººæœ‰ä»€éº¼ä¸åŒ\", reference=\"ç¤¾åœ˜èˆ‡æ³•äººç›¸åŒé»\")\n",
    "# a3 = evaluator.evaluate_strings(prediction=\"ç¤¾åœ˜èˆ‡æ³•äººæœ‰ä»€éº¼ä¸åŒ\", reference=\"ç¤¾åœ˜èˆ‡å…¬å¸ç›¸åŒé»\")\n",
    "\n",
    "# print(a1)\n",
    "# print(a2)\n",
    "# print(a3)\n",
    "# \"\"\"\n",
    "# Note: This returns a distance score, meaning that the lower the number, \n",
    "# the more similar the prediction is to the reference, according to their embedded representation.\n",
    "# \"\"\"\n",
    "# # print(_euclidean_relevance_score_fn(a1['score']))\n",
    "# # print(_euclidean_relevance_score_fn(a2['score']))\n",
    "# # print(_euclidean_relevance_score_fn(a3['score']))\n",
    "\n",
    "# from langchain.evaluation import load_evaluator\n",
    "\n",
    "# evaluator = load_evaluator(\"pairwise_embedding_distance\", embeddings=embeddings,distance_metric=\"cosine\")\n",
    "# evaluator.evaluate_string_pairs(\n",
    "#     prediction=\"Seattle is hot in June\", prediction_b=\"Seattle is cool in June.\"\n",
    "# )\n",
    "# evaluator.evaluate_string_pairs(\n",
    "#     prediction=\"Seattle is warm in June\", prediction_b=\"Seattle is cool in June.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 55.68476903152963, 'char_order': 6, 'word_order': 0, 'beta': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe chrF(++) score can be any value between 0.0 and 100.0, inclusive.\\nåˆ†æ•¸è¶Šé«˜ï¼Œè¡¨ç¤ºè³ªé‡è¶Šå¥½ã€‚\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Senetence Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ChrF and ChrF++ are two MT evaluation metrics. \n",
    "They both use the F-score statistic for character n-gram matches, \n",
    "and ChrF++ adds word n-grams as well which correlates more strongly with direct assessment. \n",
    "We use the implementation that is already present in sacrebleu. \n",
    "The implementation here is slightly different from sacrebleu in terms of the required input format. \n",
    "The length of the references and hypotheses lists need to be the same, \n",
    "so you may need to transpose your references compared to sacrebleu's required input format. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ChrF and ChrF++ are two MT evaluation metrics that use the F-score statistic for character n-gram matches.\n",
    "ChrF++ additionally includes word n-grams, which correlate more strongly with direct assessment. \n",
    "We use the implementation that is already present in sacrebleu.\n",
    "While this metric is included in sacreBLEU, \n",
    "the implementation here is slightly different from sacreBLEU in terms of the required input format. \n",
    "Here, the length of the references and hypotheses lists need to be the same, \n",
    "so you may need to transpose your references compared to sacrebleu's required input format. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "char_order (int): Character n-gram order. Defaults to 6.\n",
    "word_order (int): Word n-gram order. If equals to 2, the metric is referred to as chrF++. Defaults to 0.\n",
    "beta (int): Determine the importance of recall w.r.t precision. Defaults to 2.\n",
    "lowercase (bool): If True, enables case-insensitivity. Defaults to False.\n",
    "whitespace (bool): If True, include whitespaces when extracting character n-grams. Defaults to False.\n",
    "eps_smoothing (bool): If True, applies epsilon smoothing similar to reference chrF++.py, NLTK, and Moses implementations. If False, takes into account effective match order similar to sacreBLEU < 2.0.0. Defaults to False.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Limitation\n",
    "According to PopoviÄ‡ 2017, \n",
    "chrF+ (where word_order=1) and chrF++ (where word_order=2) \n",
    "produce scores that correlate better with human judgements than chrF (where word_order=0) does.\n",
    "\"\"\"\n",
    "prediction = [\"The relationship between cats and dogs is not exactly friendly.\", \" å¥½çš„æ›¸åŸ is just a genteel black hole that knows how to read.\"]\n",
    "reference = [\"è²“å’Œç‹—çš„relationshipä¸æ€éº¼friendly\", \"A good bookshop is just a genteel Black Hole that knows how to read.\"]\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "results = chrf.compute(predictions=prediction, references=reference,char_order=6,word_order=0,beta=1,lowercase=False,whitespace=False,eps_smoothing=False)\n",
    "print(results)\n",
    "\n",
    "\"\"\"\n",
    "The chrF(++) score can be any value between 0.0 and 100.0, inclusive.\n",
    "åˆ†æ•¸è¶Šé«˜ï¼Œè¡¨ç¤ºè³ªé‡è¶Šå¥½ã€‚\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## é€™ä¸èƒ½ä½¿ç”¨toxicity\n",
    "# male_model_completions = [\n",
    "# 'was working so hard at an enterprise that he needed his own quarters',\n",
    "# 'needs the answer',\n",
    "# 'the pregnancy and the womanâ€™s condition.'\n",
    "# ]\n",
    "\n",
    "# female_model_completions = [\n",
    "# 'got up after 3 and gave him a few \"fucks\"',\n",
    "# 'usually doesnâ€™t have any money',\n",
    "# 'the course and it would be a great opportunity to meet with patients during her time at this hospital.'\n",
    "# ]\n",
    "# toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
    "# male_results = toxicity.compute(predictions=male_model_completions)\n",
    "\n",
    "# female_results = toxicity.compute(predictions=female_model_completions)\n",
    "\n",
    "# print(male_results)\n",
    "# print(female_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [0.68645840883255], 'recall': [0.6680428981781006], 'f1': [0.6771254539489746], 'hashcode': 'bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.41.2)'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BERTScore leverages the pre-trained contextual embeddings from BERT and matches words \n",
    "in candidate and reference sentences by cosine similarity. \n",
    "It has been shown to correlate with human judgment on sentence-level and system-level evaluation. \n",
    "Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful \n",
    "for evaluating different language generation tasks. \n",
    "\"\"\"\n",
    "\n",
    "# ä¸ç”¨åšsegmentation\n",
    "sentence1 = [\"ä¸€éš»è²“ç¨…åœ¨mattressä¸Šï¼Œcute cat\"]\n",
    "sentence2 = [\"é‚£æœ‰ cat ç¡åœ¨å¢Šå­ä¸Šï¼Œå¯æ€•çš„è²“!!!\"]\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "result = bertscore.compute(predictions=sentence1, references=sentence2, model_type = \"bert-base-multilingual-cased\",device=None,nthreads=4,rescale_with_baseline=False,) # (bertscore lang=\"en\" \"zh\" \"jp\") #æœ‰ä¸­è‹±å¤¾é›œä½¿ç”¨jp å‰‡æ˜¯bert-base-multilingual-cased_L9_no-idf_version=0.3.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1489: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ğŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc411fcf15fa4d5ea072fc73f04dba19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccc177822234ea0b9ead6008a6046ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': [0.436601, 0.8199912]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Senetence Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "FrugalScore is a reference-based metric for NLG models evaluation. \n",
    "It is based on a distillation approach that allows to learn a fixed,\n",
    "low cost version of any expensive NLG metric, while retaining most of its original performance.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "FrugalScore is a reference-based metric for Natural Language Generation (NLG) model evaluation. \n",
    "It is based on a distillation approach that allows to learn a fixed, \n",
    "low cost version of any expensive NLG metric, while retaining most of its original performance.\n",
    "The FrugalScore models are obtained by continuing the pretraining of small models on a synthetic \n",
    "dataset constructed using summarization, backtranslation and denoising models. \n",
    "During the training, the small models learn the internal mapping of the expensive metric, \n",
    "including any similarity function.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Limitation and Bias\n",
    "FrugalScore is based on BertScore and MoverScore, \n",
    "and the models used are based on the original models used for these scores.\n",
    "\"\"\"\n",
    "\n",
    "prediction = [\"The relationship between cats and dogs is not exactly friendly.\", \" å¥½çš„æ›¸åŸ is just a genteel black hole that knows how to read.\"]\n",
    "reference = [\"è²“å’Œç‹—çš„relationshipä¸æ€éº¼friendly\", \"A good bookshop is just a genteel Black Hole that knows how to read.\"]\n",
    "frugalscore = evaluate.load(\"frugalscore\") #evaluate.load(\"frugalscore\", \"moussaKam/frugalscore_medium_bert-base_mover-score\")\n",
    "results = frugalscore.compute(predictions=prediction, references=reference, batch_size=16, max_length=64,device=None)\n",
    "print(results)\n",
    "# åˆ†æ•¸è¶Šé«˜è¶Šå¥½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n",
      "Tokenizing text...\n",
      "Loading tokenizer\n",
      "Loading model\n",
      "Featurizing tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40aae598a4d44c3b8e12d8e84c1f66e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "Featurizing tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaadfa1c389d4d9a9d12e46ff7753eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 25\n",
      "performing clustering in lower dimension = 1\n",
      "kmeans time: 0.33 s\n",
      "total discretization time: 0.43 seconds\n",
      "namespace(p_hist=array([1., 0.]), q_hist=array([0.5, 0.5]), divergence_curve=array([[1.00000000e+00, 0.00000000e+00],\n",
      "       [1.00000000e+00, 3.12501563e-02],\n",
      "       [9.95665182e-01, 3.83262005e-02],\n",
      "       [9.82728863e-01, 4.66296573e-02],\n",
      "       [9.61393611e-01, 5.63137024e-02],\n",
      "       [9.31995061e-01, 6.75437565e-02],\n",
      "       [8.94998923e-01, 8.04979556e-02],\n",
      "       [8.50996750e-01, 9.53676224e-02],\n",
      "       [8.00700392e-01, 1.12357737e-01],\n",
      "       [7.44935073e-01, 1.31687407e-01],\n",
      "       [6.84630987e-01, 1.53590342e-01],\n",
      "       [6.20813269e-01, 1.78315319e-01],\n",
      "       [5.54590183e-01, 2.06126657e-01],\n",
      "       [4.87139290e-01, 2.37304688e-01],\n",
      "       [4.19691287e-01, 2.72146226e-01],\n",
      "       [3.53511139e-01, 3.10965041e-01],\n",
      "       [2.89875920e-01, 3.54092326e-01],\n",
      "       [2.30048606e-01, 4.01877170e-01],\n",
      "       [1.75246688e-01, 4.54687031e-01],\n",
      "       [1.26603877e-01, 5.12908203e-01],\n",
      "       [8.51221585e-02, 5.76946289e-01],\n",
      "       [5.16094712e-02, 6.47226672e-01],\n",
      "       [2.65940385e-02, 7.24194986e-01],\n",
      "       [1.01958572e-02, 8.08317586e-01],\n",
      "       [1.90200570e-03, 9.00082020e-01],\n",
      "       [5.65684718e-15, 9.99997500e-01],\n",
      "       [0.00000000e+00, 1.00000000e+00]]), mauve=0.27811372536724027, frontier_integral=0.3068528194400547, num_buckets=2)\n"
     ]
    }
   ],
   "source": [
    "# pip install mauve-text\n",
    "\"\"\"\n",
    "\n",
    "Senetence Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "MAUVE is a measure of the gap between neural text and human text. \n",
    "It is computed using the Kullbackâ€“Leibler (KL) divergences between the two distributions of text in a quantized embedding \n",
    "space of a large language model. \n",
    "MAUVE can identify differences in quality arising from model sizes and decoding algorithms.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "The metric takes two lists of strings of tokens separated by spaces: \n",
    "one representing predictions (i.e. the text generated by the model) \n",
    " the second representing references (a reference text for each prediction)\n",
    "\n",
    "\n",
    "mauve: MAUVE score, which ranges between 0 and 1. Larger values indicate that P and Q are closer.\n",
    "\n",
    "The original MAUVE paper reported values ranging from 0.88 to 0.94 for \n",
    "open-ended text generation using a text completion task in the web text domain. \n",
    "The authors found that bigger models resulted in higher MAUVE scores and that MAUVE is correlated with human judgments.\n",
    "\n",
    "\n",
    "frontier_integral: Frontier Integral, which ranges between 0 and 1. Smaller values indicate that P and Q are closer.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "mauve = evaluate.load('mauve')\n",
    "predictions = [\"The relationship between cats and dogs is not exactly friendly.\", \" å¥½çš„æ›¸åŸ is just a genteel black hole that knows how to read.\"]\n",
    "references = [\"è²“å’Œç‹—çš„relationshipä¸æ€éº¼friendly\", \"A good bookshop is just a genteel Black Hole that knows how to read.\"]\n",
    "mauve_results = mauve.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(mauve_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis metric outputs a dictionary with the perplexity scores for the text input in the list, \\nand the average perplexity. \\nIf one of the input texts is longer than the max input length of the model, \\nthen it is truncated to the max length for the perplexity computation.\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity ä¸å¯ä½¿ç”¨ï¼Œä¸»è¦è©•ä¼°è¨“ç·´æ¨¡å‹å¾Œçš„å¥½å£\n",
    "\"\"\"\n",
    "This metric outputs a dictionary with the perplexity scores for the text input in the list, \n",
    "and the average perplexity. \n",
    "If one of the input texts is longer than the max input length of the model, \n",
    "then it is truncated to the max length for the perplexity computation.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.3333333333333333, 'rouge2': 0.0, 'rougeL': 0.3333333333333333, 'rougeLsum': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Word Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, \n",
    "is a set of metrics and a software package used for evaluating automatic summarization \n",
    "and machine translation software in natural language processing. \n",
    "The metrics compare an automatically produced summary or translation \n",
    "against a reference or a set of references (human-produced) summary or translation.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"rouge1\": unigram (1-gram) based scoring\n",
    "\"rouge2\": bigram (2-gram) based scoring\n",
    "\"rougeL\": Longest common subsequence based scoring.\n",
    "\"rougeLSum\": splits text using \"\\n\"\n",
    "\n",
    "\"\"\"\n",
    "# è¦åšsegmentation\n",
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=['ä½ å¥½å•Š', 'general kenobi'],\n",
    "                         references=['hello ä½ å¥½å•Š', 'kenobiå°‡è»'])#, tokenizer=lambda x: x.split()\n",
    "print(results)\n",
    "# UNIGEN ç”¨ 'rougeL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.3333333333333333, 'precisions': [0.3333333333333333], 'brevity_penalty': 1.0, 'length_ratio': 1.5, 'translation_length': 3, 'reference_length': 2}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Word Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. \n",
    "Quality is considered to be the correspondence between a machine's output and that of a human: \n",
    "\"the closer a machine translation is to a professional human translation, the better it is\" â€“ this is the central idea behind BLEU. \n",
    "BLEU was one of the first metrics to claim a high correlation with human judgements of quality, \n",
    "and remains one of the most popular automated and inexpensive metrics.\n",
    "\n",
    "Scores are calculated for individual translated segmentsâ€”generally sentencesâ€”by comparing them with a set of good quality reference translations. \n",
    "Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. \n",
    "Neither intelligibility nor grammatical correctness are not taken into account.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "max_order (int): Maximum n-gram order to use when computing BLEU score. Defaults to 4.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "BLEU's output is always a number between 0 and 1. \n",
    "This value indicates how similar the candidate text is to the reference texts,\n",
    "with values closer to 1 representing more similar texts. \n",
    "Few human translations will attain a score of 1, \n",
    "since this would indicate that the candidate is identical to one of the reference translations. \n",
    "For this reason, it is not necessary to attain a score of 1. \n",
    "Because there are more opportunities to match, adding additional reference translations will increase the BLEU score.\n",
    "\"\"\"\n",
    "predictions = ['ä½ å¥½å•Š', 'general kenobi']\n",
    "references = [['hello there', 'kenobiå°‡è»'],\n",
    "                [\"ä½ å¥½å•Š\", \"general è‚¯è«¾æ¯”\"]]\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references,  max_order=1)#, ç”¨ 1 grame\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 65.39657130693087, 'counts': [13, 12, 11, 10], 'totals': [16, 14, 12, 10], 'precisions': [81.25, 85.71428571428571, 91.66666666666667, 100.0], 'bp': 0.7316156289466418, 'sys_len': 16, 'ref_len': 21}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Word Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "SacreBLEU provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. \n",
    "Inspired by Rico Sennrich's `multi-bleu-detok.perl`, \n",
    "it produces the official WMT scores but works with plain text. \n",
    "It also knows all the standard test sets and handles downloading, processing, and tokenization for you.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "SacreBLEU provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. \n",
    "Inspired by Rico Sennrich's multi-bleu-detok.perl, \n",
    "it produces the official Workshop on Machine Translation (WMT) \n",
    "scores but works with plain text. \n",
    "It also knows all the standard test sets and handles downloading, processing, and tokenization.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tokenize (str): \n",
    "Tokenization method to use for BLEU. \n",
    "If not provided, defaults to 'zh' for Chinese, 'ja-mecab' for Japanese and '13a' (mteval) otherwise. Possible values are:\n",
    "   'none': No tokenization.\n",
    "   'zh': Chinese tokenization.\n",
    "   '13a': mimics the mteval-v13a script from Moses.\n",
    "   'intl': International tokenization, mimics the mteval-v14 script from Moses\n",
    "   'char': Language-agnostic character-level tokenization.\n",
    "   'ja-mecab': Japanese tokenization. Uses the MeCab tokenizer.\n",
    "\n",
    "   intl æ–¹æ³•é©åˆï¼šå¤§å¤šæ•¸å°æ­èªç³»èªè¨€ï¼ˆå¦‚è‹±èªã€æ³•èªã€å¾·èªç­‰ï¼‰ï¼Œé€™äº›èªè¨€çš„å–®è©é‚Šç•Œæ˜ç¢ºï¼Œä½¿ç”¨åœ‹éš›æ¨™æº–çš„å–®è©ç´šåˆ†è©æ›´èƒ½åæ˜ ç¿»è­¯è³ªé‡ã€‚\n",
    "   char æ–¹æ³•é©åˆï¼šæ¼¢è—èªç³»èªè¨€ï¼ˆå¦‚ä¸­æ–‡ã€æ—¥èªã€éŸ“èªç­‰ï¼‰ï¼Œé€™äº›èªè¨€å–®è©é‚Šç•Œä¸æ˜é¡¯ï¼Œå­—ç¬¦ç´šåˆ†è©æ›´èƒ½æº–ç¢ºè©•ä¼°ç¿»è­¯è³ªé‡ã€‚\n",
    "   !! é€™éƒ¨åˆ†éœ€è¦å¥½å¥½review\n",
    "\n",
    "   intl (International tokenization)ï¼š\n",
    "    åŸºæ–¼åœ‹éš›æ¨™æº–çš„åˆ†è©æ–¹æ³•ï¼Œæ¨¡ä»¿ Moses mteval-v14 è…³æœ¬ã€‚\n",
    "    ä¸»è¦é‡å°å–®è©ç´šåˆ¥çš„åŒ¹é…ï¼Œè€ƒæ…®å–®è©é‚Šç•Œã€æ¨™é»ç¬¦è™Ÿå’Œå…¶ä»–èªè¨€ç‰¹æ€§ã€‚\n",
    "    é©åˆè™•ç†å¤§å¤šæ•¸èªè¨€ï¼Œç‰¹åˆ¥æ˜¯é‚£äº›è©å½™é‚Šç•Œæ¸…æ™°çš„èªè¨€ï¼Œå¦‚è‹±èªã€æ³•èªç­‰ã€‚\n",
    "   char (Character-level tokenization)ï¼š\n",
    "    åŸºæ–¼å­—ç¬¦ç´šåˆ¥çš„åˆ†è©æ–¹æ³•ï¼Œä¸ä¾è³´æ–¼å–®è©é‚Šç•Œã€‚\n",
    "    å°‡æ–‡æœ¬åˆ†è§£ç‚ºå–®å€‹å­—ç¬¦é€²è¡ŒåŒ¹é…ã€‚\n",
    "    æ›´åŠ èªè¨€ç„¡é—œï¼Œé©åˆè™•ç†é‚£äº›å–®è©é‚Šç•Œä¸æ˜é¡¯æˆ–å½¢æ…‹è¤‡é›œçš„èªè¨€ï¼Œå¦‚ä¸­æ–‡ã€æ—¥èªç­‰ã€‚\n",
    "\n",
    "lowercase (bool): \n",
    "   If True, lowercases the input, enabling case-insensitivity. Defaults to False.\n",
    "force (bool): \n",
    "   If True, insists that your tokenized input is actually detokenized. Defaults to False.\n",
    "use_effective_order (bool): \n",
    "   If True, stops including n-gram orders for which precision is 0. \n",
    "   This should be True, if sentence-level BLEU will be computed. Defaults to False.\n",
    "\n",
    "   use_effective_order åƒæ•¸æ±ºå®šäº†åœ¨è¨ˆç®— BLEU åˆ†æ•¸æ™‚æ˜¯å¦å¿½ç•¥ç²¾ç¢ºåº¦ç‚º 0 çš„ n-gram é †åºã€‚\n",
    "   è¨­ç‚º True æ™‚ï¼Œæœ‰åŠ©æ–¼æé«˜å¥å­ç´šåˆ¥ BLEU åˆ†æ•¸çš„æº–ç¢ºæ€§ï¼Œç‰¹åˆ¥é©ç”¨æ–¼çŸ­å¥å­çš„è©•ä¼°ã€‚\n",
    "   è¨­ç‚º False æ™‚ï¼Œå‰‡é©åˆè©•ä¼°é•·æ–‡æœ¬çš„æ•´é«”ç¿»è­¯è³ªé‡\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Limitation and Bias\n",
    "Because what this metric calculates is BLEU scores, \n",
    "it has the same limitations as that metric, except that sacreBLEU is more easily reproducible.\n",
    "\n",
    "\"\"\"\n",
    "predictions = ['ä½ å¥½å•Š', 'general kenobi']\n",
    "references = [['hello there', 'kenobiå°‡è»'],\n",
    "                [\"ä½ å¥½å•Š\", \"general kenobi\"]]\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "results = sacrebleu.compute(predictions=predictions, \n",
    "                            references=references,tokenize=\"char\",lowercase=True,use_effective_order=False) # è¦èª¿æ•´tokenizeï¼Œç‚ºä½•ç”¨char\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sacremoses\n",
    "sari = evaluate.load(\"sari\")\n",
    "sources=[\"'kenobiå°‡è»ï¼Œæ‚¨å¥½!\"]\n",
    "predictions = ['ä½ å¥½å•Š! general kenobi']\n",
    "references = [['hello there! kenobiå°‡è»',\n",
    "                \"ä½ å¥½å•Š! general è‚¯è«¾æ¯”\"]]\n",
    "sari_score = sari.compute(sources=sources, predictions=predictions, references=references)\n",
    "\"\"\"\n",
    "\n",
    "SARI (system output against references and against the input sentence) is \n",
    "a metric used for evaluating automatic text simplification systems.\n",
    "The metric compares the predicted simplified sentences against the reference and the source sentences. \n",
    "It explicitly measures the goodness of words that are added, deleted and kept by the system.\n",
    "\n",
    "The range of values for the SARI score is between 0 and 100 -- \n",
    "the higher the value, the better the performance of the model being evaluated,\n",
    "with a SARI of 100 being a perfect score.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 72.0, 'num_edits': 9, 'ref_length': 12.5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Sentence Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The metric can take on any value 0 and above. 0 is a perfect score, \n",
    "meaning the predictions exactly match the references and no edits were necessary.\n",
    " Higher scores are worse. \n",
    " Scores above 100 mean that the cumulative number of edits, num_edits, \n",
    " is higher than the cumulative length of the references, ref_length.\n",
    "\n",
    "\"\"\"\n",
    "predictions = [\"é€™å¥è©±å°å—??\",\n",
    "               \"é‚£é€™å¥è©±å‘¢?\",\n",
    "          \"é€™ç¬‘è©±å¾ˆé›£ç¬‘\"     ]\n",
    "references = [[\"é€™å¥è©±å°å—\", \"é€™å¥è©±å°å—!?!\"],\n",
    "               [\"é€™å¥è©±æ€éº¼æ¨£?\", \"wHaT aBoUt ThIs SeNtEnCe?\"],\n",
    "               [\"ä½ çš„ç¬‘è©±...\", \"...TERrible\"]]\n",
    "\n",
    "ter = evaluate.load(\"ter\")\n",
    "results = ter.compute(predictions=predictions,\n",
    "                        references=references,\n",
    "                         case_sensitive=True,support_zh_ja_chars=True,ignore_punct=True,normalized=True)\n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3333333333333333\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Word Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Word error rate (WER) is a common metric of the performance of an automatic speech recognition (ASR) system.\n",
    "\n",
    "The general difficulty of measuring the performance of ASR systems lies in the fact that the recognized word sequence can have a different \n",
    "length from the reference word sequence (supposedly the correct one). The WER is derived from the Levenshtein distance, working at the word level.\n",
    "\n",
    "This problem is solved by first aligning the recognized word sequence with the reference (spoken) \n",
    "word sequence using dynamic string alignment. \n",
    "Examination of this issue is seen through a theory called the power law that states the correlation between \n",
    "perplexity and word error rate (see this article for further information).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The lower the value, the better the performance of the ASR system, with a WER of 0 being a perfect score.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# !pip install jiwer\n",
    "wer = evaluate.load(\"wer\")\n",
    "predictions = ['ä½ å¥½å•Š', 'general kenobi']\n",
    "references1 = ['hello there', 'kenobiå°‡è»']\n",
    "references2 = [\"ä½ å¥½å•Š\", \"general è‚¯è«¾æ¯”\"]\n",
    "wer_score1 = wer.compute(predictions=predictions, references=references1)\n",
    "wer_score2 = wer.compute(predictions=predictions, references=references2)\n",
    "\n",
    "print(wer_score1)\n",
    "print(wer_score2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wer': 1.3333333333333333, 'cer': 1.105263157894737}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Word Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "https://huggingface.co/datasets/google/xtreme_s\n",
    "\n",
    "wer: \n",
    "Word error rate (WER) is a common metric of the performance of an automatic speech recognition system. \n",
    "The lower the value, the better the performance of the ASR system, with a WER of 0 being a perfect score (see WER score for more information). \n",
    "It is returned for the mls, fleurs-asr, voxpopuli and babel subsets of the benchmark.\n",
    "\n",
    "cer: \n",
    "Character error rate (CER) is similar to WER, but operates on character instead of word. \n",
    "The lower the CER value, the better the performance of the ASR system, \n",
    "with a CER of 0 being a perfect score (see CER score for more information). \n",
    "It is returned for the mls, fleurs-asr, voxpopuli and babel subsets of the benchmark.\n",
    "\n",
    "å¤šèªè¨€èªéŸ³åŸºæº– (Multilingual Speech Benchmark, MLS) \n",
    "æ˜¯ä¸€å€‹ç”¨æ–¼è©•ä¼°å¤šèªè¨€è‡ªå‹•èªéŸ³è­˜åˆ¥ (ASR) æ¨¡å‹æ€§èƒ½çš„æ•¸æ“šé›†ã€‚\n",
    "MLS æ•¸æ“šé›†åŒ…å«å¤šç¨®èªè¨€çš„èªéŸ³æ•¸æ“šï¼Œæ—¨åœ¨ä¿ƒé€²å¤šèªè¨€èªéŸ³æŠ€è¡“çš„ç™¼å±•ã€‚è©•ä¼°æ¨¡å‹æ€§èƒ½æ™‚ï¼Œ\n",
    "å¸¸ç”¨çš„æŒ‡æ¨™åŒ…æ‹¬å­—éŒ¯èª¤ç‡ (WER) å’ŒèªéŸ³ç‰‡æ®µçš„æº–ç¢ºæ€§ã€‚\n",
    "\n",
    "\n",
    "Fleurs-ASRï¼š\n",
    "é€™ä»£è¡¨çš„æ˜¯ Fleurs çš„è‡ªå‹•èªéŸ³è­˜åˆ¥ï¼ˆAutomatic Speech Recognition, ASRï¼‰ä»»å‹™ã€‚\n",
    "Fleurs æ˜¯ä¸€å€‹å¤šèªç¨®èªéŸ³æ•¸æ“šé›†ï¼Œ\n",
    "ç”¨æ–¼è¨“ç·´å’Œè©•ä¼°èªéŸ³è­˜åˆ¥ç³»çµ±ã€‚åœ¨é€™å€‹ä»»å‹™ä¸­ï¼Œæ¨¡å‹éœ€è¦å°‡èªéŸ³è½‰éŒ„ç‚ºç›¸æ‡‰çš„æ–‡æœ¬ï¼Œä¸¦åœ¨å¤šç¨®èªè¨€ä¸Šé€²è¡Œè©•ä¼°ã€‚\n",
    "\"\"\"\n",
    "\n",
    "xtreme_s_metric = evaluate.load('xtreme_s', 'mls')\n",
    "predictions = ['ä½ å¥½å•Š', 'general kenobi']\n",
    "references = ['hello there', 'kenobiå°‡è»']\n",
    "results = xtreme_s_metric.compute(predictions=predictions, references=references)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"ArithmeticError\n",
    "pip uninstall spacy\n",
    "pip uninstall srsly\n",
    "pip uninstall blis\n",
    "pip uninstall cymem\n",
    "pip uninstall preshed\n",
    "pip uninstall murmurhash\n",
    "\n",
    "pip install thinc==8.2.2\n",
    "pip install blis==0.7.8\n",
    "pip install cymem\n",
    "pip install preshed\n",
    "pip install murmurhash\n",
    "pip install spacy\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download ja_core_news_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "python -m spacy download es_core_news_sm\n",
    "python -m spacy download it_core_news_sm\n",
    "python -m spacy download ko_core_news_sm\n",
    "python -m spacy download ru_core_news_sm\n",
    "python -m spacy download fr_core_news_sm\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
