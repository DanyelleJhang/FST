{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.evaluation import load_evaluator\n",
    "# from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# embedding_model = \"thenlper/gte-large-zh\"\n",
    "# gpu = 0\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=embedding_model)#, model_kwargs={\"device\": f\"cuda:{gpu}\"\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# <EmbeddingDistance.COSINE: 'cosine'>,\n",
    "#  <EmbeddingDistance.EUCLIDEAN: 'euclidean'>,\n",
    "#  <EmbeddingDistance.MANHATTAN: 'manhattan'>,\n",
    "#  <EmbeddingDistance.CHEBYSHEV: 'chebyshev'>,\n",
    "#  <EmbeddingDistance.HAMMING: 'hamming'>\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# evaluator = load_evaluator(\"embedding_distance\", embeddings=embeddings,distance_metric=\"cosine\")#, distance_metric=EmbeddingDistance.EUCLIDEAN\n",
    "\n",
    "# a1 = evaluator.evaluate_strings(prediction=\"社團與法人有什麼不同\", reference=\"社團與法人差異有哪些\")\n",
    "# a2 = evaluator.evaluate_strings(prediction=\"社團與法人有什麼不同\", reference=\"社團與法人相同點\")\n",
    "# a3 = evaluator.evaluate_strings(prediction=\"社團與法人有什麼不同\", reference=\"社團與公司相同點\")\n",
    "\n",
    "# print(a1)\n",
    "# print(a2)\n",
    "# print(a3)\n",
    "# \"\"\"\n",
    "# Note: This returns a distance score, meaning that the lower the number, \n",
    "# the more similar the prediction is to the reference, according to their embedded representation.\n",
    "# \"\"\"\n",
    "# # print(_euclidean_relevance_score_fn(a1['score']))\n",
    "# # print(_euclidean_relevance_score_fn(a2['score']))\n",
    "# # print(_euclidean_relevance_score_fn(a3['score']))\n",
    "\n",
    "# from langchain.evaluation import load_evaluator\n",
    "\n",
    "# evaluator = load_evaluator(\"pairwise_embedding_distance\", embeddings=embeddings,distance_metric=\"cosine\")\n",
    "# evaluator.evaluate_string_pairs(\n",
    "#     prediction=\"Seattle is hot in June\", prediction_b=\"Seattle is cool in June.\"\n",
    "# )\n",
    "# evaluator.evaluate_string_pairs(\n",
    "#     prediction=\"Seattle is warm in June\", prediction_b=\"Seattle is cool in June.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 55.68476903152963, 'char_order': 6, 'word_order': 0, 'beta': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe chrF(++) score can be any value between 0.0 and 100.0, inclusive.\\n分數越高，表示質量越好。\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Senetence Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ChrF and ChrF++ are two MT evaluation metrics. \n",
    "They both use the F-score statistic for character n-gram matches, \n",
    "and ChrF++ adds word n-grams as well which correlates more strongly with direct assessment. \n",
    "We use the implementation that is already present in sacrebleu. \n",
    "The implementation here is slightly different from sacrebleu in terms of the required input format. \n",
    "The length of the references and hypotheses lists need to be the same, \n",
    "so you may need to transpose your references compared to sacrebleu's required input format. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ChrF and ChrF++ are two MT evaluation metrics that use the F-score statistic for character n-gram matches.\n",
    "ChrF++ additionally includes word n-grams, which correlate more strongly with direct assessment. \n",
    "We use the implementation that is already present in sacrebleu.\n",
    "While this metric is included in sacreBLEU, \n",
    "the implementation here is slightly different from sacreBLEU in terms of the required input format. \n",
    "Here, the length of the references and hypotheses lists need to be the same, \n",
    "so you may need to transpose your references compared to sacrebleu's required input format. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "char_order (int): Character n-gram order. Defaults to 6.\n",
    "word_order (int): Word n-gram order. If equals to 2, the metric is referred to as chrF++. Defaults to 0.\n",
    "beta (int): Determine the importance of recall w.r.t precision. Defaults to 2.\n",
    "lowercase (bool): If True, enables case-insensitivity. Defaults to False.\n",
    "whitespace (bool): If True, include whitespaces when extracting character n-grams. Defaults to False.\n",
    "eps_smoothing (bool): If True, applies epsilon smoothing similar to reference chrF++.py, NLTK, and Moses implementations. If False, takes into account effective match order similar to sacreBLEU < 2.0.0. Defaults to False.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Limitation\n",
    "According to Popović 2017, \n",
    "chrF+ (where word_order=1) and chrF++ (where word_order=2) \n",
    "produce scores that correlate better with human judgements than chrF (where word_order=0) does.\n",
    "\"\"\"\n",
    "prediction = [\"The relationship between cats and dogs is not exactly friendly.\", \" 好的書城 is just a genteel black hole that knows how to read.\"]\n",
    "reference = [\"貓和狗的relationship不怎麼friendly\", \"A good bookshop is just a genteel Black Hole that knows how to read.\"]\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "results = chrf.compute(predictions=prediction, references=reference,char_order=6,word_order=0,beta=1,lowercase=False,whitespace=False,eps_smoothing=False)\n",
    "print(results)\n",
    "\n",
    "\"\"\"\n",
    "The chrF(++) score can be any value between 0.0 and 100.0, inclusive.\n",
    "分數越高，表示質量越好。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## 這不能使用toxicity\n",
    "# male_model_completions = [\n",
    "# 'was working so hard at an enterprise that he needed his own quarters',\n",
    "# 'needs the answer',\n",
    "# 'the pregnancy and the woman’s condition.'\n",
    "# ]\n",
    "\n",
    "# female_model_completions = [\n",
    "# 'got up after 3 and gave him a few \"fucks\"',\n",
    "# 'usually doesn’t have any money',\n",
    "# 'the course and it would be a great opportunity to meet with patients during her time at this hospital.'\n",
    "# ]\n",
    "# toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
    "# male_results = toxicity.compute(predictions=male_model_completions)\n",
    "\n",
    "# female_results = toxicity.compute(predictions=female_model_completions)\n",
    "\n",
    "# print(male_results)\n",
    "# print(female_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [0.68645840883255], 'recall': [0.6680428981781006], 'f1': [0.6771254539489746], 'hashcode': 'bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.41.2)'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BERTScore leverages the pre-trained contextual embeddings from BERT and matches words \n",
    "in candidate and reference sentences by cosine similarity. \n",
    "It has been shown to correlate with human judgment on sentence-level and system-level evaluation. \n",
    "Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful \n",
    "for evaluating different language generation tasks. \n",
    "\"\"\"\n",
    "\n",
    "# 不用做segmentation\n",
    "sentence1 = [\"一隻貓稅在mattress上，cute cat\"]\n",
    "sentence2 = [\"那有 cat 睡在墊子上，可怕的貓!!!\"]\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "result = bertscore.compute(predictions=sentence1, references=sentence2, model_type = \"bert-base-multilingual-cased\",device=None,nthreads=4,rescale_with_baseline=False,) # (bertscore lang=\"en\" \"zh\" \"jp\") #有中英夾雜使用jp 則是bert-base-multilingual-cased_L9_no-idf_version=0.3.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foresight_User\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1489: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc411fcf15fa4d5ea072fc73f04dba19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccc177822234ea0b9ead6008a6046ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': [0.436601, 0.8199912]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Senetence Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "FrugalScore is a reference-based metric for NLG models evaluation. \n",
    "It is based on a distillation approach that allows to learn a fixed,\n",
    "low cost version of any expensive NLG metric, while retaining most of its original performance.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "FrugalScore is a reference-based metric for Natural Language Generation (NLG) model evaluation. \n",
    "It is based on a distillation approach that allows to learn a fixed, \n",
    "low cost version of any expensive NLG metric, while retaining most of its original performance.\n",
    "The FrugalScore models are obtained by continuing the pretraining of small models on a synthetic \n",
    "dataset constructed using summarization, backtranslation and denoising models. \n",
    "During the training, the small models learn the internal mapping of the expensive metric, \n",
    "including any similarity function.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Limitation and Bias\n",
    "FrugalScore is based on BertScore and MoverScore, \n",
    "and the models used are based on the original models used for these scores.\n",
    "\"\"\"\n",
    "\n",
    "prediction = [\"The relationship between cats and dogs is not exactly friendly.\", \" 好的書城 is just a genteel black hole that knows how to read.\"]\n",
    "reference = [\"貓和狗的relationship不怎麼friendly\", \"A good bookshop is just a genteel Black Hole that knows how to read.\"]\n",
    "frugalscore = evaluate.load(\"frugalscore\") #evaluate.load(\"frugalscore\", \"moussaKam/frugalscore_medium_bert-base_mover-score\")\n",
    "results = frugalscore.compute(predictions=prediction, references=reference, batch_size=16, max_length=64,device=None)\n",
    "print(results)\n",
    "# 分數越高越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n",
      "Tokenizing text...\n",
      "Loading tokenizer\n",
      "Loading model\n",
      "Featurizing tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40aae598a4d44c3b8e12d8e84c1f66e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "Featurizing tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaadfa1c389d4d9a9d12e46ff7753eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 25\n",
      "performing clustering in lower dimension = 1\n",
      "kmeans time: 0.33 s\n",
      "total discretization time: 0.43 seconds\n",
      "namespace(p_hist=array([1., 0.]), q_hist=array([0.5, 0.5]), divergence_curve=array([[1.00000000e+00, 0.00000000e+00],\n",
      "       [1.00000000e+00, 3.12501563e-02],\n",
      "       [9.95665182e-01, 3.83262005e-02],\n",
      "       [9.82728863e-01, 4.66296573e-02],\n",
      "       [9.61393611e-01, 5.63137024e-02],\n",
      "       [9.31995061e-01, 6.75437565e-02],\n",
      "       [8.94998923e-01, 8.04979556e-02],\n",
      "       [8.50996750e-01, 9.53676224e-02],\n",
      "       [8.00700392e-01, 1.12357737e-01],\n",
      "       [7.44935073e-01, 1.31687407e-01],\n",
      "       [6.84630987e-01, 1.53590342e-01],\n",
      "       [6.20813269e-01, 1.78315319e-01],\n",
      "       [5.54590183e-01, 2.06126657e-01],\n",
      "       [4.87139290e-01, 2.37304688e-01],\n",
      "       [4.19691287e-01, 2.72146226e-01],\n",
      "       [3.53511139e-01, 3.10965041e-01],\n",
      "       [2.89875920e-01, 3.54092326e-01],\n",
      "       [2.30048606e-01, 4.01877170e-01],\n",
      "       [1.75246688e-01, 4.54687031e-01],\n",
      "       [1.26603877e-01, 5.12908203e-01],\n",
      "       [8.51221585e-02, 5.76946289e-01],\n",
      "       [5.16094712e-02, 6.47226672e-01],\n",
      "       [2.65940385e-02, 7.24194986e-01],\n",
      "       [1.01958572e-02, 8.08317586e-01],\n",
      "       [1.90200570e-03, 9.00082020e-01],\n",
      "       [5.65684718e-15, 9.99997500e-01],\n",
      "       [0.00000000e+00, 1.00000000e+00]]), mauve=0.27811372536724027, frontier_integral=0.3068528194400547, num_buckets=2)\n"
     ]
    }
   ],
   "source": [
    "# pip install mauve-text\n",
    "\"\"\"\n",
    "\n",
    "Senetence Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "MAUVE is a measure of the gap between neural text and human text. \n",
    "It is computed using the Kullback–Leibler (KL) divergences between the two distributions of text in a quantized embedding \n",
    "space of a large language model. \n",
    "MAUVE can identify differences in quality arising from model sizes and decoding algorithms.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "The metric takes two lists of strings of tokens separated by spaces: \n",
    "one representing predictions (i.e. the text generated by the model) \n",
    " the second representing references (a reference text for each prediction)\n",
    "\n",
    "\n",
    "mauve: MAUVE score, which ranges between 0 and 1. Larger values indicate that P and Q are closer.\n",
    "\n",
    "The original MAUVE paper reported values ranging from 0.88 to 0.94 for \n",
    "open-ended text generation using a text completion task in the web text domain. \n",
    "The authors found that bigger models resulted in higher MAUVE scores and that MAUVE is correlated with human judgments.\n",
    "\n",
    "\n",
    "frontier_integral: Frontier Integral, which ranges between 0 and 1. Smaller values indicate that P and Q are closer.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "mauve = evaluate.load('mauve')\n",
    "predictions = [\"The relationship between cats and dogs is not exactly friendly.\", \" 好的書城 is just a genteel black hole that knows how to read.\"]\n",
    "references = [\"貓和狗的relationship不怎麼friendly\", \"A good bookshop is just a genteel Black Hole that knows how to read.\"]\n",
    "mauve_results = mauve.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(mauve_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis metric outputs a dictionary with the perplexity scores for the text input in the list, \\nand the average perplexity. \\nIf one of the input texts is longer than the max input length of the model, \\nthen it is truncated to the max length for the perplexity computation.\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity 不可使用，主要評估訓練模型後的好壞\n",
    "\"\"\"\n",
    "This metric outputs a dictionary with the perplexity scores for the text input in the list, \n",
    "and the average perplexity. \n",
    "If one of the input texts is longer than the max input length of the model, \n",
    "then it is truncated to the max length for the perplexity computation.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.3333333333333333, 'rouge2': 0.0, 'rougeL': 0.3333333333333333, 'rougeLsum': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Word Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, \n",
    "is a set of metrics and a software package used for evaluating automatic summarization \n",
    "and machine translation software in natural language processing. \n",
    "The metrics compare an automatically produced summary or translation \n",
    "against a reference or a set of references (human-produced) summary or translation.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"rouge1\": unigram (1-gram) based scoring\n",
    "\"rouge2\": bigram (2-gram) based scoring\n",
    "\"rougeL\": Longest common subsequence based scoring.\n",
    "\"rougeLSum\": splits text using \"\\n\"\n",
    "\n",
    "\"\"\"\n",
    "# 要做segmentation\n",
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=['你好啊', 'general kenobi'],\n",
    "                         references=['hello 你好啊', 'kenobi將軍'])#, tokenizer=lambda x: x.split()\n",
    "print(results)\n",
    "# UNIGEN 用 'rougeL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.3333333333333333, 'precisions': [0.3333333333333333], 'brevity_penalty': 1.0, 'length_ratio': 1.5, 'translation_length': 3, 'reference_length': 2}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Word Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. \n",
    "Quality is considered to be the correspondence between a machine's output and that of a human: \n",
    "\"the closer a machine translation is to a professional human translation, the better it is\" – this is the central idea behind BLEU. \n",
    "BLEU was one of the first metrics to claim a high correlation with human judgements of quality, \n",
    "and remains one of the most popular automated and inexpensive metrics.\n",
    "\n",
    "Scores are calculated for individual translated segments—generally sentences—by comparing them with a set of good quality reference translations. \n",
    "Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. \n",
    "Neither intelligibility nor grammatical correctness are not taken into account.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "max_order (int): Maximum n-gram order to use when computing BLEU score. Defaults to 4.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "BLEU's output is always a number between 0 and 1. \n",
    "This value indicates how similar the candidate text is to the reference texts,\n",
    "with values closer to 1 representing more similar texts. \n",
    "Few human translations will attain a score of 1, \n",
    "since this would indicate that the candidate is identical to one of the reference translations. \n",
    "For this reason, it is not necessary to attain a score of 1. \n",
    "Because there are more opportunities to match, adding additional reference translations will increase the BLEU score.\n",
    "\"\"\"\n",
    "predictions = ['你好啊', 'general kenobi']\n",
    "references = [['hello there', 'kenobi將軍'],\n",
    "                [\"你好啊\", \"general 肯諾比\"]]\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references,  max_order=1)#, 用 1 grame\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 65.39657130693087, 'counts': [13, 12, 11, 10], 'totals': [16, 14, 12, 10], 'precisions': [81.25, 85.71428571428571, 91.66666666666667, 100.0], 'bp': 0.7316156289466418, 'sys_len': 16, 'ref_len': 21}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Word Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "SacreBLEU provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. \n",
    "Inspired by Rico Sennrich's `multi-bleu-detok.perl`, \n",
    "it produces the official WMT scores but works with plain text. \n",
    "It also knows all the standard test sets and handles downloading, processing, and tokenization for you.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "SacreBLEU provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. \n",
    "Inspired by Rico Sennrich's multi-bleu-detok.perl, \n",
    "it produces the official Workshop on Machine Translation (WMT) \n",
    "scores but works with plain text. \n",
    "It also knows all the standard test sets and handles downloading, processing, and tokenization.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tokenize (str): \n",
    "Tokenization method to use for BLEU. \n",
    "If not provided, defaults to 'zh' for Chinese, 'ja-mecab' for Japanese and '13a' (mteval) otherwise. Possible values are:\n",
    "   'none': No tokenization.\n",
    "   'zh': Chinese tokenization.\n",
    "   '13a': mimics the mteval-v13a script from Moses.\n",
    "   'intl': International tokenization, mimics the mteval-v14 script from Moses\n",
    "   'char': Language-agnostic character-level tokenization.\n",
    "   'ja-mecab': Japanese tokenization. Uses the MeCab tokenizer.\n",
    "\n",
    "   intl 方法適合：大多數印歐語系語言（如英語、法語、德語等），這些語言的單詞邊界明確，使用國際標準的單詞級分詞更能反映翻譯質量。\n",
    "   char 方法適合：漢藏語系語言（如中文、日語、韓語等），這些語言單詞邊界不明顯，字符級分詞更能準確評估翻譯質量。\n",
    "   !! 這部分需要好好review\n",
    "\n",
    "   intl (International tokenization)：\n",
    "    基於國際標準的分詞方法，模仿 Moses mteval-v14 腳本。\n",
    "    主要針對單詞級別的匹配，考慮單詞邊界、標點符號和其他語言特性。\n",
    "    適合處理大多數語言，特別是那些詞彙邊界清晰的語言，如英語、法語等。\n",
    "   char (Character-level tokenization)：\n",
    "    基於字符級別的分詞方法，不依賴於單詞邊界。\n",
    "    將文本分解為單個字符進行匹配。\n",
    "    更加語言無關，適合處理那些單詞邊界不明顯或形態複雜的語言，如中文、日語等。\n",
    "\n",
    "lowercase (bool): \n",
    "   If True, lowercases the input, enabling case-insensitivity. Defaults to False.\n",
    "force (bool): \n",
    "   If True, insists that your tokenized input is actually detokenized. Defaults to False.\n",
    "use_effective_order (bool): \n",
    "   If True, stops including n-gram orders for which precision is 0. \n",
    "   This should be True, if sentence-level BLEU will be computed. Defaults to False.\n",
    "\n",
    "   use_effective_order 參數決定了在計算 BLEU 分數時是否忽略精確度為 0 的 n-gram 順序。\n",
    "   設為 True 時，有助於提高句子級別 BLEU 分數的準確性，特別適用於短句子的評估。\n",
    "   設為 False 時，則適合評估長文本的整體翻譯質量\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Limitation and Bias\n",
    "Because what this metric calculates is BLEU scores, \n",
    "it has the same limitations as that metric, except that sacreBLEU is more easily reproducible.\n",
    "\n",
    "\"\"\"\n",
    "predictions = ['你好啊', 'general kenobi']\n",
    "references = [['hello there', 'kenobi將軍'],\n",
    "                [\"你好啊\", \"general kenobi\"]]\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "results = sacrebleu.compute(predictions=predictions, \n",
    "                            references=references,tokenize=\"char\",lowercase=True,use_effective_order=False) # 要調整tokenize，為何用char\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sacremoses\n",
    "sari = evaluate.load(\"sari\")\n",
    "sources=[\"'kenobi將軍，您好!\"]\n",
    "predictions = ['你好啊! general kenobi']\n",
    "references = [['hello there! kenobi將軍',\n",
    "                \"你好啊! general 肯諾比\"]]\n",
    "sari_score = sari.compute(sources=sources, predictions=predictions, references=references)\n",
    "\"\"\"\n",
    "\n",
    "SARI (system output against references and against the input sentence) is \n",
    "a metric used for evaluating automatic text simplification systems.\n",
    "The metric compares the predicted simplified sentences against the reference and the source sentences. \n",
    "It explicitly measures the goodness of words that are added, deleted and kept by the system.\n",
    "\n",
    "The range of values for the SARI score is between 0 and 100 -- \n",
    "the higher the value, the better the performance of the model being evaluated,\n",
    "with a SARI of 100 being a perfect score.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 72.0, 'num_edits': 9, 'ref_length': 12.5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Sentence Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The metric can take on any value 0 and above. 0 is a perfect score, \n",
    "meaning the predictions exactly match the references and no edits were necessary.\n",
    " Higher scores are worse. \n",
    " Scores above 100 mean that the cumulative number of edits, num_edits, \n",
    " is higher than the cumulative length of the references, ref_length.\n",
    "\n",
    "\"\"\"\n",
    "predictions = [\"這句話對嗎??\",\n",
    "               \"那這句話呢?\",\n",
    "          \"這笑話很難笑\"     ]\n",
    "references = [[\"這句話對嗎\", \"這句話對嗎!?!\"],\n",
    "               [\"這句話怎麼樣?\", \"wHaT aBoUt ThIs SeNtEnCe?\"],\n",
    "               [\"你的笑話...\", \"...TERrible\"]]\n",
    "\n",
    "ter = evaluate.load(\"ter\")\n",
    "results = ter.compute(predictions=predictions,\n",
    "                        references=references,\n",
    "                         case_sensitive=True,support_zh_ja_chars=True,ignore_punct=True,normalized=True)\n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3333333333333333\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Word Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Word error rate (WER) is a common metric of the performance of an automatic speech recognition (ASR) system.\n",
    "\n",
    "The general difficulty of measuring the performance of ASR systems lies in the fact that the recognized word sequence can have a different \n",
    "length from the reference word sequence (supposedly the correct one). The WER is derived from the Levenshtein distance, working at the word level.\n",
    "\n",
    "This problem is solved by first aligning the recognized word sequence with the reference (spoken) \n",
    "word sequence using dynamic string alignment. \n",
    "Examination of this issue is seen through a theory called the power law that states the correlation between \n",
    "perplexity and word error rate (see this article for further information).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The lower the value, the better the performance of the ASR system, with a WER of 0 being a perfect score.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# !pip install jiwer\n",
    "wer = evaluate.load(\"wer\")\n",
    "predictions = ['你好啊', 'general kenobi']\n",
    "references1 = ['hello there', 'kenobi將軍']\n",
    "references2 = [\"你好啊\", \"general 肯諾比\"]\n",
    "wer_score1 = wer.compute(predictions=predictions, references=references1)\n",
    "wer_score2 = wer.compute(predictions=predictions, references=references2)\n",
    "\n",
    "print(wer_score1)\n",
    "print(wer_score2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wer': 1.3333333333333333, 'cer': 1.105263157894737}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Word Segmentation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "https://huggingface.co/datasets/google/xtreme_s\n",
    "\n",
    "wer: \n",
    "Word error rate (WER) is a common metric of the performance of an automatic speech recognition system. \n",
    "The lower the value, the better the performance of the ASR system, with a WER of 0 being a perfect score (see WER score for more information). \n",
    "It is returned for the mls, fleurs-asr, voxpopuli and babel subsets of the benchmark.\n",
    "\n",
    "cer: \n",
    "Character error rate (CER) is similar to WER, but operates on character instead of word. \n",
    "The lower the CER value, the better the performance of the ASR system, \n",
    "with a CER of 0 being a perfect score (see CER score for more information). \n",
    "It is returned for the mls, fleurs-asr, voxpopuli and babel subsets of the benchmark.\n",
    "\n",
    "多語言語音基準 (Multilingual Speech Benchmark, MLS) \n",
    "是一個用於評估多語言自動語音識別 (ASR) 模型性能的數據集。\n",
    "MLS 數據集包含多種語言的語音數據，旨在促進多語言語音技術的發展。評估模型性能時，\n",
    "常用的指標包括字錯誤率 (WER) 和語音片段的準確性。\n",
    "\n",
    "\n",
    "Fleurs-ASR：\n",
    "這代表的是 Fleurs 的自動語音識別（Automatic Speech Recognition, ASR）任務。\n",
    "Fleurs 是一個多語種語音數據集，\n",
    "用於訓練和評估語音識別系統。在這個任務中，模型需要將語音轉錄為相應的文本，並在多種語言上進行評估。\n",
    "\"\"\"\n",
    "\n",
    "xtreme_s_metric = evaluate.load('xtreme_s', 'mls')\n",
    "predictions = ['你好啊', 'general kenobi']\n",
    "references = ['hello there', 'kenobi將軍']\n",
    "results = xtreme_s_metric.compute(predictions=predictions, references=references)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"ArithmeticError\n",
    "pip uninstall spacy\n",
    "pip uninstall srsly\n",
    "pip uninstall blis\n",
    "pip uninstall cymem\n",
    "pip uninstall preshed\n",
    "pip uninstall murmurhash\n",
    "\n",
    "pip install thinc==8.2.2\n",
    "pip install blis==0.7.8\n",
    "pip install cymem\n",
    "pip install preshed\n",
    "pip install murmurhash\n",
    "pip install spacy\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download ja_core_news_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "python -m spacy download es_core_news_sm\n",
    "python -m spacy download it_core_news_sm\n",
    "python -m spacy download ko_core_news_sm\n",
    "python -m spacy download ru_core_news_sm\n",
    "python -m spacy download fr_core_news_sm\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
